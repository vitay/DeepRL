<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Julien Vitay">
  <title>Deep Reinforcement Learning</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <link rel="stylesheet" href="github.css">
  <script src="/usr/share/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>

<body class="markdown-body">

<header>
<h1 class="title"><strong>Deep Reinforcement Learning</strong></h1>
<p class="author">Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de">julien.vitay@informatik.tu-chemnitz.de</a></p>
</header>

<article>
<p class="abstract">The goal of this document is to summarize the state-of-the-art in deep reinforcement learning. It starts with basics in reinforcement learning and deep learning to introduce the notations. It is then composed of three main parts: 1) value-based algorithms (DQN…) 2) policy-gradient based algorithms (A3C, DDPG…) 3) Recurrent attention models (RAM…). Finally it provides and explains code snippets for these algorithms.</p>

<nav id="TOC" class ="toc">
<h2><strong>Deep Reinforcement Learning</strong></h2>
<p class="author">Julien Vitay</p>
<ul>
<li><a href="#sec:introduction"><span class="toc-section-number">1</span> Introduction</a><ul>
<li><a href="#sec:basic-reinforcement-learning"><span class="toc-section-number">1.1</span> Basic reinforcement learning</a><ul>
<li><a href="#sec:mdp-markov-decision-process"><span class="toc-section-number">1.1.1</span> MDP: Markov Decision Process</a></li>
<li><a href="#sec:pomdp-partially-observable-markov-decision-process"><span class="toc-section-number">1.1.2</span> POMDP: Partially Observable Markov Decision Process</a></li>
<li><a href="#sec:policies"><span class="toc-section-number">1.1.3</span> Policies</a></li>
<li><a href="#sec:bellman-equations"><span class="toc-section-number">1.1.4</span> Bellman equations</a></li>
<li><a href="#sec:dynamic-programming"><span class="toc-section-number">1.1.5</span> Dynamic programming</a></li>
<li><a href="#sec:monte-carlo-sampling"><span class="toc-section-number">1.1.6</span> Monte-Carlo sampling</a></li>
<li><a href="#sec:temporal-difference"><span class="toc-section-number">1.1.7</span> Temporal Difference</a></li>
<li><a href="#sec:actor-critic-architectures"><span class="toc-section-number">1.1.8</span> Actor-critic architectures</a></li>
<li><a href="#sec:function-approximation"><span class="toc-section-number">1.1.9</span> Function approximation</a></li>
</ul></li>
<li><a href="#sec:deep-learning"><span class="toc-section-number">1.2</span> Deep learning</a><ul>
<li><a href="#sec:deep-neural-networks"><span class="toc-section-number">1.2.1</span> Deep neural networks</a></li>
<li><a href="#sec:convolutional-networks"><span class="toc-section-number">1.2.2</span> Convolutional networks</a></li>
<li><a href="#sec:recurrent-neural-networks"><span class="toc-section-number">1.2.3</span> Recurrent neural networks</a></li>
</ul></li>
</ul></li>
<li><a href="#sec:value-based-methods"><span class="toc-section-number">2</span> Value-based methods</a><ul>
<li><a href="#sec:limitations-of-deep-neural-networks-for-function-approximation"><span class="toc-section-number">2.1</span> Limitations of deep neural networks for function approximation</a></li>
<li><a href="#sec:deep-q-network-dqn"><span class="toc-section-number">2.2</span> Deep Q Network (DQN)</a></li>
<li><a href="#sec:double-dqn"><span class="toc-section-number">2.3</span> Double DQN</a></li>
<li><a href="#sec:prioritised-replay"><span class="toc-section-number">2.4</span> Prioritised replay</a></li>
<li><a href="#sec:duelling-network"><span class="toc-section-number">2.5</span> Duelling network</a></li>
<li><a href="#sec:gorila"><span class="toc-section-number">2.6</span> GORILA</a></li>
</ul></li>
<li><a href="#sec:policy-gradient-methods"><span class="toc-section-number">3</span> Policy-gradient methods</a><ul>
<li><a href="#sec:reinforce"><span class="toc-section-number">3.1</span> REINFORCE</a></li>
<li><a href="#sec:a3c"><span class="toc-section-number">3.2</span> A3C</a></li>
<li><a href="#sec:continuous-action-spaces"><span class="toc-section-number">3.3</span> Continuous action spaces</a></li>
<li><a href="#sec:policy-gradient-theorems"><span class="toc-section-number">3.4</span> Policy gradient theorems</a></li>
<li><a href="#sec:stochastic-policy-gradient-svg"><span class="toc-section-number">3.5</span> Stochastic Policy Gradient (SVG)</a></li>
<li><a href="#sec:deterministic-policy-gradient-dpg"><span class="toc-section-number">3.6</span> Deterministic Policy Gradient (DPG)</a></li>
<li><a href="#sec:deep-deterministic-policy-gradient-ddpg"><span class="toc-section-number">3.7</span> Deep Deterministic Policy Gradient (DDPG)</a></li>
<li><a href="#sec:fictitious-self-play-fsp"><span class="toc-section-number">3.8</span> Fictitious Self-Play (FSP)</a></li>
</ul></li>
<li><a href="#sec:recurrent-attention-models"><span class="toc-section-number">4</span> Recurrent Attention Models</a></li>
<li><a href="#sec:deep-rl-for-robotics"><span class="toc-section-number">5</span> Deep RL for robotics</a></li>
<li><a href="#sec:code-samples"><span class="toc-section-number">6</span> Code samples</a><ul>
<li><a href="#sec:environments"><span class="toc-section-number">6.1</span> Environments</a></li>
<li><a href="#sec:algorithms"><span class="toc-section-number">6.2</span> Algorithms</a></li>
</ul></li>
<li><a href="#sec:references">References</a></li>
</ul>
</nav>

<h1 id="sec:introduction"><span class="header-section-number">1</span> Introduction</h1>
<p>Deep reinforcement learning (deep RL) is the successful interation of deep learning methods, classically used in supervised or unsupervised learning contexts, with reinforcement learning (RL), a well-studied adaptive control method used in problems with delayed and partial feedback <span class="citation">(R. S. Sutton and Barto, <a href="#ref-Sutton1998">1998</a>)</span>. This section starts with the basics of RL, mostly to set the notations, and provides a quick overview of deep neural networks.</p>
<h2 id="sec:basic-reinforcement-learning"><span class="header-section-number">1.1</span> Basic reinforcement learning</h2>
<p>RL methods apply to problems where an agent interacts with an environment in discrete time steps (Fig. <a href="#fig:agentenv">1</a>). At time <span class="math inline">\(t\)</span>, the agent is in state <span class="math inline">\(s_t\)</span> and decides to perform an action <span class="math inline">\(a_t\)</span>. At the next time step, it arrives in the state <span class="math inline">\(s_{t+1}\)</span> and obtains the reward <span class="math inline">\(r_{t+1}\)</span>. The goal of the agent is to maximize the reward obtained on the long term.</p>
<div class="figure">
<img src="img/rl-agent.jpg" alt="Figure 1: Interaction between an agent and its environment (R. S. Sutton and Barto, 1998)." id="fig:agentenv" style="width:30.0%" />
<p class="caption">Figure 1: Interaction between an agent and its environment <span class="citation">(R. S. Sutton and Barto, <a href="#ref-Sutton1998">1998</a>)</span>.</p>
</div>
<h3 id="sec:mdp-markov-decision-process"><span class="header-section-number">1.1.1</span> MDP: Markov Decision Process</h3>
<p>Most reinforcement learning problems are modeled as <strong>Markov Decision Processes</strong> (MDP) defined by five quantities:</p>
<ul>
<li>a state space <span class="math inline">\(\mathcal{S}\)</span> where each state <span class="math inline">\(s\)</span> respects the Markovian property. It can be finite or infinite.</li>
<li>an action space <span class="math inline">\(\mathcal{A}\)</span> of actions <span class="math inline">\(a\)</span>, which can be finite or infinite, discrete or continuous.</li>
<li>an initial state distribution <span class="math inline">\(p_0(s_0)\)</span> (from which states is the agent likely to start).</li>
<li>a transition dynamics model with density <span class="math inline">\(p(s&#39;|s, a)\)</span>, sometimes noted <span class="math inline">\(\mathcal{P}_{ss&#39;}^a\)</span>. It defines the probability of arriving in the state <span class="math inline">\(s&#39;\)</span> at time <span class="math inline">\(t+1\)</span> when being in the state <span class="math inline">\(s\)</span> and performing the action <span class="math inline">\(a\)</span> a time <span class="math inline">\(t\)</span>.</li>
<li>a reward function <span class="math inline">\(r(s, a, s&#39;) : \mathcal{S}\times\mathcal{A}\times\mathcal{S} \rightarrow \Re\)</span> defining the (stochastic) reward obtained after performing <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span> and arriving in <span class="math inline">\(s&#39;\)</span>.</li>
</ul>
<p>The <strong>Markovian property</strong> states that:</p>
<p><span><span class="math display">\[
    p(s_{t+1}|s_t, a_t) = p(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, \dots s_0, a_0)
\qquad(1)\]</span></span></p>
<p>i.e. you do not need the full history of the agent to predict where it will arrive after an action. If this is not the case, RL methods will not converge (or poorly). For example, solving video games by using a single frame as state is not Markovian: you can not predict in which direction an object is moving based on a single frame. You will either need to stack several consecutive frames together to create Markovian states, or use recurrent networks in the model to integrate non-Markovian states over time and imitate that property (see Section <a href="#sec:recurrent-attention-models">4</a>).</p>
<h3 id="sec:pomdp-partially-observable-markov-decision-process"><span class="header-section-number">1.1.2</span> POMDP: Partially Observable Markov Decision Process</h3>
<p>In many problems (e.g. vision-based), one does not have access to the true states of the agent, but one can only indirectly observe them. For example, in a video game, the true state is defined by a couple of variables: coordinates <span class="math inline">\((x, y)\)</span> of the two players, position of the ball, speed, etc. However, all you have access to are the raw pixels: sometimes the ball may be hidden behing a wall or a tree, but it still exists in the state space.</p>
<p>In a <strong>Partially Observable Markov Decision Process</strong> (POMDP), observations <span class="math inline">\(o_t\)</span> come from a space <span class="math inline">\(\mathcal{O}\)</span> and are linked to underlying states using the density function <span class="math inline">\(p(o_t| s_t)\)</span>. Observations are usually not Markovian, so the full history of observations <span class="math inline">\(h_t = (o_0, a_0, \dots o_t, a_t)\)</span> is needed to solve the problem (see Section <a href="#sec:recurrent-attention-models">4</a>).</p>
<h3 id="sec:policies"><span class="header-section-number">1.1.3</span> Policies</h3>
<p>The policy defines the behavior of the agent: which action should be taken in each state. One distinguishes two kinds of policies:</p>
<ul>
<li>a stochastic policy <span class="math inline">\(\pi : \mathcal{S} \rightarrow P(\mathcal{A})\)</span> defines the probability distribution <span class="math inline">\(P(\mathcal{A})\)</span> of performing an action.</li>
<li>a deterministic policy <span class="math inline">\(\mu(s_t)\)</span> is a discrete mapping of <span class="math inline">\(\mathcal{S} \rightarrow \mathcal{A}\)</span>.</li>
</ul>
<p>The policy can be used to explore the environment and generate trajectories of states, rewards and actions. The performance of a policy is determined by calculating the <strong>expected discounted return</strong>, i.e. the sum of all rewards received from time step t onwards:</p>
<p><span><span class="math display">\[
    R_t = \sum_{k=0}^{\infty} \gamma^k \, r_{t+k+1}
\qquad(2)\]</span></span></p>
<p>where <span class="math inline">\(0 &lt; \gamma &lt; 1\)</span> is the discount rate and <span class="math inline">\(r_{t+1}\)</span> represents the reward obtained during the transition from <span class="math inline">\(s_t\)</span> to <span class="math inline">\(s_{t+1}\)</span>.</p>
<p>The Q-value of a state-action pair <span class="math inline">\((s, a)\)</span> is defined as the expected discounted reward received if the agent takes <span class="math inline">\(a\)</span> from a state <span class="math inline">\(s\)</span> and follows the policy distribution <span class="math inline">\(\pi\)</span> thereafter:</p>
<p><span><span class="math display">\[
    Q^{\pi}(s, a) = {E}_{\pi}(R_t | s_t = s, a_t=a)
\qquad(3)\]</span></span></p>
<p>Similarly, the V-value of a state <span class="math inline">\(s\)</span> is the expected discounted reward received if the agent starts in <span class="math inline">\(s\)</span> and follows its policy <span class="math inline">\(\pi\)</span>.</p>
<p><span><span class="math display">\[
    V^{\pi}(s) = {E}_{\pi}(R_t | s_t = s)
\qquad(4)\]</span></span></p>
<p>Obviously, these quantities depend on the states/actions themselves (some chessboard configurations are intrinsically better than others, i.e. you are more likely to win from that state), but also on the policy (if you can kill your opponent in one move - meaning you are in an intrinsically good state - but systematically take the wrong decision and lose, this is a bad state).</p>
<h3 id="sec:bellman-equations"><span class="header-section-number">1.1.4</span> Bellman equations</h3>
<p>The V- and Q-values are obviously linked with each other. The value of state depend on the value of the actions possible in that state, modulated by the probability that an action will be taken (i.e. the policy):</p>
<p><span id="eq:v-value"><span class="math display">\[
    V^{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(s, a) \, Q^\pi(s,a)
\qquad(5)\]</span></span></p>
<p>For a deterministic policy (<span class="math inline">\(\pi(s, a) = 1\)</span> if <span class="math inline">\(a=a^*\)</span> and <span class="math inline">\(0\)</span> otherwise), the value of a state is the same as the value of the action that will be systematically taken.</p>
<p>Noting that:</p>
<p><span id="eq:return"><span class="math display">\[
    R_t = r_{t+1} + \gamma R_{t+1}
\qquad(6)\]</span></span></p>
<p>i.e. that the expected return at time <span class="math inline">\(t\)</span> is the sum of the immediate reward received during the next transition <span class="math inline">\(r_{t+1}\)</span> and of the expected return at the next state (<span class="math inline">\(R_{t+1}\)</span>, discounted by <span class="math inline">\(\gamma\)</span>), we can also write:</p>
<p><span id="eq:q-value"><span class="math display">\[
    Q^{\pi}(s, a) = \sum_{s&#39; \in \mathcal{S}} p(s&#39;|s, a) [r(s, a, s&#39;) + \gamma \, V^\pi(s&#39;)]
\qquad(7)\]</span></span></p>
<p>The value of an action depends on which state you arrive in (<span class="math inline">\(s&#39;\)</span>), with which probability (<span class="math inline">\(p(s&#39;|s, a)\)</span>), how much reward you receive immediately (<span class="math inline">\(r(s, a, s&#39;)\)</span>) and how much you will receive later (summarized by <span class="math inline">\(V^\pi(s&#39;)\)</span>).</p>
<p>Putting together Eq. <a href="#eq:v-value">5</a> and Eq. <a href="#eq:q-value">7</a>, we obtain the <strong>Bellman equations</strong>:</p>
<p><span><span class="math display">\[
    V^{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(s, a) \, \sum_{s&#39; \in \mathcal{S}} p(s&#39;|s, a) [r(s, a, s&#39;) + \gamma \, V^\pi(s&#39;)]
\qquad(8)\]</span></span></p>
<p><span><span class="math display">\[
    Q^{\pi}(s, a) = \sum_{s&#39; \in \mathcal{S}} p(s&#39;|s, a) [r(s, a, s&#39;) + \gamma \, \sum_{a&#39; \in \mathcal{A}} \pi(s&#39;, a&#39;) \, Q^\pi(s&#39;,a&#39;)]
\qquad(9)\]</span></span></p>
<p>The Bellman equations mean that the value of a state (resp. state-action pair) depends on the value of all other states (resp. state-action pairs), the current policy <span class="math inline">\(\pi\)</span> and the dynamics of the MDP (<span class="math inline">\(p(s&#39;|s, a)\)</span> and <span class="math inline">\(r(s, a, s&#39;)\)</span>).</p>
<div class="figure">
<img src="img/backup.png" alt="Figure 2: Backup diagrams correponding to the Bellman equations (R. S. Sutton and Barto, 1998)." id="fig:backup" style="width:50.0%" />
<p class="caption">Figure 2: Backup diagrams correponding to the Bellman equations <span class="citation">(R. S. Sutton and Barto, <a href="#ref-Sutton1998">1998</a>)</span>.</p>
</div>
<h3 id="sec:dynamic-programming"><span class="header-section-number">1.1.5</span> Dynamic programming</h3>
<p>The interesting property of the Bellman equations is that, if the states have the Markovian property, they admit <em>one and only one</em> solution. This means that for a given policy, if the dynamics of the MDP are known, it is possible to compute the value of all states or state-action pairs by solving the Bellman equations for all states or state-action pairs (<em>policy evaluation</em>).</p>
<p>Once the values are known for a given policy, it is possible to improve the policy by selecting with the highest probability the action with the highest Q-value. For example, if the current policy chooses the action <span class="math inline">\(a_1\)</span> over <span class="math inline">\(a_2\)</span> in <span class="math inline">\(s\)</span> (<span class="math inline">\(\pi(s, a_1) &gt; \pi(s, a_2)\)</span>), but after evaluating the policy it turns out that <span class="math inline">\(Q^\pi(s, a_2) &gt; Q^\pi(s, a_1)\)</span> (the expected return after <span class="math inline">\(a_2\)</span> is higher than after <span class="math inline">\(a_1\)</span>), it makes more sense to preferentially select <span class="math inline">\(a_2\)</span>, as there is more reward afterwards. We can then create a new policy <span class="math inline">\(\pi&#39;\)</span> where <span class="math inline">\(\pi&#39;(s, a_2) &gt; \pi&#39;(s, a_1)\)</span>, which is is <em>better</em> policy than <span class="math inline">\(\pi\)</span> as more reward can be gathered after <span class="math inline">\(s\)</span>.</p>
<div class="figure">
<img src="img/dynamicprogramming.png" alt="Figure 3: Dynamic programming alternates between policy evaluation and policy improvement (R. S. Sutton and Barto, 1998)." id="fig:dynamicprogramming" style="width:20.0%" />
<p class="caption">Figure 3: Dynamic programming alternates between policy evaluation and policy improvement <span class="citation">(R. S. Sutton and Barto, <a href="#ref-Sutton1998">1998</a>)</span>.</p>
</div>
<p><strong>Dynamic programming</strong> (DP) alternates between policy evaluation and policy improvement. If the problem is Markovian, it can be shown that DP converges to the <em>optimal policy</em> <span class="math inline">\(\pi^*\)</span>, i.e. the policy where the expected return is maximal in all states.</p>
<p>Note that by definition the optimal policy is <em>deterministic</em> and <em>greedy</em>: if there is an action with a maximal Q-value for the optimal policy, it should be systematically taken. For the optimal policy <span class="math inline">\(\pi^*\)</span>, the Bellman equations become:</p>
<p><span><span class="math display">\[
    V^{*}(s) = \max_{a \in \mathcal{A}} \sum_{s \in \mathcal{S}} p(s&#39; | s, a) \cdot [ r(s, a, s&#39;) + \gamma \cdot V^{*} (s&#39;) ]
\qquad(10)\]</span></span></p>
<p><span><span class="math display">\[
    Q^{*}(s, a) = \sum_{s&#39; \in \mathcal{S}} p(s&#39; | s, a) \cdot [r(s, a, s&#39;) + \gamma \max_{a&#39; \in \mathcal{A}} Q^* (s&#39;, a&#39;) ]
\qquad(11)\]</span></span></p>
<p>Dynamic programming can only be used when:</p>
<ul>
<li>the dynamics of the MDP (<span class="math inline">\(p(s&#39;|s, a)\)</span> and <span class="math inline">\(r(s, a, s&#39;)\)</span>) are fully known.</li>
<li>the number of states and state-action pairs is small (one Bellman equation per state or state/action to solve).</li>
</ul>
<p>In practice, sample-based methods such as Monte-Carlo or temporal difference are used.</p>
<h3 id="sec:monte-carlo-sampling"><span class="header-section-number">1.1.6</span> Monte-Carlo sampling</h3>
<div class="figure">
<img src="img/unifiedreturn.png" alt="Figure 4: Monte-Carlo methods accumulate rewards over a complete episode. (R. S. Sutton and Barto, 1998)." id="fig:mc" style="width:50.0%" />
<p class="caption">Figure 4: Monte-Carlo methods accumulate rewards over a complete episode. <span class="citation">(R. S. Sutton and Barto, <a href="#ref-Sutton1998">1998</a>)</span>.</p>
</div>
<p>When the environment is <em>a priori</em> unknown, it has to be explored in order to build estimates of the V or Q value functions. The key idea of <strong>Monte-Carlo</strong> sampling (MC) is rather simple:</p>
<ol style="list-style-type: decimal">
<li>Start from a state <span class="math inline">\(s_0\)</span>.</li>
<li>Perform an episode (sequence of state-action transitions) until a terminal state <span class="math inline">\(s_T\)</span> is reached using your current policy <span class="math inline">\(\pi\)</span>.</li>
<li>Accumulate the rewards into the actual return for that episode <span class="math inline">\(R_t^{(e)} = \sum_{k=0}^T r_{t+k+1}\)</span> for each time step.</li>
<li>Repeat often enough so that the value of a state <span class="math inline">\(s\)</span> can be approximated by the average of many actual returns:</li>
</ol>
<p><span><span class="math display">\[V^\pi(s) = E^\pi(R_t | s_t = s) = \frac{1}{M} \sum_{e=1}^M R_t^{(e)}\qquad(12)\]</span></span></p>
<p>In practice, the estimated values are updated using continuous updates:</p>
<p><span><span class="math display">\[
    V^\pi(s) \leftarrow V^\pi(s) + \alpha (R_t - V^\pi(s))
\qquad(13)\]</span></span></p>
<p>Q-values can also be approximated using the same procedure:</p>
<p><span><span class="math display">\[
    Q^\pi(s, a) \leftarrow Q^\pi(s, a) + \alpha (R_t - Q^\pi(s, a))
\qquad(14)\]</span></span></p>
<p>The two main drawbacks of MC methods are:</p>
<ol style="list-style-type: decimal">
<li>The task must be episodic, i.e. stop after a finite amount of transitions. Updates are only applied at the end of an episode.</li>
<li>A sufficient level of exploration has to be ensured to make sure the estimates converge to the optimal values.</li>
</ol>
<p>The second issue is linked to the <strong>exploration-exploitation</strong> dilemma: the episode is generated using the current policy (or a policy derived from it, see later). If the policy always select the same actions from the beginning (exploitation), the agent will never discover better alternatives: the values will converge to a local minimum. If the policy always pick randomly actions (exploration), the policy which is evaluated is not the current policy <span class="math inline">\(\pi\)</span>, but the random policy. A trade-off between the two therefore has to be maintained: usually a lot of exploration at the beginning of learning to accumulate knowledge about the environment, less towards the end to actually use the knowledge and perform optimally.</p>
<p>There are two types of methods trying to cope with exploration:</p>
<ul>
<li><strong>On-policy</strong> methods generate the episodes using the learned policy <span class="math inline">\(\pi\)</span>, but it has to be <em><span class="math inline">\(\epsilon\)</span>-soft</em>, i.e. stochastic: it has to let a probability of at least <span class="math inline">\(\epsilon\)</span> of selecting another action than the greedy action (the one with the highest estimated Q-value).</li>
<li><strong>Off-policy</strong> methods use a second policy called the <em>behavior policy</em> to generate the episodes, but learn a different policy for exploitation, which can even be deterministic.</li>
</ul>
<p><span class="math inline">\(\epsilon\)</span>-soft policies are easy to create. The simplest one is the <strong><span class="math inline">\(\epsilon\)</span>-greedy</strong> action selection method, which assigns a probability <span class="math inline">\((1-\epsilon)\)</span> of selecting the greedy action (the one with the highest Q-value), and a probability <span class="math inline">\(\epsilon\)</span> of selecting any of the other available actions:</p>
<p><span><span class="math display">\[ 
    a_t = \begin{cases} a_t^* \quad \text{with probability} \quad (1 - \epsilon) \\
                       \text{any other action with probability } \epsilon \end{cases}
\qquad(15)\]</span></span></p>
<p>Another solution is the <strong>Softmax</strong> (or Gibbs distribution) action selection method, which assigns to each action a probability of being selected depending on their relative Q-values:</p>
<p><span><span class="math display">\[
    P(s, a) = \frac{\exp Q^\pi(s, a) / \tau}{ \sum_b \exp Q^\pi(s, b) / \tau}
\qquad(16)\]</span></span></p>
<p><span class="math inline">\(\tau\)</span> is a positive parameter called the temperature: high temperatures cause the actions to be nearly equiprobable, while low temperatures cause <span class="math inline">\(\tau\)</span> is a positive parameter called the temperature.</p>
<p>The advantage of off-policy methods is that domain knowledge can be used to restrict the search in the state-action space. For example, only moves actually played by chess experts in a given state will be actually explored, not random stupid moves. The obvious drawback being that if the optimal solution is not explored by the behavior policy, the agent has no way to discover it by itself.</p>
<h3 id="sec:temporal-difference"><span class="header-section-number">1.1.7</span> Temporal Difference</h3>
<p>The main drawback of Monte-Carlo methods is that the task must be composed of finite episodes. Not only is it not always possible, but value updates have to wait for the end of the episode, what slows learning down. <strong>Temporal difference</strong> methods simply replace the actual return obtained after a state or an action, by an estimation composed of the reward immediately received plus the value of the next state or action, as in Eq. <a href="#eq:return">6</a>:</p>
<p><span><span class="math display">\[
    R_t \approx r_{t+1} + \gamma \, V^\pi(s_{t+1}) \approx r_{t+1} + \gamma \, Q^\pi(s_{t+1}, a_{t+1})
\qquad(17)\]</span></span></p>
<p>This gives us the following learning rules:</p>
<p><span><span class="math display">\[
    V^\pi(s_t) \leftarrow V^\pi(s_t) + \alpha (r_{t+1} + \gamma \, V^\pi(s_{t+1}) - V^\pi(s_t))
\qquad(18)\]</span></span></p>
<p><span><span class="math display">\[
    Q^\pi(s_t, a_t) \leftarrow Q^\pi(s_t, a_t) + \alpha (r_{t+1} + \gamma \, Q^\pi(s_{t+1}, a_{t+1}) - Q^\pi(s_t, a_t))
\qquad(19)\]</span></span></p>
<p>The quantity:</p>
<p><span><span class="math display">\[
 \delta_t = r_{t+1} + \gamma \, V^\pi(s_{t+1}) - V^\pi(s_t)
\qquad(20)\]</span></span></p>
<p>or:</p>
<p><span><span class="math display">\[
    \delta_t = r_{t+1} + \gamma \, Q^\pi(s_{t+1}, a_{t+1}) - Q^\pi(s_t, a_t)
\qquad(21)\]</span></span></p>
<p>is called the <strong>reward-prediction error</strong> (RPE): it defines the surprise between the current reward prediction (<span class="math inline">\(V^\pi(s_t)\)</span> or <span class="math inline">\(Q^\pi(s_t, a_t)\)</span>) and the sum of the immediate reward plus the reward prediction in the next state / after the next action.</p>
<ul>
<li>If <span class="math inline">\(\delta_t &gt; 0\)</span>, the transition was positively surprising: one obtains more reward or lands in a better state than expected. The initial state or action was actually underrated, so its estimated value must be increased.</li>
<li>If <span class="math inline">\(\delta_t &lt; 0\)</span>, the transition was negatively surprising. The initial state or action was overrated, its value must be decreased.</li>
<li>If <span class="math inline">\(\delta_t = 0\)</span>, the transition was fully predicted: one obtains as much reward as expected, so the values should stay as they are.</li>
</ul>
<p>The main advantage of this learning method is that the update of the V- or Q-value can be applied immediately after a transition: no need to wait until the end of an episode, or even to have episodes at all.</p>
<div class="figure">
<img src="img/backup-TD.png" alt="Figure 5: Temporal difference algorithms update values after a single transition. (R. S. Sutton and Barto, 1998)." id="fig:td" style="width:3.0%" />
<p class="caption">Figure 5: Temporal difference algorithms update values after a single transition. <span class="citation">(R. S. Sutton and Barto, <a href="#ref-Sutton1998">1998</a>)</span>.</p>
</div>
<p>When learning Q-values directly, the question is which next action <span class="math inline">\(a_{t+1}\)</span> should be used in the update rule: the action that will actually be taken for the next transition (defined by <span class="math inline">\(\pi(s_{t+1}, a_{t+1})\)</span>), or the greedy action (<span class="math inline">\(a^* = \text{argmax}_a Q^\pi(s_{t+1}, a)\)</span>). This relates to the <em>on-policy / off-policy</em> distinction already seen for MC methods:</p>
<ul>
<li><em>On-policy</em> TD learning is called <strong>SARSA</strong> (state-action-reward-state-action). It uses the next action sampled from the policy <span class="math inline">\(\pi(s_{t+1}, a_{t+1})\)</span> to update the current transition. It is required that this next action will actually be performed for the next transition. The policy must be <span class="math inline">\(\epsilon\)</span>-soft, for example <span class="math inline">\(\epsilon\)</span>-greedy or softmax:</li>
</ul>
<p><span><span class="math display">\[
    \delta_t = r_{t+1} + \gamma \, Q^\pi(s_{t+1}, a_{t+1}) - Q^\pi(s_t, a_t)
\qquad(22)\]</span></span></p>
<ul>
<li><em>Off-policy</em> TD learning is called <strong>Q-learning</strong> <span class="citation">(Watkins, <a href="#ref-Watkins1989">1989</a>)</span>. The greedy action in the next state (the one with the highest Q-value) is used to update the current transition. It does not mean that the greedy action will actually have to be selected for the next transition. The learned policy can therefore also be deterministic:</li>
</ul>
<p><span><span class="math display">\[
    \delta_t = r_{t+1} + \gamma \, \max_a Q^\pi(s_{t+1}, a) - Q^\pi(s_t, a_t)
\qquad(23)\]</span></span></p>
<p>In Q-learning, the behavior policy has to ensure exploration, while this is achieved implicitely by the learned policy in SARSA, as it must be <span class="math inline">\(\epsilon\)</span>-soft. An easy way of building a behavior policy based on a deterministic learned policy is <span class="math inline">\(\epsilon\)</span>-greedy: the deterministic action <span class="math inline">\(\mu(s_t)\)</span> is chosen with probability 1 - <span class="math inline">\(\epsilon\)</span>, the other actions with probability <span class="math inline">\(\epsilon\)</span>. In continuous action spaces, additive noise (e.g. Ohrstein-Uhlenbeck) can be added to the action.</p>
<p>Alternatively, domain knowledge can be used to create the behavior policy and restrict the search to meaningful actions: compilation of expert moves in games, approximate solutions, etc. Again, the risk is that the behavior policy never explores the actually optimal actions.</p>
<h3 id="sec:actor-critic-architectures"><span class="header-section-number">1.1.8</span> Actor-critic architectures</h3>
<p>Let’s consider the reward-prediction error (RPE) based on state values:</p>
<p><span><span class="math display">\[
 \delta_t = r_{t+1} + \gamma \, V^\pi(s_{t+1}) - V^\pi(s_t)
\qquad(24)\]</span></span></p>
<p>As noted in the previous section, the RPE represents how surprisingly good (or bad) a transition between two states has been. It can be used to update the value of the state <span class="math inline">\(s_t\)</span>:</p>
<p><span><span class="math display">\[
    V^\pi(s_t) \leftarrow V^\pi(s_t) + \alpha \, \delta_t
\qquad(25)\]</span></span></p>
<p>This allows to estimate the values of all states for the current policy. However, this does not help to 1) directy select the best action or 2) improve the policy. When only the V-values are given, one can only want to reach the next state <span class="math inline">\(V^\pi(s_{t+1})\)</span> with the highest value: one needs to know which action leads to this better state, i.e. have a model of the environment. Actually, one selects the action with the highest Q-value:</p>
<p><span><span class="math display">\[
    Q^{\pi}(s, a) = \sum_{s&#39; \in \mathcal{S}} p(s&#39;|s, a) [r(s, a, s&#39;) + \gamma \, V^\pi(s&#39;)]
\qquad(26)\]</span></span></p>
<p>An action may lead to a high-valued state, but with such a small probability that it is actually not worth it. <span class="math inline">\(p(s&#39;|s, a)\)</span> and <span class="math inline">\(r(s, a, s&#39;)\)</span> therefore have to be known (or at least approximated), what defeats the purpose of sample-based methods.</p>
<div class="figure">
<img src="img/actorcritic.jpg" alt="Figure 6: Actor-critic architecture." id="fig:actorcritic" style="width:50.0%" />
<p class="caption">Figure 6: Actor-critic architecture.</p>
</div>
<p><strong>Actor-critic</strong> architectures have been proposed to solve this problem:</p>
<ol style="list-style-type: decimal">
<li>The <strong>critic</strong> learns to estimate the value of a state <span class="math inline">\(V^\pi(s_t)\)</span> and compute the RPE <span class="math inline">\(\delta_t = r_{t+1} + \gamma \, V^\pi(s_{t+1}) - V^\pi(s_t)\)</span>.</li>
<li>The <strong>actor</strong> uses the RPE to update a <em>preference</em> for the executed action: action with positive RPEs (positively surprising) should be reinforced (i.e. taken again in the future), while actions with negative RPEs should be avoided in the future.</li>
</ol>
<p>The main interest of this architecture is that the actor can take any form (neural network, decision tree), as long as it able to use the RPE for learning.</p>
<h3 id="sec:function-approximation"><span class="header-section-number">1.1.9</span> Function approximation</h3>
<p>The goal of the agent is to find the optimal policy maximizing the expected return from every state.  methods (such as DQN) achieve that goal by estimating the Q-value of each state-action pair. Discrete algorithms transform these Q-values into a stochastic policy by sampling from a Gibbs distribution (softmax) to obtain the probability of choosing an action. The Q-values can be approximated by a deep neural network, by minimizing the quadratic error between the predicted Q-value <span class="math inline">\(Q^{\pi}(s, a)\)</span> and an estimation of the real expected return <span class="math inline">\(R_t\)</span> after that action:</p>
<p><span><span class="math display">\[
    \mathcal{L}(\theta) = {E}_{\pi} [r_t + \gamma Q^{\pi}(s_{t+1}, a_{t+1}) - Q^{\pi}(s_t, a_t)]^2
\qquad(27)\]</span></span></p>
<h2 id="sec:deep-learning"><span class="header-section-number">1.2</span> Deep learning</h2>
<h3 id="sec:deep-neural-networks"><span class="header-section-number">1.2.1</span> Deep neural networks</h3>
<h3 id="sec:convolutional-networks"><span class="header-section-number">1.2.2</span> Convolutional networks</h3>
<h3 id="sec:recurrent-neural-networks"><span class="header-section-number">1.2.3</span> Recurrent neural networks</h3>
<h1 id="sec:value-based-methods"><span class="header-section-number">2</span> Value-based methods</h1>
<h2 id="sec:limitations-of-deep-neural-networks-for-function-approximation"><span class="header-section-number">2.1</span> Limitations of deep neural networks for function approximation</h2>
<h2 id="sec:deep-q-network-dqn"><span class="header-section-number">2.2</span> Deep Q Network (DQN)</h2>
<p><span class="citation">Mnih et al. (<a href="#ref-Mnih2015">2015</a>)</span></p>
<h2 id="sec:double-dqn"><span class="header-section-number">2.3</span> Double DQN</h2>
<p><a href="https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/" class="uri">https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/</a></p>
<p><span class="citation">Hasselt, Guez, and Silver (<a href="#ref-vanHasselt2015">2015</a>)</span></p>
<h2 id="sec:prioritised-replay"><span class="header-section-number">2.4</span> Prioritised replay</h2>
<p><span class="citation">Schaul, Quan, Antonoglou, and Silver (<a href="#ref-Schaul2015">2015</a>)</span></p>
<h2 id="sec:duelling-network"><span class="header-section-number">2.5</span> Duelling network</h2>
<p><span class="citation">Wang et al. (<a href="#ref-Wang2016">2016</a>)</span></p>
<h2 id="sec:gorila"><span class="header-section-number">2.6</span> GORILA</h2>
<h1 id="sec:policy-gradient-methods"><span class="header-section-number">3</span> Policy-gradient methods</h1>
<p><em>Policy gradient</em> methods directly learn to produce the policy (stochastic or not). The goal of the neural network is to maximize an objective function <span class="math inline">\(J(\theta) = {E}_{\pi_\theta}(R_t)\)</span>. The   provides a useful estimate of the gradient that should be given to the neural network:</p>
<p><span><span class="math display">\[
\nabla_\theta J(\theta) = {E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) Q^{\pi_\theta}(s, a)]
\qquad(28)\]</span></span></p>
<p><span class="citation">R. Sutton, McAllester, Singh, and Mansour (<a href="#ref-Sutton1999">1999</a>)</span>, <span class="citation">Silver et al. (<a href="#ref-Silver2014">2014</a>)</span></p>
<h2 id="sec:reinforce"><span class="header-section-number">3.1</span> REINFORCE</h2>
<p><span class="citation">Williams (<a href="#ref-Williams1992">1992</a>)</span></p>
<h2 id="sec:a3c"><span class="header-section-number">3.2</span> A3C</h2>
<p><span class="citation">Mnih et al. (<a href="#ref-Mnih2016">2016</a>)</span></p>
<p>HogWild!: <span class="citation">Niu, Recht, Re, and Wright (<a href="#ref-Niu2011">2011</a>)</span></p>
<h2 id="sec:continuous-action-spaces"><span class="header-section-number">3.3</span> Continuous action spaces</h2>
<h2 id="sec:policy-gradient-theorems"><span class="header-section-number">3.4</span> Policy gradient theorems</h2>
<h2 id="sec:stochastic-policy-gradient-svg"><span class="header-section-number">3.5</span> Stochastic Policy Gradient (SVG)</h2>
<p><span class="citation">Heess et al. (<a href="#ref-Heess2015">2015</a>)</span></p>
<h2 id="sec:deterministic-policy-gradient-dpg"><span class="header-section-number">3.6</span> Deterministic Policy Gradient (DPG)</h2>
<p><span class="citation">Silver et al. (<a href="#ref-Silver2014">2014</a>)</span></p>
<h2 id="sec:deep-deterministic-policy-gradient-ddpg"><span class="header-section-number">3.7</span> Deep Deterministic Policy Gradient (DDPG)</h2>
<p><span class="citation">Lillicrap et al. (<a href="#ref-Lillicrap2015">2015</a>)</span></p>
<h2 id="sec:fictitious-self-play-fsp"><span class="header-section-number">3.8</span> Fictitious Self-Play (FSP)</h2>
<h1 id="sec:recurrent-attention-models"><span class="header-section-number">4</span> Recurrent Attention Models</h1>
<p><span class="citation">Mnih, Heess, Graves, and Kavukcuoglu (<a href="#ref-Mnih2014">2014</a>)</span>, <span class="citation">Ba, Mnih, and Kavukcuoglu (<a href="#ref-Ba2014">2014</a>)</span>. <span class="citation">Stollenga, Masci, Gomez, and Schmidhuber (<a href="#ref-Stollenga2014">2014</a>)</span></p>
<h1 id="sec:deep-rl-for-robotics"><span class="header-section-number">5</span> Deep RL for robotics</h1>
<p><span class="citation">Agrawal, Nair, Abbeel, Malik, and Levine (<a href="#ref-Agrawal2016">2016</a>)</span>, <span class="citation">Dosovitskiy and Koltun (<a href="#ref-Dosovitskiy2016">2016</a>)</span>, <span class="citation">Gu, Holly, Lillicrap, and Levine (<a href="#ref-Gu2017">2017</a>)</span>, <span class="citation">Levine, Finn, Darrell, and Abbeel (<a href="#ref-Levine2016a">2016</a>)</span>, <span class="citation">Levine, Pastor, Krizhevsky, and Quillen (<a href="#ref-Levine2016b">2016</a>)</span>, <span class="citation">Zhang, Leitner, Milford, Upcroft, and Corke (<a href="#ref-Zhang2015">2015</a>)</span></p>
<h1 id="sec:code-samples"><span class="header-section-number">6</span> Code samples</h1>
<h2 id="sec:environments"><span class="header-section-number">6.1</span> Environments</h2>
<ul>
<li>OpenAI Gym <a href="https://gym.openai.com" class="uri">https://gym.openai.com</a>: a toolkit for comparing RL algorithms.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> gym
env <span class="op">=</span> gym.make(<span class="st">&quot;Taxi-v1&quot;</span>)
observation <span class="op">=</span> env.reset()
<span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):
    env.render()
    action <span class="op">=</span> env.action_space.sample()
    observation, reward, done, info <span class="op">=</span> env.step(action)</code></pre></div>
<ul>
<li><p>OpenAi Universe <a href="https://universe.openai.com" class="uri">https://universe.openai.com</a>: a framework for controlling video games.</p></li>
<li><p>Darts environment <a href="https://github.com/DartEnv/dart-env" class="uri">https://github.com/DartEnv/dart-env</a>: an extension of gym to use the Darts simulator instead of Mujoco.</p></li>
<li><p>NIPS 2017 musculoskettal challenge <a href="https://github.com/stanfordnmbl/osim-rl" class="uri">https://github.com/stanfordnmbl/osim-rl</a></p></li>
</ul>
<h2 id="sec:algorithms"><span class="header-section-number">6.2</span> Algorithms</h2>
<ul>
<li><p><code>rl-code</code> <a href="https://github.com/rlcode/reinforcement-learning" class="uri">https://github.com/rlcode/reinforcement-learning</a>: many code samples for simple RL problems</p>
<ul>
<li><p>Gridworld</p>
<ul>
<li>Policy Iteration</li>
<li>Value Iteration</li>
<li>Monte Carlo</li>
<li>SARSA</li>
<li>Q-Learning</li>
<li>Deep SARSA</li>
<li>REINFORCE</li>
</ul></li>
<li><p>CartPole - Applying deep reinforcement learning on basic Cartpole game.</p>
<ul>
<li>Deep Q Network</li>
<li>Double Deep Q Network</li>
<li>Policy Gradient</li>
<li>Actor Critic (A2C)</li>
<li>Asynchronous Advantage Actor Critic (A3C)</li>
</ul></li>
<li><p>Atari - Mastering Atari games with Deep Reinforcement Learning</p>
<ul>
<li>Breakout - DQN, DDQN Dueling DDQN A3C</li>
<li>Pong - Policy Gradient</li>
</ul></li>
<li><p>OpenAI GYM - [WIP]</p>
<ul>
<li>Mountain Car - DQN</li>
</ul></li>
</ul></li>
<li><p><code>keras-rl</code> <a href="https://github.com/matthiasplappert/keras-rl" class="uri">https://github.com/matthiasplappert/keras-rl</a>: many RL algorithms implemented in keras.</p>
<ul>
<li>Deep Q Learning (DQN)</li>
<li>Double DQN</li>
<li>Deep Deterministic Policy Gradient (DDPG)</li>
<li>Continuous DQN (CDQN or NAF)</li>
<li>Cross-Entropy Method (CEM)</li>
<li>Dueling network DQN (Dueling DQN)</li>
<li>Deep SARSA</li>
</ul></li>
<li><p>DDPG in tensorflow <a href="http://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html" class="uri">http://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html</a></p></li>
</ul>
<h1 id="sec:references" class="unnumbered">References</h1>
<div id="refs" class="references">
<div id="ref-Agrawal2016">
<p>Agrawal, P., Nair, A., Abbeel, P., Malik, J., and Levine, S. (2016). Learning to Poke by Poking: Experiential Learning of Intuitive Physics. Retrieved from <a href="http://arxiv.org/abs/1606.07419" class="uri">http://arxiv.org/abs/1606.07419</a></p>
</div>
<div id="ref-Ba2014">
<p>Ba, J., Mnih, V., and Kavukcuoglu, K. (2014). Multiple Object Recognition with Visual Attention. Retrieved from <a href="http://arxiv.org/abs/1412.7755" class="uri">http://arxiv.org/abs/1412.7755</a></p>
</div>
<div id="ref-Dosovitskiy2016">
<p>Dosovitskiy, A., and Koltun, V. (2016). Learning to Act by Predicting the Future. Retrieved from <a href="http://arxiv.org/abs/1611.01779" class="uri">http://arxiv.org/abs/1611.01779</a></p>
</div>
<div id="ref-Gu2017">
<p>Gu, S., Holly, E., Lillicrap, T., and Levine, S. (2017). Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates. In <em>Proc. icra</em>. Retrieved from <a href="http://arxiv.org/abs/1610.00633" class="uri">http://arxiv.org/abs/1610.00633</a></p>
</div>
<div id="ref-vanHasselt2015">
<p>Hasselt, H. van, Guez, A., and Silver, D. (2015). Deep Reinforcement Learning with Double Q-learning. Retrieved from <a href="http://arxiv.org/abs/1509.06461" class="uri">http://arxiv.org/abs/1509.06461</a></p>
</div>
<div id="ref-Heess2015">
<p>Heess, N., Wayne, G., Silver, D., Lillicrap, T., Tassa, Y., and Erez, T. (2015). Learning continuous control policies by stochastic value gradients. MIT Press. Retrieved from <a href="http://dl.acm.org/citation.cfm?id=2969569" class="uri">http://dl.acm.org/citation.cfm?id=2969569</a></p>
</div>
<div id="ref-Levine2016a">
<p>Levine, S., Finn, C., Darrell, T., and Abbeel, P. (2016). End-to-End Training of Deep Visuomotor Policies. <em>JMLR</em>, <em>17</em>. Retrieved from <a href="http://arxiv.org/abs/1504.00702" class="uri">http://arxiv.org/abs/1504.00702</a></p>
</div>
<div id="ref-Levine2016b">
<p>Levine, S., Pastor, P., Krizhevsky, A., and Quillen, D. (2016). Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection. In <em>Proc. iser</em>. Retrieved from <a href="http://arxiv.org/abs/1603.02199" class="uri">http://arxiv.org/abs/1603.02199</a></p>
</div>
<div id="ref-Lillicrap2015">
<p>Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., … Wierstra, D. (2015). Continuous control with deep reinforcement learning. <em>CoRR</em>. Retrieved from <a href="http://arxiv.org/abs/1509.02971" class="uri">http://arxiv.org/abs/1509.02971</a></p>
</div>
<div id="ref-Mnih2016">
<p>Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T. P., Harley, T., … Kavukcuoglu, K. (2016). Asynchronous Methods for Deep Reinforcement Learning. In <em>Proc. icml</em>. Retrieved from <a href="http://arxiv.org/abs/1602.01783" class="uri">http://arxiv.org/abs/1602.01783</a></p>
</div>
<div id="ref-Mnih2014">
<p>Mnih, V., Heess, N., Graves, A., and Kavukcuoglu, K. (2014). Recurrent Models of Visual Attention. Retrieved from <a href="http://arxiv.org/abs/1406.6247" class="uri">http://arxiv.org/abs/1406.6247</a></p>
</div>
<div id="ref-Mnih2015">
<p>Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., … Hassabis, D. (2015). Human-level control through deep reinforcement learning. <em>Nature</em>, <em>518</em>(7540), 529–533. <a href="https://doi.org/10.1038/nature14236" class="uri">https://doi.org/10.1038/nature14236</a></p>
</div>
<div id="ref-Niu2011">
<p>Niu, F., Recht, B., Re, C., and Wright, S. J. (2011). HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent. In <em>Proc. adv. neural inf. process. syst.</em> (p. 21). Retrieved from <a href="http://arxiv.org/abs/1106.5730" class="uri">http://arxiv.org/abs/1106.5730</a></p>
</div>
<div id="ref-Schaul2015">
<p>Schaul, T., Quan, J., Antonoglou, I., and Silver, D. (2015). Prioritized Experience Replay. Retrieved from <a href="http://arxiv.org/abs/1511.05952" class="uri">http://arxiv.org/abs/1511.05952</a></p>
</div>
<div id="ref-Silver2014">
<p>Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M. (2014). Deterministic Policy Gradient Algorithms. In E. P. Xing and T. Jebara (Eds.), <em>Proc. icml</em> (Vol. 32, pp. 387–395). Bejing, China: PMLR. Retrieved from <a href="http://proceedings.mlr.press/v32/silver14.html" class="uri">http://proceedings.mlr.press/v32/silver14.html</a></p>
</div>
<div id="ref-Stollenga2014">
<p>Stollenga, M., Masci, J., Gomez, F., and Schmidhuber, J. (2014). Deep Networks with Internal Selective Attention through Feedback Connections. Retrieved from <a href="http://arxiv.org/abs/1407.3068" class="uri">http://arxiv.org/abs/1407.3068</a></p>
</div>
<div id="ref-Sutton1998">
<p>Sutton, R. S., and Barto, A. G. (1998). <em>Reinforcement learning: An introduction</em> (Vol. 28). MIT press.</p>
</div>
<div id="ref-Sutton1999">
<p>Sutton, R., McAllester, D., Singh, S., and Mansour, Y. (1999). Policy Gradient Methods for Reinforcement Learning with Function Approximation. In <em>Neural inf. process. syst. 12</em> (pp. 1057–1063).</p>
</div>
<div id="ref-Wang2016">
<p>Wang, Z., Schaul, T., Hessel, M., Hasselt, H. van, Lanctot, M., and Freitas, N. de. (2016). Dueling Network Architectures for Deep Reinforcement Learning. In <em>Proc. icml</em>. New York, NY, USA. Retrieved from <a href="http://arxiv.org/abs/1511.06581" class="uri">http://arxiv.org/abs/1511.06581</a></p>
</div>
<div id="ref-Watkins1989">
<p>Watkins, C. J. (1989). <em>Learning from delayed rewards</em> (PhD thesis). University of Cambridge, England.</p>
</div>
<div id="ref-Williams1992">
<p>Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. <em>Mach. Learn.</em>, <em>8</em>, 229–256.</p>
</div>
<div id="ref-Zhang2015">
<p>Zhang, F., Leitner, J., Milford, M., Upcroft, B., and Corke, P. (2015). Towards Vision-Based Deep Reinforcement Learning for Robotic Motion Control. In <em>Proc. acra</em>. Retrieved from <a href="http://arxiv.org/abs/1511.03791" class="uri">http://arxiv.org/abs/1511.03791</a></p>
</div>
</div>

<article>



</body>
</html>
