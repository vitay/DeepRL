<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Julien Vitay">
  <title>Deep Reinforcement Learning</title>


  <link rel="stylesheet" href="assets/syntax.css">
  <link rel="stylesheet" href="assets/github.css">
  <script src="/usr/share/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->

</head>

<body class="markdown-body">

<header>
  <h1 class="title"><strong>Deep Reinforcement Learning</strong></h1>
  <p class="author">Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de">julien.vitay@informatik.tu-chemnitz.de</a></p>
</header>


<article>

<nav id="TOC" class ="toc">
  <h2><strong>Deep Reinforcement Learning</strong></h2>
  <p class="author">Julien Vitay</p>

<ul>
<li><a href="./0-Introduction.html#sec:introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="./1.1-BasicRL.html#sec:basics"><span class="toc-section-number">2</span> Basics</a><ul>
<li><a href="./1.1-BasicRL.html#sec:reinforcement-learning-and-markov-decision-process"><span class="toc-section-number">2.1</span> Reinforcement learning and Markov Decision Process</a><ul>
<li><a href="./1.1-BasicRL.html#sec:policy-and-value-functions"><span class="toc-section-number">2.1.1</span> Policy and value functions</a></li>
<li><a href="./1.1-BasicRL.html#sec:bellman-equations"><span class="toc-section-number">2.1.2</span> Bellman equations</a></li>
<li><a href="./1.1-BasicRL.html#sec:dynamic-programming"><span class="toc-section-number">2.1.3</span> Dynamic programming</a></li>
<li><a href="./1.1-BasicRL.html#sec:monte-carlo-sampling"><span class="toc-section-number">2.1.4</span> Monte-Carlo sampling</a></li>
<li><a href="./1.1-BasicRL.html#sec:temporal-difference"><span class="toc-section-number">2.1.5</span> Temporal Difference</a></li>
<li><a href="./1.1-BasicRL.html#sec:actor-critic-architectures"><span class="toc-section-number">2.1.6</span> Actor-critic architectures</a></li>
<li><a href="./1.1-BasicRL.html#sec:function-approximation"><span class="toc-section-number">2.1.7</span> Function approximation</a></li>
</ul></li>
<li><a href="./1.2-DeepLearning.html#sec:deep-learning"><span class="toc-section-number">2.2</span> Deep learning</a><ul>
<li><a href="./1.2-DeepLearning.html#sec:deep-neural-networks"><span class="toc-section-number">2.2.1</span> Deep neural networks</a></li>
<li><a href="./1.2-DeepLearning.html#sec:convolutional-networks"><span class="toc-section-number">2.2.2</span> Convolutional networks</a></li>
<li><a href="./1.2-DeepLearning.html#sec:recurrent-neural-networks"><span class="toc-section-number">2.2.3</span> Recurrent neural networks</a></li>
</ul></li>
</ul></li>
<li><a href="./2-Valuebased.html#sec:value-based-methods"><span class="toc-section-number">3</span> Value-based methods</a><ul>
<li><a href="./2-Valuebased.html#sec:limitations-of-deep-neural-networks-for-function-approximation"><span class="toc-section-number">3.1</span> Limitations of deep neural networks for function approximation</a></li>
<li><a href="./2-Valuebased.html#sec:deep-q-network-dqn"><span class="toc-section-number">3.2</span> Deep Q-Network (DQN)</a></li>
<li><a href="./2-Valuebased.html#sec:double-dqn"><span class="toc-section-number">3.3</span> Double DQN</a></li>
<li><a href="./2-Valuebased.html#sec:prioritised-replay"><span class="toc-section-number">3.4</span> Prioritised replay</a></li>
<li><a href="./2-Valuebased.html#sec:duelling-network"><span class="toc-section-number">3.5</span> Duelling network</a></li>
<li><a href="./2-Valuebased.html#sec:distributed-dqn-gorila"><span class="toc-section-number">3.6</span> Distributed DQN (GORILA)</a></li>
<li><a href="./2-Valuebased.html#sec:deep-recurrent-q-learning-drqn"><span class="toc-section-number">3.7</span> Deep Recurrent Q-learning (DRQN)</a></li>
<li><a href="./2-Valuebased.html#sec:other-variants-of-dqn"><span class="toc-section-number">3.8</span> Other variants of DQN</a></li>
</ul></li>
<li><a href="./3.1-PolicyGradient.html#sec:policy-gradient-methods"><span class="toc-section-number">4</span> Policy Gradient methods</a><ul>
<li><a href="./3.1-PolicyGradient.html#sec:reinforce"><span class="toc-section-number">4.1</span> REINFORCE</a><ul>
<li><a href="./3.1-PolicyGradient.html#sec:estimating-the-policy-gradient"><span class="toc-section-number">4.1.1</span> Estimating the policy gradient</a></li>
<li><a href="./3.1-PolicyGradient.html#sec:reducing-the-variance"><span class="toc-section-number">4.1.2</span> Reducing the variance</a></li>
<li><a href="./3.1-PolicyGradient.html#sec:policy-gradient-theorem"><span class="toc-section-number">4.1.3</span> Policy Gradient theorem</a></li>
</ul></li>
<li><a href="./3.2-ActorCritic.html#sec:advantage-actor-critic-methods"><span class="toc-section-number">4.2</span> Advantage Actor-Critic methods</a><ul>
<li><a href="./3.2-ActorCritic.html#sec:advantage-actor-critic-a2c"><span class="toc-section-number">4.2.1</span> Advantage Actor-Critic (A2C)</a></li>
<li><a href="./3.2-ActorCritic.html#sec:asynchronous-advantage-actor-critic-a3c"><span class="toc-section-number">4.2.2</span> Asynchronous Advantage Actor-Critic (A3C)</a></li>
<li><a href="./3.2-ActorCritic.html#sec:generalized-advantage-estimation-gae"><span class="toc-section-number">4.2.3</span> Generalized Advantage Estimation (GAE)</a></li>
<li><a href="./3.2-ActorCritic.html#sec:stochastic-actor-critic-for-continuous-action-spaces"><span class="toc-section-number">4.2.4</span> Stochastic actor-critic for continuous action spaces</a></li>
</ul></li>
<li><a href="./3.3-DPG.html#sec:deterministic-policy-gradient-dpg"><span class="toc-section-number">4.3</span> Deterministic Policy Gradient (DPG)</a><ul>
<li><a href="./3.3-DPG.html#sec:deterministic-policy-gradient-theorem"><span class="toc-section-number">4.3.1</span> Deterministic policy gradient theorem</a></li>
<li><a href="./3.3-DPG.html#sec:deep-deterministic-policy-gradient-ddpg"><span class="toc-section-number">4.3.2</span> Deep Deterministic Policy Gradient (DDPG)</a></li>
</ul></li>
<li><a href="./3.4-NaturalGradient.html#sec:natural-gradients"><span class="toc-section-number">4.4</span> Natural Gradients</a><ul>
<li><a href="./3.4-NaturalGradient.html#sec:natural-actor-critic-nac"><span class="toc-section-number">4.4.1</span> Natural Actor Critic (NAC)</a></li>
<li><a href="./3.4-NaturalGradient.html#sec:trust-region-policy-optimization-trpo"><span class="toc-section-number">4.4.2</span> Trust Region Policy Optimization (TRPO)</a></li>
<li><a href="./3.4-NaturalGradient.html#sec:proximal-policy-optimization-ppo"><span class="toc-section-number">4.4.3</span> Proximal Policy Optimization (PPO)</a></li>
</ul></li>
<li><a href="./3.4-NaturalGradient.html#sec:off-policy-actor-critic"><span class="toc-section-number">4.5</span> Off-policy Actor-Critic</a><ul>
<li><a href="./3.4-NaturalGradient.html#sec:importance-sampling"><span class="toc-section-number">4.5.1</span> Importance sampling</a></li>
<li><a href="./3.4-NaturalGradient.html#sec:linear-off-policy-actor-critic-offpac"><span class="toc-section-number">4.5.2</span> Linear Off-Policy Actor-Critic (OffPAC)</a></li>
<li><a href="./3.4-NaturalGradient.html#sec:actor-critic-with-experience-replay-acer"><span class="toc-section-number">4.5.3</span> Actor-Critic with Experience Replay (ACER)</a></li>
</ul></li>
<li><a href="./3.4-NaturalGradient.html#sec:other-policy-gradient-methods"><span class="toc-section-number">4.6</span> Other policy gradient methods</a><ul>
<li><a href="./3.4-NaturalGradient.html#sec:stochastic-value-gradient-svg"><span class="toc-section-number">4.6.1</span> Stochastic Value Gradient (SVG)</a></li>
<li><a href="./3.4-NaturalGradient.html#sec:fictitious-self-play-fsp"><span class="toc-section-number">4.6.2</span> Fictitious Self-Play (FSP)</a></li>
<li><a href="./3.4-NaturalGradient.html#sec:q-prop"><span class="toc-section-number">4.6.3</span> Q-Prop</a></li>
<li><a href="./3.4-NaturalGradient.html#sec:normalized-advantage-function-naf"><span class="toc-section-number">4.6.4</span> Normalized Advantage Function (NAF)</a></li>
</ul></li>
<li><a href="./3.4-NaturalGradient.html#sec:comparison-between-value-based-and-policy-gradient-methods"><span class="toc-section-number">4.7</span> Comparison between value-based and policy gradient methods</a></li>
<li><a href="./3.4-NaturalGradient.html#sec:gradient-free-policy-search"><span class="toc-section-number">4.8</span> Gradient-free policy search</a><ul>
<li><a href="./3.4-NaturalGradient.html#sec:cross-entropy-method-cem"><span class="toc-section-number">4.8.1</span> Cross-entropy Method (CEM)</a></li>
<li><a href="./3.4-NaturalGradient.html#sec:evolutionary-search-es"><span class="toc-section-number">4.8.2</span> Evolutionary Search (ES)</a></li>
</ul></li>
</ul></li>
<li><a href="./4-RAM.html#sec:recurrent-attention-models"><span class="toc-section-number">5</span> Recurrent Attention Models</a></li>
<li><a href="./5-ModelBased.html#sec:model-based-rl"><span class="toc-section-number">6</span> Model-based RL</a></li>
<li><a href="./6-Robotics.html#sec:deep-rl-for-robotics"><span class="toc-section-number">7</span> Deep RL for robotics</a></li>
<li><a href="./7-Practice.html#sec:deep-rl-in-practice"><span class="toc-section-number">8</span> Deep RL in practice</a><ul>
<li><a href="./7-Practice.html#sec:limitations"><span class="toc-section-number">8.1</span> Limitations</a></li>
<li><a href="./7-Practice.html#sec:reward-shaping"><span class="toc-section-number">8.2</span> Reward shaping</a></li>
<li><a href="./7-Practice.html#sec:simulation-environments"><span class="toc-section-number">8.3</span> Simulation environments</a></li>
<li><a href="./7-Practice.html#sec:algorithm-implementations"><span class="toc-section-number">8.4</span> Algorithm implementations</a></li>
</ul></li>
<li><a href="./8-References.html#sec:references">References</a></li>
</ul>


</nav>


</p>
<h1 id="sec:deep-rl-for-robotics"><span class="header-section-number">7</span> Deep RL for robotics</h1>
<p><strong>Work in progress</strong></p>
<p><span class="citation" data-cites="Agrawal2016">Agrawal, Nair, Abbeel, Malik, and Levine (<a href="8-References.html#ref-Agrawal2016">2016</a>)</span>, <span class="citation" data-cites="Dosovitskiy2016">Dosovitskiy and Koltun (<a href="8-References.html#ref-Dosovitskiy2016">2016</a>)</span>, <span class="citation" data-cites="Gu2017">Gu et al. (<a href="8-References.html#ref-Gu2017">2017</a>)</span>, <span class="citation" data-cites="Levine2016a">Levine et al. (<a href="8-References.html#ref-Levine2016a">2016</a><a href="8-References.html#ref-Levine2016a">a</a>)</span>, <span class="citation" data-cites="Levine2016b">Levine et al. (<a href="8-References.html#ref-Levine2016b">2016</a><a href="8-References.html#ref-Levine2016b">b</a>)</span>, <span class="citation" data-cites="Zhang2015">Zhang, Leitner, Milford, Upcroft, and Corke (<a href="8-References.html#ref-Zhang2015">2015</a>)</span></p>
<p>State of the art: <span class="citation" data-cites="Amarjyoti2017">Amarjyoti (<a href="8-References.html#ref-Amarjyoti2017">2017</a>)</span> 
<br>
<div class="arrows">
<a href="5-ModelBased.html" class="previous">&laquo; Previous</a>
<a href="7-Practice.html" class="next">Next &raquo;</a>
</div>


</article>
</body>
</html>
