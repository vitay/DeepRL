<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Julien Vitay">
  <title>Deep Reinforcement Learning</title>


  <link rel="stylesheet" href="assets/syntax.css">
  <link rel="stylesheet" href="assets/github.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->

</head>

<body class="markdown-body">

<header>
  <h1 class="title"><strong>Deep Reinforcement Learning</strong></h1>
  <p class="author">Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de">julien.vitay@informatik.tu-chemnitz.de</a></p>
</header>


<article>

<nav id="TOC" class ="toc">
  <h2><strong>Deep Reinforcement Learning</strong></h2>
  <p class="author">Julien Vitay</p>

<ul>
<li><a href="./Introduction.html#sec:introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="./BasicRL.html#sec:basics"><span class="toc-section-number">2</span> Basics</a><ul>
<li><a href="./BasicRL.html#sec:reinforcement-learning-and-markov-decision-process"><span class="toc-section-number">2.1</span> Reinforcement learning and Markov Decision Process</a><ul>
<li><a href="./BasicRL.html#sec:policy-and-value-functions"><span class="toc-section-number">2.1.1</span> Policy and value functions</a></li>
<li><a href="./BasicRL.html#sec:bellman-equations"><span class="toc-section-number">2.1.2</span> Bellman equations</a></li>
<li><a href="./BasicRL.html#sec:dynamic-programming"><span class="toc-section-number">2.1.3</span> Dynamic programming</a></li>
<li><a href="./BasicRL.html#sec:monte-carlo-sampling"><span class="toc-section-number">2.1.4</span> Monte-Carlo sampling</a></li>
<li><a href="./BasicRL.html#sec:temporal-difference"><span class="toc-section-number">2.1.5</span> Temporal Difference</a></li>
<li><a href="./BasicRL.html#sec:eligibility-traces"><span class="toc-section-number">2.1.6</span> Eligibility traces</a></li>
<li><a href="./BasicRL.html#sec:actor-critic-architectures"><span class="toc-section-number">2.1.7</span> Actor-critic architectures</a></li>
<li><a href="./BasicRL.html#sec:function-approximation"><span class="toc-section-number">2.1.8</span> Function approximation</a></li>
</ul></li>
<li><a href="./DeepLearning.html#sec:deep-learning"><span class="toc-section-number">2.2</span> Deep learning</a><ul>
<li><a href="./DeepLearning.html#sec:deep-neural-networks"><span class="toc-section-number">2.2.1</span> Deep neural networks</a></li>
<li><a href="./DeepLearning.html#sec:convolutional-networks"><span class="toc-section-number">2.2.2</span> Convolutional networks</a></li>
<li><a href="./DeepLearning.html#sec:recurrent-neural-networks"><span class="toc-section-number">2.2.3</span> Recurrent neural networks</a></li>
</ul></li>
</ul></li>
<li><a href="./Valuebased.html#sec:value-based-methods"><span class="toc-section-number">3</span> Value-based methods</a><ul>
<li><a href="./Valuebased.html#sec:limitations-of-deep-neural-networks-for-function-approximation"><span class="toc-section-number">3.1</span> Limitations of deep neural networks for function approximation</a></li>
<li><a href="./Valuebased.html#sec:deep-q-network-dqn"><span class="toc-section-number">3.2</span> Deep Q-Network (DQN)</a></li>
<li><a href="./Valuebased.html#sec:double-dqn"><span class="toc-section-number">3.3</span> Double DQN</a></li>
<li><a href="./Valuebased.html#sec:prioritized-experience-replay"><span class="toc-section-number">3.4</span> Prioritized experience replay</a></li>
<li><a href="./Valuebased.html#sec:duelling-network"><span class="toc-section-number">3.5</span> Duelling network</a></li>
<li><a href="./Valuebased.html#sec:distributed-dqn-gorila"><span class="toc-section-number">3.6</span> Distributed DQN (GORILA)</a></li>
<li><a href="./Valuebased.html#sec:deep-recurrent-q-learning-drqn"><span class="toc-section-number">3.7</span> Deep Recurrent Q-learning (DRQN)</a></li>
<li><a href="./Valuebased.html#sec:other-variants-of-dqn"><span class="toc-section-number">3.8</span> Other variants of DQN</a></li>
</ul></li>
<li><a href="./PolicyGradient.html#sec:policy-gradient-methods"><span class="toc-section-number">4</span> Policy Gradient methods</a><ul>
<li><a href="./PolicyGradient.html#sec:reinforce"><span class="toc-section-number">4.1</span> REINFORCE</a><ul>
<li><a href="./PolicyGradient.html#sec:estimating-the-policy-gradient"><span class="toc-section-number">4.1.1</span> Estimating the policy gradient</a></li>
<li><a href="./PolicyGradient.html#sec:reducing-the-variance"><span class="toc-section-number">4.1.2</span> Reducing the variance</a></li>
<li><a href="./PolicyGradient.html#sec:policy-gradient-theorem"><span class="toc-section-number">4.1.3</span> Policy Gradient theorem</a></li>
</ul></li>
<li><a href="./ActorCritic.html#sec:advantage-actor-critic-methods"><span class="toc-section-number">4.2</span> Advantage Actor-Critic methods</a><ul>
<li><a href="./ActorCritic.html#sec:advantage-actor-critic-a2c"><span class="toc-section-number">4.2.1</span> Advantage Actor-Critic (A2C)</a></li>
<li><a href="./ActorCritic.html#sec:asynchronous-advantage-actor-critic-a3c"><span class="toc-section-number">4.2.2</span> Asynchronous Advantage Actor-Critic (A3C)</a></li>
<li><a href="./ActorCritic.html#sec:generalized-advantage-estimation-gae"><span class="toc-section-number">4.2.3</span> Generalized Advantage Estimation (GAE)</a></li>
<li><a href="./ActorCritic.html#sec:stochastic-actor-critic-for-continuous-action-spaces"><span class="toc-section-number">4.2.4</span> Stochastic actor-critic for continuous action spaces</a></li>
</ul></li>
<li><a href="./ImportanceSampling.html#sec:off-policy-actor-critic"><span class="toc-section-number">4.3</span> Off-policy Actor-Critic</a><ul>
<li><a href="./ImportanceSampling.html#sec:importance-sampling"><span class="toc-section-number">4.3.1</span> Importance sampling</a></li>
<li><a href="./ImportanceSampling.html#sec:linear-off-policy-actor-critic-off-pac"><span class="toc-section-number">4.3.2</span> Linear Off-Policy Actor-Critic (Off-PAC)</a></li>
<li><a href="./ImportanceSampling.html#sec:retrace"><span class="toc-section-number">4.3.3</span> Retrace</a></li>
<li><a href="./ImportanceSampling.html#sec:self-imitation-learning-sil"><span class="toc-section-number">4.3.4</span> Self-Imitation Learning (SIL)</a></li>
</ul></li>
<li><a href="./DPG.html#sec:deterministic-policy-gradient-dpg"><span class="toc-section-number">4.4</span> Deterministic Policy Gradient (DPG)</a><ul>
<li><a href="./DPG.html#sec:deterministic-policy-gradient-theorem"><span class="toc-section-number">4.4.1</span> Deterministic policy gradient theorem</a></li>
<li><a href="./DPG.html#sec:deep-deterministic-policy-gradient-ddpg"><span class="toc-section-number">4.4.2</span> Deep Deterministic Policy Gradient (DDPG)</a></li>
<li><a href="./DPG.html#sec:distributed-distributional-ddpg-d4pg"><span class="toc-section-number">4.4.3</span> Distributed Distributional DDPG (D4PG)</a></li>
</ul></li>
<li><a href="./NaturalGradient.html#sec:natural-gradients"><span class="toc-section-number">4.5</span> Natural Gradients</a><ul>
<li><a href="./NaturalGradient.html#sec:natural-actor-critic-nac"><span class="toc-section-number">4.5.1</span> Natural Actor Critic (NAC)</a></li>
<li><a href="./NaturalGradient.html#sec:trust-region-policy-optimization-trpo"><span class="toc-section-number">4.5.2</span> Trust Region Policy Optimization (TRPO)</a></li>
<li><a href="./NaturalGradient.html#sec:proximal-policy-optimization-ppo"><span class="toc-section-number">4.5.3</span> Proximal Policy Optimization (PPO)</a></li>
<li><a href="./NaturalGradient.html#sec:actor-critic-with-experience-replay-acer"><span class="toc-section-number">4.5.4</span> Actor-Critic with Experience Replay (ACER)</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:distributional-learning"><span class="toc-section-number">4.6</span> Distributional learning</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:the-reactor"><span class="toc-section-number">4.6.1</span> The Reactor</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:entropy-based-rl"><span class="toc-section-number">4.7</span> Entropy-based RL</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:soft-actor-critic-sac"><span class="toc-section-number">4.7.1</span> Soft Actor-Critic (SAC)</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:other-policy-search-methods"><span class="toc-section-number">4.8</span> Other policy search methods</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:stochastic-value-gradient-svg"><span class="toc-section-number">4.8.1</span> Stochastic Value Gradient (SVG)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:q-prop"><span class="toc-section-number">4.8.2</span> Q-Prop</a></li>
<li><a href="./OtherPolicyGradient.html#sec:normalized-advantage-function-naf"><span class="toc-section-number">4.8.3</span> Normalized Advantage Function (NAF)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:fictitious-self-play-fsp"><span class="toc-section-number">4.8.4</span> Fictitious Self-Play (FSP)</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:comparison-between-value-based-and-policy-gradient-methods"><span class="toc-section-number">4.9</span> Comparison between value-based and policy gradient methods</a></li>
<li><a href="./OtherPolicyGradient.html#sec:gradient-free-policy-search"><span class="toc-section-number">4.10</span> Gradient-free policy search</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:cross-entropy-method-cem"><span class="toc-section-number">4.10.1</span> Cross-entropy Method (CEM)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:evolutionary-search-es"><span class="toc-section-number">4.10.2</span> Evolutionary Search (ES)</a></li>
</ul></li>
</ul></li>
<li><a href="./Practice.html#sec:deep-rl-in-practice"><span class="toc-section-number">5</span> Deep RL in practice</a><ul>
<li><a href="./Practice.html#sec:limitations"><span class="toc-section-number">5.1</span> Limitations</a></li>
<li><a href="./Practice.html#sec:reward-shaping"><span class="toc-section-number">5.2</span> Reward shaping</a></li>
<li><a href="./Practice.html#sec:simulation-environments"><span class="toc-section-number">5.3</span> Simulation environments</a></li>
<li><a href="./Practice.html#sec:algorithm-implementations"><span class="toc-section-number">5.4</span> Algorithm implementations</a></li>
</ul></li>
<li><a href="./References.html#sec:references">References</a></li>
</ul>


</nav>



<h1 id="sec:references" class="unnumbered">References</h1>
<div id="refs" class="references">
<div id="ref-Andrychowicz2017">
<p>Andrychowicz, Marcin, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. 2017. “Hindsight Experience Replay,” July. <a href="http://arxiv.org/abs/1707.01495" class="uri">http://arxiv.org/abs/1707.01495</a>.</p>
</div>
<div id="ref-Anschel2016">
<p>Anschel, Oron, Nir Baram, and Nahum Shimkin. 2016. “Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning,” November. <a href="http://arxiv.org/abs/1611.01929" class="uri">http://arxiv.org/abs/1611.01929</a>.</p>
</div>
<div id="ref-Arulkumaran2017">
<p>Arulkumaran, Kai, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. 2017. “A Brief Survey of Deep Reinforcement Learning.” <a href="https://arxiv.org/pdf/1708.05866.pdf" class="uri">https://arxiv.org/pdf/1708.05866.pdf</a>.</p>
</div>
<div id="ref-Baird1993">
<p>Baird, L.C. 1993. “Advantage updating.” Technical report WL- TR-93-1146, Wright-Patterson Air Force Base.</p>
</div>
<div id="ref-Bakker2001">
<p>Bakker, Bram. 2001. “Reinforcement Learning with Long Short-Term Memory.” In <em>Adv. Neural Inf. Process. Syst. 14 (Nips 2001)</em>, 1475–82. <a href="https://papers.nips.cc/paper/1953-reinforcement-learning-with-long-short-term-memory" class="uri">https://papers.nips.cc/paper/1953-reinforcement-learning-with-long-short-term-memory</a>.</p>
</div>
<div id="ref-BarthMaron2018">
<p>Barth-Maron, Gabriel, Matthew W. Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva TB, Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. 2018. “Distributed Distributional Deterministic Policy Gradients,” April. <a href="http://arxiv.org/abs/1804.08617" class="uri">http://arxiv.org/abs/1804.08617</a>.</p>
</div>
<div id="ref-Bellemare2017">
<p>Bellemare, Marc G., Will Dabney, and Rémi Munos. 2017. “A Distributional Perspective on Reinforcement Learning,” July. <a href="http://arxiv.org/abs/1707.06887" class="uri">http://arxiv.org/abs/1707.06887</a>.</p>
</div>
<div id="ref-Cho2014">
<p>Cho, Kyunghyun, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. “Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation,” June. <a href="http://arxiv.org/abs/1406.1078" class="uri">http://arxiv.org/abs/1406.1078</a>.</p>
</div>
<div id="ref-Chou2017">
<p>Chou, Po-Wei, Daniel Maturana, and Sebastian Scherer. 2017. “Improving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning using the Beta Distribution.” In <em>Int. Conf. Mach. Learn.</em> <a href="http://proceedings.mlr.press/v70/chou17a/chou17a.pdf" class="uri">http://proceedings.mlr.press/v70/chou17a/chou17a.pdf</a>.</p>
</div>
<div id="ref-Degris2012">
<p>Degris, Thomas, Martha White, and Richard S. Sutton. 2012. “Linear Off-Policy Actor-Critic.” In <em>Proc. 2012 Int. Conf. Mach. Learn.</em> <a href="http://arxiv.org/abs/1205.4839" class="uri">http://arxiv.org/abs/1205.4839</a>.</p>
</div>
<div id="ref-Duan2016">
<p>Duan, Yan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. 2016. “Benchmarking Deep Reinforcement Learning for Continuous Control,” April. <a href="http://arxiv.org/abs/1604.06778" class="uri">http://arxiv.org/abs/1604.06778</a>.</p>
</div>
<div id="ref-Gers2001">
<p>Gers, Felix. 2001. “Long Short-Term Memory in Recurrent Neural Networks.” PhD Thesis, Ecole Polytechnique Fédérale de Lausanne. <a href="http://www.felixgers.de/papers/phd.pdf" class="uri">http://www.felixgers.de/papers/phd.pdf</a>.</p>
</div>
<div id="ref-Goodfellow2016">
<p>Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. <em>Deep Learning</em>. MIT Press. <a href="http://www.deeplearningbook.org" class="uri">http://www.deeplearningbook.org</a>.</p>
</div>
<div id="ref-Gruslys2017">
<p>Gruslys, Audrunas, Will Dabney, Mohammad Gheshlaghi Azar, Bilal Piot, Marc Bellemare, and Remi Munos. 2017. “The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning,” April. <a href="http://arxiv.org/abs/1704.04651" class="uri">http://arxiv.org/abs/1704.04651</a>.</p>
</div>
<div id="ref-Gu2017">
<p>Gu, Shixiang, Ethan Holly, Timothy Lillicrap, and Sergey Levine. 2017. “Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates.” In <em>Proc. ICRA</em>. <a href="http://arxiv.org/abs/1610.00633" class="uri">http://arxiv.org/abs/1610.00633</a>.</p>
</div>
<div id="ref-Gu2016">
<p>Gu, Shixiang, Timothy Lillicrap, Zoubin Ghahramani, Richard E. Turner, and Sergey Levine. 2016. “Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic,” November. <a href="http://arxiv.org/abs/1611.02247" class="uri">http://arxiv.org/abs/1611.02247</a>.</p>
</div>
<div id="ref-Gu2016b">
<p>Gu, Shixiang, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. 2016. “Continuous Deep Q-Learning with Model-based Acceleration,” March. <a href="http://arxiv.org/abs/1603.00748" class="uri">http://arxiv.org/abs/1603.00748</a>.</p>
</div>
<div id="ref-Haarnoya2018b">
<p>Haarnoja, Tuomas, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,” January. <a href="http://arxiv.org/abs/1801.01290" class="uri">http://arxiv.org/abs/1801.01290</a>.</p>
</div>
<div id="ref-Hafner2011">
<p>Hafner, Roland, and Martin Riedmiller. 2011. “Reinforcement learning in feedback control.” <em>Mach. Learn.</em> 84 (1-2). Springer US:137–69. <a href="https://doi.org/10.1007/s10994-011-5235-x" class="uri">https://doi.org/10.1007/s10994-011-5235-x</a>.</p>
</div>
<div id="ref-Harutyunyan2016">
<p>Harutyunyan, A., M. G. Bellemare, T. Stepleton, and R. Munos. 2016. “Q(<span class="math inline">\(\lambda\)</span>) with off-policy corrections.” <a href="http://arxiv.org/abs/1602.04951" class="uri">http://arxiv.org/abs/1602.04951</a>.</p>
</div>
<div id="ref-vanHasselt2010">
<p>Hasselt, Hado van. 2010. “Double Q-learning.” In <em>Proc. 23rd Int. Conf. Neural Inf. Process. Syst. - Vol. 2</em>, 2613–21. Curran Associates Inc. <a href="https://dl.acm.org/citation.cfm?id=2997187" class="uri">https://dl.acm.org/citation.cfm?id=2997187</a>.</p>
</div>
<div id="ref-vanHasselt2015">
<p>Hasselt, Hado van, Arthur Guez, and David Silver. 2015. “Deep Reinforcement Learning with Double Q-learning,” September. <a href="http://arxiv.org/abs/1509.06461" class="uri">http://arxiv.org/abs/1509.06461</a>.</p>
</div>
<div id="ref-Hausknecht2015">
<p>Hausknecht, Matthew, and Peter Stone. 2015. “Deep Recurrent Q-Learning for Partially Observable MDPs,” July. <a href="http://arxiv.org/abs/1507.06527" class="uri">http://arxiv.org/abs/1507.06527</a>.</p>
</div>
<div id="ref-He2016">
<p>He, Frank S., Yang Liu, Alexander G. Schwing, and Jian Peng. 2016. “Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening,” November. <a href="http://arxiv.org/abs/1611.01606" class="uri">http://arxiv.org/abs/1611.01606</a>.</p>
</div>
<div id="ref-He2015">
<p>He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Deep Residual Learning for Image Recognition,” December. <a href="http://arxiv.org/abs/1512.03385" class="uri">http://arxiv.org/abs/1512.03385</a>.</p>
</div>
<div id="ref-Heess2015">
<p>Heess, Nicolas, Greg Wayne, David Silver, Timothy Lillicrap, Yuval Tassa, and Tom Erez. 2015. “Learning continuous control policies by stochastic value gradients.” MIT Press. <a href="http://dl.acm.org/citation.cfm?id=2969569" class="uri">http://dl.acm.org/citation.cfm?id=2969569</a>.</p>
</div>
<div id="ref-Heinrich2015">
<p>Heinrich, Johannes, Marc Lanctot, and David Silver. 2015. “Fictitious Self-Play in Extensive-Form Games.” <a href="http://proceedings.mlr.press/v37/heinrich15.html" class="uri">http://proceedings.mlr.press/v37/heinrich15.html</a>.</p>
</div>
<div id="ref-Heinrich2016">
<p>Heinrich, Johannes, and David Silver. 2016. “Deep Reinforcement Learning from Self-Play in Imperfect-Information Games,” March. <a href="http://arxiv.org/abs/1603.01121" class="uri">http://arxiv.org/abs/1603.01121</a>.</p>
</div>
<div id="ref-Hessel2017">
<p>Hessel, Matteo, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. 2017. “Rainbow: Combining Improvements in Deep Reinforcement Learning,” October. <a href="http://arxiv.org/abs/1710.02298" class="uri">http://arxiv.org/abs/1710.02298</a>.</p>
</div>
<div id="ref-Hochreiter1991">
<p>Hochreiter, Sepp. 1991. “Untersuchungen zu dynamischen neuronalen Netzen.” Diploma thesis, TU Munich. <a href="http://people.idsia.ch/{~}juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf" class="uri">http://people.idsia.ch/{~}juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf</a>.</p>
</div>
<div id="ref-Hochreiter1997">
<p>Hochreiter, Sepp, and J?rgen Schmidhuber. 1997. “Long Short-Term Memory.” <em>Neural Comput.</em> 9 (8). MIT Press:1735–80. <a href="https://doi.org/10.1162/neco.1997.9.8.1735" class="uri">https://doi.org/10.1162/neco.1997.9.8.1735</a>.</p>
</div>
<div id="ref-Ioffe2015">
<p>Ioffe, Sergey, and Christian Szegedy. 2015. “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,” February. <a href="http://arxiv.org/abs/1502.03167" class="uri">http://arxiv.org/abs/1502.03167</a>.</p>
</div>
<div id="ref-Kakade2001">
<p>Kakade, Sham. 2001. “A Natural Policy Gradient.” In <em>Adv. Neural Inf. Process. Syst. 14</em>. <a href="https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf" class="uri">https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf</a>.</p>
</div>
<div id="ref-Kingma2013">
<p>Kingma, Diederik P, and Max Welling. 2013. “Auto-Encoding Variational Bayes,” December. <a href="http://arxiv.org/abs/1312.6114" class="uri">http://arxiv.org/abs/1312.6114</a>.</p>
</div>
<div id="ref-Krizhevsky2012">
<p>Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. “ImageNet Classification with Deep Convolutional Neural Networks.” In <em>Adv. Neural Inf. Process. Syst.</em> <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" class="uri">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a>.</p>
</div>
<div id="ref-Levine2013">
<p>Levine, Sergey, and Vladlen Koltun. 2013. “Guided Policy Search.” In <em>Proc. Mach. Learn. Res.</em>, 1–9. <a href="http://proceedings.mlr.press/v28/levine13.html" class="uri">http://proceedings.mlr.press/v28/levine13.html</a>.</p>
</div>
<div id="ref-Li2017">
<p>Li, Yuxi. 2017. “Deep Reinforcement Learning: An Overview,” January. <a href="http://arxiv.org/abs/1701.07274" class="uri">http://arxiv.org/abs/1701.07274</a>.</p>
</div>
<div id="ref-Lillicrap2015">
<p>Lillicrap, Timothy P., Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2015. “Continuous control with deep reinforcement learning.” <em>CoRR</em>. <a href="http://arxiv.org/abs/1509.02971" class="uri">http://arxiv.org/abs/1509.02971</a>.</p>
</div>
<div id="ref-Loetzsch2017">
<p>Lötzsch, Winfried, Julien Vitay, and Fred H. Hamker. 2017. “Training a deep policy gradient-based neural network with asynchronous learners on a simulated robotic problem.” In <em>Inform. 2017. Gesellschaft Für Inform.</em>, edited by Maximilian Eibl and Martin Gaedke, 2143–54. Bonn: Gesellschaft für Informatik, Bonn. <a href="https://dl.gi.de/handle/20.500.12116/3986" class="uri">https://dl.gi.de/handle/20.500.12116/3986</a>.</p>
</div>
<div id="ref-Meuleau2000">
<p>Meuleau, Nicolas, Leonid Peshkin, Leslie P. Kaelbling, and Kee-eung Kim. 2000. “Off-Policy Policy Search.” MIT AI Lab. <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.894" class="uri">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.894</a>.</p>
</div>
<div id="ref-Mirowski2016">
<p>Mirowski, Piotr, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew J. Ballard, Andrea Banino, Misha Denil, et al. 2016. “Learning to Navigate in Complex Environments,” November. <a href="http://arxiv.org/abs/1611.03673" class="uri">http://arxiv.org/abs/1611.03673</a>.</p>
</div>
<div id="ref-Mnih2016">
<p>Mnih, Volodymyr, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. “Asynchronous Methods for Deep Reinforcement Learning.” In <em>Proc. ICML</em>. <a href="http://arxiv.org/abs/1602.01783" class="uri">http://arxiv.org/abs/1602.01783</a>.</p>
</div>
<div id="ref-Mnih2013">
<p>Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. “Playing Atari with Deep Reinforcement Learning,” December. <a href="http://arxiv.org/abs/1312.5602" class="uri">http://arxiv.org/abs/1312.5602</a>.</p>
</div>
<div id="ref-Mnih2015">
<p>Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, et al. 2015. “Human-level control through deep reinforcement learning.” <em>Nature</em> 518 (7540). Nature Publishing Group:529–33. <a href="https://doi.org/10.1038/nature14236" class="uri">https://doi.org/10.1038/nature14236</a>.</p>
</div>
<div id="ref-Mousavi2018">
<p>Mousavi, Seyed Sajad, Michael Schukat, and Enda Howley. 2018. “Deep Reinforcement Learning: An Overview.” In, 426–40. Springer, Cham. <a href="https://doi.org/10.1007/978-3-319-56991-8_32" class="uri">https://doi.org/10.1007/978-3-319-56991-8_32</a>.</p>
</div>
<div id="ref-Munos2016">
<p>Munos, Rémi, Tom Stepleton, Anna Harutyunyan, and Marc G. Bellemare. 2016. “Safe and Efficient Off-Policy Reinforcement Learning,” June. <a href="http://arxiv.org/abs/1606.02647" class="uri">http://arxiv.org/abs/1606.02647</a>.</p>
</div>
<div id="ref-Nair2015">
<p>Nair, Arun, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon, Alessandro De Maria, Vedavyas Panneershelvam, et al. 2015. “Massively Parallel Methods for Deep Reinforcement Learning.” <a href="https://arxiv.org/pdf/1507.04296.pdf" class="uri">https://arxiv.org/pdf/1507.04296.pdf</a>.</p>
</div>
<div id="ref-Nielsen2015">
<p>Nielsen, Michael A. 2015. <em>Neural Networks and Deep Learning</em>. Determination Press. <a href="http://neuralnetworksanddeeplearning.com/" class="uri">http://neuralnetworksanddeeplearning.com/</a>.</p>
</div>
<div id="ref-Niu2011">
<p>Niu, Feng, Benjamin Recht, Christopher Re, and Stephen J. Wright. 2011. “HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent.” In <em>Proc. Adv. Neural Inf. Process. Syst.</em>, 21. 1. <a href="http://arxiv.org/abs/1106.5730" class="uri">http://arxiv.org/abs/1106.5730</a>.</p>
</div>
<div id="ref-Oh2018">
<p>Oh, Junhyuk, Yijie Guo, Satinder Singh, and Honglak Lee. 2018. “Self-Imitation Learning,” June. <a href="http://arxiv.org/abs/1806.05635" class="uri">http://arxiv.org/abs/1806.05635</a>.</p>
</div>
<div id="ref-Peshkin2002">
<p>Peshkin, Leonid, and Christian R. Shelton. 2002. “Learning from Scarce Experience,” April. <a href="http://arxiv.org/abs/cs/0204043" class="uri">http://arxiv.org/abs/cs/0204043</a>.</p>
</div>
<div id="ref-Peters2008">
<p>Peters, Jan, and Stefan Schaal. 2008. “Reinforcement learning of motor skills with policy gradients.” <em>Neural Networks</em> 21 (4):682–97. <a href="https://doi.org/10.1016/j.neunet.2008.02.003" class="uri">https://doi.org/10.1016/j.neunet.2008.02.003</a>.</p>
</div>
<div id="ref-Pong2018">
<p>Pong, Vitchyr, Shixiang Gu, Murtaza Dalal, and Sergey Levine. 2018. “Temporal Difference Models: Model-Free Deep RL for Model-Based Control,” February. <a href="http://arxiv.org/abs/1802.09081" class="uri">http://arxiv.org/abs/1802.09081</a>.</p>
</div>
<div id="ref-Popov2017">
<p>Popov, Ivaylo, Nicolas Heess, Timothy Lillicrap, Roland Hafner, Gabriel Barth-Maron, Matej Vecerik, Thomas Lampe, Yuval Tassa, Tom Erez, and Martin Riedmiller. 2017. “Data-efficient Deep Reinforcement Learning for Dexterous Manipulation,” April. <a href="http://arxiv.org/abs/1704.03073" class="uri">http://arxiv.org/abs/1704.03073</a>.</p>
</div>
<div id="ref-Precup2000">
<p>Precup, D., R. S Sutton, and S. Singh. 2000. “Eligibility traces for off-policy policy evaluation.” In <em>Proc. Seventeenth Int. Conf. Mach. Learn.</em></p>
</div>
<div id="ref-Ruder2016">
<p>Ruder, Sebastian. 2016. “An overview of gradient descent optimization algorithms,” September. <a href="http://arxiv.org/abs/1609.04747" class="uri">http://arxiv.org/abs/1609.04747</a>.</p>
</div>
<div id="ref-Salimans2017">
<p>Salimans, Tim, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. 2017. “Evolution Strategies as a Scalable Alternative to Reinforcement Learning,” March. <a href="http://arxiv.org/abs/1703.03864" class="uri">http://arxiv.org/abs/1703.03864</a>.</p>
</div>
<div id="ref-Schaul2015">
<p>Schaul, Tom, John Quan, Ioannis Antonoglou, and David Silver. 2015. “Prioritized Experience Replay,” November. <a href="http://arxiv.org/abs/1511.05952" class="uri">http://arxiv.org/abs/1511.05952</a>.</p>
</div>
<div id="ref-Schulman2015a">
<p>Schulman, John, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. 2015. “Trust Region Policy Optimization.” <a href="http://proceedings.mlr.press/v37/schulman15.html" class="uri">http://proceedings.mlr.press/v37/schulman15.html</a>.</p>
</div>
<div id="ref-Schulman2015b">
<p>Schulman, John, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. 2015. “High-Dimensional Continuous Control Using Generalized Advantage Estimation,” June. <a href="http://arxiv.org/abs/1506.02438" class="uri">http://arxiv.org/abs/1506.02438</a>.</p>
</div>
<div id="ref-Schulman2017">
<p>Schulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. “Proximal Policy Optimization Algorithms,” July. <a href="http://arxiv.org/abs/1707.06347" class="uri">http://arxiv.org/abs/1707.06347</a>.</p>
</div>
<div id="ref-Silver2016">
<p>Silver, David, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, et al. 2016. “Mastering the game of Go with deep neural networks and tree search.” <em>Nature</em>. <a href="https://doi.org/10.1038/nature16961" class="uri">https://doi.org/10.1038/nature16961</a>.</p>
</div>
<div id="ref-Silver2014">
<p>Silver, David, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. 2014. “Deterministic Policy Gradient Algorithms.” In <em>Proc. ICML</em>, edited by Eric P Xing and Tony Jebara, 32:387–95. Proceedings of Machine Learning Research 1. Bejing, China: PMLR. <a href="http://proceedings.mlr.press/v32/silver14.html" class="uri">http://proceedings.mlr.press/v32/silver14.html</a>.</p>
</div>
<div id="ref-Simonyan2014">
<p>Simonyan, Karen, and Andrew Zisserman. 2014. “Very Deep Convolutional Networks for Large-Scale Image Recognition,” September. <a href="http://arxiv.org/abs/1409.1556" class="uri">http://arxiv.org/abs/1409.1556</a>.</p>
</div>
<div id="ref-Sutton1998">
<p>Sutton, R. S., and A. G. Barto. 1998. <em>Reinforcement Learning: An introduction</em>. Cambridge, MA: MIT press.</p>
</div>
<div id="ref-Sutton2017">
<p>———. 2017. <em>Reinforcement Learning: An Introduction</em>. 2nd ed. Cambridge, MA: MIT Press. <a href="http://incompleteideas.net/sutton/book/the-book-2nd.html" class="uri">http://incompleteideas.net/sutton/book/the-book-2nd.html</a>.</p>
</div>
<div id="ref-Sutton1999">
<p>Sutton, R. S., D. A McAllester, S. P. Singh, and Y. Mansour. 1999. “Policy Gradient Methods for Reinforcement Learning with Function Approximation.” In <em>Neural Inf. Process. Syst. 12</em>, 1057–63.</p>
</div>
<div id="ref-Szita2006">
<p>Szita, István, and András Lörincz. 2006. “Learning Tetris Using the Noisy Cross-Entropy Method.” <em>Neural Comput.</em> 18 (12). MIT Press 238 Main St., Suite 500, Cambridge, MA 02142-1046 USA journals-info@mit.edu:2936–41. <a href="https://doi.org/10.1162/neco.2006.18.12.2936" class="uri">https://doi.org/10.1162/neco.2006.18.12.2936</a>.</p>
</div>
<div id="ref-Tang2010">
<p>Tang, Jie, and Pieter Abbeel. 2010. “On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient.” In <em>Adv. Neural Inf. Process. Syst.</em> <a href="http://rll.berkeley.edu/{~}jietang/pubs/nips10{\_}Tang.pdf" class="uri">http://rll.berkeley.edu/{~}jietang/pubs/nips10{\_}Tang.pdf</a>.</p>
</div>
<div id="ref-Ornstein1930">
<p>Uhlenbeck, G. E., and L.S Ornstein. 1930. “On the Theory of the Brownian Motion.” <em>Phys. Rev.</em> 36. <a href="https://doi.org/10.1103/PhysRev.36.823" class="uri">https://doi.org/10.1103/PhysRev.36.823</a>.</p>
</div>
<div id="ref-Wang2017">
<p>Wang, Ziyu, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, and Nando de Freitas. 2017. “Sample Efficient Actor-Critic with Experience Replay,” November. <a href="http://arxiv.org/abs/1611.01224" class="uri">http://arxiv.org/abs/1611.01224</a>.</p>
</div>
<div id="ref-Wang2016">
<p>Wang, Ziyu, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando de Freitas. 2016. “Dueling Network Architectures for Deep Reinforcement Learning.” In <em>Proc. ICML</em>. New York, NY, USA. <a href="http://arxiv.org/abs/1511.06581" class="uri">http://arxiv.org/abs/1511.06581</a>.</p>
</div>
<div id="ref-Watkins1989">
<p>Watkins, Christopher JCH. 1989. “Learning from delayed rewards.” PhD thesis, University of Cambridge, England.</p>
</div>
<div id="ref-Wierstra2007">
<p>Wierstra, Daan, Alexander Foerster, Jan Peters, and Jürgen Schmidhuber. 2007. “Solving Deep Memory POMDPs with Recurrent Policy Gradients.” In, 697–706. Springer, Berlin, Heidelberg. <a href="https://doi.org/10.1007/978-3-540-74690-4_71" class="uri">https://doi.org/10.1007/978-3-540-74690-4_71</a>.</p>
</div>
<div id="ref-Williams1992">
<p>Williams, R. J. 1992. “Simple statistical gradient-following algorithms for connectionist reinforcement learning.” <em>Mach. Learn.</em> 8:229–56.</p>
</div>
<div id="ref-Williams1991">
<p>Williams, Ronald J, and Jing Peng. 1991. “Function optimization using connectionist reinforcement learning algorithms.” <em>Conn. Sci.</em> 3 (3):241–68.</p>
</div>
</div>

<br>
<div class="arrows">
<a href="Practice.html" class="previous">&laquo; Previous</a>
<a href="#" class="next">Next &raquo;</a>
</div>


</article>
</body>
</html>
