<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Julien Vitay">
  <title>Deep Reinforcement Learning</title>


  <link rel="stylesheet" href="assets/syntax.css">
  <link rel="stylesheet" href="assets/github.css">
  <script src="/usr/share/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->

</head>

<body class="markdown-body">

<header>
  <h1 class="title"><strong>Deep Reinforcement Learning</strong></h1>
  <p class="author">Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de">julien.vitay@informatik.tu-chemnitz.de</a></p>
</header>


<article>

<nav id="TOC" class ="toc">
  <h2><strong>Deep Reinforcement Learning</strong></h2>
  <p class="author">Julien Vitay</p>

<ul>
<li><a href="./0-Introduction.html#sec:introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="./1.1-BasicRL.html#sec:basics"><span class="toc-section-number">2</span> Basics</a><ul>
<li><a href="./1.1-BasicRL.html#sec:reinforcement-learning-and-markov-decision-process"><span class="toc-section-number">2.1</span> Reinforcement learning and Markov Decision Process</a><ul>
<li><a href="./1.1-BasicRL.html#sec:policy-and-value-functions"><span class="toc-section-number">2.1.1</span> Policy and value functions</a></li>
<li><a href="./1.1-BasicRL.html#sec:bellman-equations"><span class="toc-section-number">2.1.2</span> Bellman equations</a></li>
<li><a href="./1.1-BasicRL.html#sec:dynamic-programming"><span class="toc-section-number">2.1.3</span> Dynamic programming</a></li>
<li><a href="./1.1-BasicRL.html#sec:monte-carlo-sampling"><span class="toc-section-number">2.1.4</span> Monte-Carlo sampling</a></li>
<li><a href="./1.1-BasicRL.html#sec:temporal-difference"><span class="toc-section-number">2.1.5</span> Temporal Difference</a></li>
<li><a href="./1.1-BasicRL.html#sec:actor-critic-architectures"><span class="toc-section-number">2.1.6</span> Actor-critic architectures</a></li>
<li><a href="./1.1-BasicRL.html#sec:function-approximation"><span class="toc-section-number">2.1.7</span> Function approximation</a></li>
</ul></li>
<li><a href="./1.2-DeepLearning.html#sec:deep-learning"><span class="toc-section-number">2.2</span> Deep learning</a><ul>
<li><a href="./1.2-DeepLearning.html#sec:deep-neural-networks"><span class="toc-section-number">2.2.1</span> Deep neural networks</a></li>
<li><a href="./1.2-DeepLearning.html#sec:convolutional-networks"><span class="toc-section-number">2.2.2</span> Convolutional networks</a></li>
<li><a href="./1.2-DeepLearning.html#sec:recurrent-neural-networks"><span class="toc-section-number">2.2.3</span> Recurrent neural networks</a></li>
</ul></li>
</ul></li>
<li><a href="./2-Valuebased.html#sec:value-based-methods"><span class="toc-section-number">3</span> Value-based methods</a><ul>
<li><a href="./2-Valuebased.html#sec:limitations-of-deep-neural-networks-for-function-approximation"><span class="toc-section-number">3.1</span> Limitations of deep neural networks for function approximation</a></li>
<li><a href="./2-Valuebased.html#sec:deep-q-network-dqn"><span class="toc-section-number">3.2</span> Deep Q-Network (DQN)</a></li>
<li><a href="./2-Valuebased.html#sec:double-dqn"><span class="toc-section-number">3.3</span> Double DQN</a></li>
<li><a href="./2-Valuebased.html#sec:prioritised-replay"><span class="toc-section-number">3.4</span> Prioritised replay</a></li>
<li><a href="./2-Valuebased.html#sec:duelling-network"><span class="toc-section-number">3.5</span> Duelling network</a></li>
<li><a href="./2-Valuebased.html#sec:distributed-dqn-gorila"><span class="toc-section-number">3.6</span> Distributed DQN (GORILA)</a></li>
<li><a href="./2-Valuebased.html#sec:deep-recurrent-q-learning-drqn"><span class="toc-section-number">3.7</span> Deep Recurrent Q-learning (DRQN)</a></li>
<li><a href="./2-Valuebased.html#sec:other-variants-of-dqn"><span class="toc-section-number">3.8</span> Other variants of DQN</a></li>
</ul></li>
<li><a href="./3.1-PolicyGradient.html#sec:policy-gradient-methods"><span class="toc-section-number">4</span> Policy Gradient methods</a><ul>
<li><a href="./3.1-PolicyGradient.html#sec:reinforce"><span class="toc-section-number">4.1</span> REINFORCE</a><ul>
<li><a href="./3.1-PolicyGradient.html#sec:estimating-the-policy-gradient"><span class="toc-section-number">4.1.1</span> Estimating the policy gradient</a></li>
<li><a href="./3.1-PolicyGradient.html#sec:reducing-the-variance"><span class="toc-section-number">4.1.2</span> Reducing the variance</a></li>
<li><a href="./3.1-PolicyGradient.html#sec:policy-gradient-theorem"><span class="toc-section-number">4.1.3</span> Policy Gradient theorem</a></li>
</ul></li>
<li><a href="./3.2-ActorCritic.html#sec:advantage-actor-critic-methods"><span class="toc-section-number">4.2</span> Advantage Actor-Critic methods</a><ul>
<li><a href="./3.2-ActorCritic.html#sec:advantage-actor-critic-a2c"><span class="toc-section-number">4.2.1</span> Advantage Actor-Critic (A2C)</a></li>
<li><a href="./3.2-ActorCritic.html#sec:asynchronous-advantage-actor-critic-a3c"><span class="toc-section-number">4.2.2</span> Asynchronous Advantage Actor-Critic (A3C)</a></li>
<li><a href="./3.2-ActorCritic.html#sec:generalized-advantage-estimation-gae"><span class="toc-section-number">4.2.3</span> Generalized Advantage Estimation (GAE)</a></li>
<li><a href="./3.2-ActorCritic.html#sec:stochastic-actor-critic-for-continuous-action-spaces"><span class="toc-section-number">4.2.4</span> Stochastic actor-critic for continuous action spaces</a></li>
</ul></li>
<li><a href="./3.3-DPG.html#sec:deterministic-policy-gradient-dpg"><span class="toc-section-number">4.3</span> Deterministic Policy Gradient (DPG)</a><ul>
<li><a href="./3.3-DPG.html#sec:deterministic-policy-gradient-theorem"><span class="toc-section-number">4.3.1</span> Deterministic policy gradient theorem</a></li>
<li><a href="./3.3-DPG.html#sec:deep-deterministic-policy-gradient-ddpg"><span class="toc-section-number">4.3.2</span> Deep Deterministic Policy Gradient (DDPG)</a></li>
</ul></li>
<li><a href="./3.4-NaturalGradient.html#sec:natural-gradients"><span class="toc-section-number">4.4</span> Natural Gradients</a><ul>
<li><a href="./3.4-NaturalGradient.html#sec:natural-actor-critic-nac"><span class="toc-section-number">4.4.1</span> Natural Actor Critic (NAC)</a></li>
<li><a href="./3.4-NaturalGradient.html#sec:trust-region-policy-optimization-trpo"><span class="toc-section-number">4.4.2</span> Trust Region Policy Optimization (TRPO)</a></li>
<li><a href="./3.4-NaturalGradient.html#sec:proximal-policy-optimization-ppo"><span class="toc-section-number">4.4.3</span> Proximal Policy Optimization (PPO)</a></li>
</ul></li>
<li><a href="./3.4-NaturalGradient.html#sec:off-policy-actor-critic"><span class="toc-section-number">4.5</span> Off-policy Actor-Critic</a><ul>
<li><a href="./3.4-NaturalGradient.html#sec:importance-sampling"><span class="toc-section-number">4.5.1</span> Importance sampling</a></li>
<li><a href="./3.4-NaturalGradient.html#sec:linear-off-policy-actor-critic-offpac"><span class="toc-section-number">4.5.2</span> Linear Off-Policy Actor-Critic (OffPAC)</a></li>
<li><a href="./3.4-NaturalGradient.html#sec:actor-critic-with-experience-replay-acer"><span class="toc-section-number">4.5.3</span> Actor-Critic with Experience Replay (ACER)</a></li>
</ul></li>
<li><a href="./3.4-NaturalGradient.html#sec:other-policy-gradient-methods"><span class="toc-section-number">4.6</span> Other policy gradient methods</a><ul>
<li><a href="./3.4-NaturalGradient.html#sec:stochastic-value-gradient-svg"><span class="toc-section-number">4.6.1</span> Stochastic Value Gradient (SVG)</a></li>
<li><a href="./3.4-NaturalGradient.html#sec:fictitious-self-play-fsp"><span class="toc-section-number">4.6.2</span> Fictitious Self-Play (FSP)</a></li>
<li><a href="./3.4-NaturalGradient.html#sec:q-prop"><span class="toc-section-number">4.6.3</span> Q-Prop</a></li>
<li><a href="./3.4-NaturalGradient.html#sec:normalized-advantage-function-naf"><span class="toc-section-number">4.6.4</span> Normalized Advantage Function (NAF)</a></li>
</ul></li>
<li><a href="./3.4-NaturalGradient.html#sec:comparison-between-value-based-and-policy-gradient-methods"><span class="toc-section-number">4.7</span> Comparison between value-based and policy gradient methods</a></li>
<li><a href="./3.4-NaturalGradient.html#sec:gradient-free-policy-search"><span class="toc-section-number">4.8</span> Gradient-free policy search</a><ul>
<li><a href="./3.4-NaturalGradient.html#sec:cross-entropy-method-cem"><span class="toc-section-number">4.8.1</span> Cross-entropy Method (CEM)</a></li>
<li><a href="./3.4-NaturalGradient.html#sec:evolutionary-search-es"><span class="toc-section-number">4.8.2</span> Evolutionary Search (ES)</a></li>
</ul></li>
</ul></li>
<li><a href="./7-Practice.html#sec:deep-rl-in-practice"><span class="toc-section-number">5</span> Deep RL in practice</a><ul>
<li><a href="./7-Practice.html#sec:limitations"><span class="toc-section-number">5.1</span> Limitations</a></li>
<li><a href="./7-Practice.html#sec:reward-shaping"><span class="toc-section-number">5.2</span> Reward shaping</a></li>
<li><a href="./7-Practice.html#sec:simulation-environments"><span class="toc-section-number">5.3</span> Simulation environments</a></li>
<li><a href="./7-Practice.html#sec:algorithm-implementations"><span class="toc-section-number">5.4</span> Algorithm implementations</a></li>
</ul></li>
<li><a href="./8-References.html#sec:references">References</a></li>
</ul>


</nav>




<h1 id="sec:introduction"><span class="header-section-number">1</span> Introduction</h1>
<p>The goal of this document is to keep track the state-of-the-art in deep reinforcement learning. It starts with basics in reinforcement learning and deep learning to introduce the notations and covers different classes of deep RL methods, value-based or policy-based, model-free or model-based, etc.</p>
<p>Different classes of deep RL methods can be identified. This document will focus on the following ones:</p>
<ol type="1">
<li>Value-based algorithms (DQN…) used mostly for discrete problems like video games.</li>
<li>Policy-gradient algorithms (A3C, DDPG…) used for continuous control problems such as robotics.</li>
<li>Recurrent attention models (RAM…) for partially observable problems.</li>
<li>Model-based RL to reduce the sample complexity by incorporating a model of the environment.</li>
<li>Application of deep RL to robotics</li>
</ol>
<p>One could extend the list and talk about hierarchical RL, inverse RL, imitation-based RL, etc…</p>
<p><strong>Additional resources</strong></p>
<p>See <span class="citation" data-cites="Li2017">Li (<a href="8-References.html#ref-Li2017">2017</a>)</span>, <span class="citation" data-cites="Arulkumaran2017">Arulkumaran et al. (<a href="8-References.html#ref-Arulkumaran2017">2017</a>)</span> and <span class="citation" data-cites="Mousavi2018">Mousavi, Schukat, and Howley (<a href="8-References.html#ref-Mousavi2018">2018</a>)</span> for recent overviews of deep RL.</p>
<p>The CS294 course of Sergey Levine at Berkeley is incredibly complete: <a href="http://rll.berkeley.edu/deeprlcourse/" class="uri">http://rll.berkeley.edu/deeprlcourse/</a>. The Reinforcement Learning course by David Silver at UCL covers also the whole field: <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" class="uri">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html</a>.</p>
<p>This series of posts from Arthur Juliani <a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0" class="uri">https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0</a> also provide a very good introduction to deep RL, associated to code samples using tensorflow.</p>
<p><strong>Notes</strong></p>
<p>This document is meant to stay <em>work in progress</em> forever, as new algorithms will be added as they are published. Feel free to comment, correct, suggest, pull request by writing to <a href="mailto:julien.vitay@informatik.tu-chemnitz.de">julien.vitay@informatik.tu-chemnitz.de</a>.</p>
<p>For some reason, this document is better printed using chrome. Use the single file version <a href="./single.html">here</a> and print it to pdf. The style is adapted from the Github-Markdown CSS template <a href="https://www.npmjs.com/package/github-markdown-css" class="uri">https://www.npmjs.com/package/github-markdown-css</a>. The document is written in Pandoc’s Markdown and converted to html using pandoc-citeproc and pandoc-crossref.</p>
<p>Some figures are taken from the original publication (“Taken from” or “Source” in the caption). Their copyright stays to the respective authors, naturally. The rest is my own work and can be reproduced under any free license.</p>
<p><strong>Thanks</strong></p>
<p>Thanks to all the students who helped me dive into that exciting research field, in particular: Winfried Lötzsch, Johannes Jung, Frank Witscher, Danny Hofmann, Oliver Lange, Vinayakumar Murganoor.</p>

<br>
<div class="arrows">
<a href="#" class="previous">&laquo; Previous</a>
<a href="1.1-BasicRL.html" class="next">Next &raquo;</a>
</div>


</article>
</body>
</html>
