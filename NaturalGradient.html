<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Julien Vitay">
  <title>Deep Reinforcement Learning</title>


  <link rel="stylesheet" href="assets/syntax.css">
  <link rel="stylesheet" href="assets/github.css">
  <script src="/usr/share/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->

</head>

<body class="markdown-body">

<header>
  <h1 class="title"><strong>Deep Reinforcement Learning</strong></h1>
  <p class="author">Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de">julien.vitay@informatik.tu-chemnitz.de</a></p>
</header>


<article>

<nav id="TOC" class ="toc">
  <h2><strong>Deep Reinforcement Learning</strong></h2>
  <p class="author">Julien Vitay</p>

<ul>
<li><a href="./Introduction.html#sec:introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="./BasicRL.html#sec:basics"><span class="toc-section-number">2</span> Basics</a><ul>
<li><a href="./BasicRL.html#sec:reinforcement-learning-and-markov-decision-process"><span class="toc-section-number">2.1</span> Reinforcement learning and Markov Decision Process</a><ul>
<li><a href="./BasicRL.html#sec:policy-and-value-functions"><span class="toc-section-number">2.1.1</span> Policy and value functions</a></li>
<li><a href="./BasicRL.html#sec:bellman-equations"><span class="toc-section-number">2.1.2</span> Bellman equations</a></li>
<li><a href="./BasicRL.html#sec:dynamic-programming"><span class="toc-section-number">2.1.3</span> Dynamic programming</a></li>
<li><a href="./BasicRL.html#sec:monte-carlo-sampling"><span class="toc-section-number">2.1.4</span> Monte-Carlo sampling</a></li>
<li><a href="./BasicRL.html#sec:temporal-difference"><span class="toc-section-number">2.1.5</span> Temporal Difference</a></li>
<li><a href="./BasicRL.html#sec:eligibility-traces"><span class="toc-section-number">2.1.6</span> Eligibility traces</a></li>
<li><a href="./BasicRL.html#sec:actor-critic-architectures"><span class="toc-section-number">2.1.7</span> Actor-critic architectures</a></li>
<li><a href="./BasicRL.html#sec:function-approximation"><span class="toc-section-number">2.1.8</span> Function approximation</a></li>
</ul></li>
<li><a href="./DeepLearning.html#sec:deep-learning"><span class="toc-section-number">2.2</span> Deep learning</a><ul>
<li><a href="./DeepLearning.html#sec:deep-neural-networks"><span class="toc-section-number">2.2.1</span> Deep neural networks</a></li>
<li><a href="./DeepLearning.html#sec:convolutional-networks"><span class="toc-section-number">2.2.2</span> Convolutional networks</a></li>
<li><a href="./DeepLearning.html#sec:recurrent-neural-networks"><span class="toc-section-number">2.2.3</span> Recurrent neural networks</a></li>
</ul></li>
</ul></li>
<li><a href="./Valuebased.html#sec:value-based-methods"><span class="toc-section-number">3</span> Value-based methods</a><ul>
<li><a href="./Valuebased.html#sec:limitations-of-deep-neural-networks-for-function-approximation"><span class="toc-section-number">3.1</span> Limitations of deep neural networks for function approximation</a></li>
<li><a href="./Valuebased.html#sec:deep-q-network-dqn"><span class="toc-section-number">3.2</span> Deep Q-Network (DQN)</a></li>
<li><a href="./Valuebased.html#sec:double-dqn"><span class="toc-section-number">3.3</span> Double DQN</a></li>
<li><a href="./Valuebased.html#sec:prioritized-experience-replay"><span class="toc-section-number">3.4</span> Prioritized experience replay</a></li>
<li><a href="./Valuebased.html#sec:duelling-network"><span class="toc-section-number">3.5</span> Duelling network</a></li>
<li><a href="./Valuebased.html#sec:distributed-dqn-gorila"><span class="toc-section-number">3.6</span> Distributed DQN (GORILA)</a></li>
<li><a href="./Valuebased.html#sec:deep-recurrent-q-learning-drqn"><span class="toc-section-number">3.7</span> Deep Recurrent Q-learning (DRQN)</a></li>
<li><a href="./Valuebased.html#sec:other-variants-of-dqn"><span class="toc-section-number">3.8</span> Other variants of DQN</a></li>
</ul></li>
<li><a href="./PolicyGradient.html#sec:policy-gradient-methods"><span class="toc-section-number">4</span> Policy Gradient methods</a><ul>
<li><a href="./PolicyGradient.html#sec:reinforce"><span class="toc-section-number">4.1</span> REINFORCE</a><ul>
<li><a href="./PolicyGradient.html#sec:estimating-the-policy-gradient"><span class="toc-section-number">4.1.1</span> Estimating the policy gradient</a></li>
<li><a href="./PolicyGradient.html#sec:reducing-the-variance"><span class="toc-section-number">4.1.2</span> Reducing the variance</a></li>
<li><a href="./PolicyGradient.html#sec:policy-gradient-theorem"><span class="toc-section-number">4.1.3</span> Policy Gradient theorem</a></li>
</ul></li>
<li><a href="./ActorCritic.html#sec:advantage-actor-critic-methods"><span class="toc-section-number">4.2</span> Advantage Actor-Critic methods</a><ul>
<li><a href="./ActorCritic.html#sec:advantage-actor-critic-a2c"><span class="toc-section-number">4.2.1</span> Advantage Actor-Critic (A2C)</a></li>
<li><a href="./ActorCritic.html#sec:asynchronous-advantage-actor-critic-a3c"><span class="toc-section-number">4.2.2</span> Asynchronous Advantage Actor-Critic (A3C)</a></li>
<li><a href="./ActorCritic.html#sec:generalized-advantage-estimation-gae"><span class="toc-section-number">4.2.3</span> Generalized Advantage Estimation (GAE)</a></li>
<li><a href="./ActorCritic.html#sec:stochastic-actor-critic-for-continuous-action-spaces"><span class="toc-section-number">4.2.4</span> Stochastic actor-critic for continuous action spaces</a></li>
</ul></li>
<li><a href="./ImportanceSampling.html#sec:off-policy-actor-critic"><span class="toc-section-number">4.3</span> Off-policy Actor-Critic</a><ul>
<li><a href="./ImportanceSampling.html#sec:importance-sampling"><span class="toc-section-number">4.3.1</span> Importance sampling</a></li>
<li><a href="./ImportanceSampling.html#sec:linear-off-policy-actor-critic-off-pac"><span class="toc-section-number">4.3.2</span> Linear Off-Policy Actor-Critic (Off-PAC)</a></li>
<li><a href="./ImportanceSampling.html#sec:retrace"><span class="toc-section-number">4.3.3</span> Retrace</a></li>
<li><a href="./ImportanceSampling.html#sec:self-imitation-learning-sil"><span class="toc-section-number">4.3.4</span> Self-Imitation Learning (SIL)</a></li>
</ul></li>
<li><a href="./DPG.html#sec:deterministic-policy-gradient-dpg"><span class="toc-section-number">4.4</span> Deterministic Policy Gradient (DPG)</a><ul>
<li><a href="./DPG.html#sec:deterministic-policy-gradient-theorem"><span class="toc-section-number">4.4.1</span> Deterministic policy gradient theorem</a></li>
<li><a href="./DPG.html#sec:deep-deterministic-policy-gradient-ddpg"><span class="toc-section-number">4.4.2</span> Deep Deterministic Policy Gradient (DDPG)</a></li>
<li><a href="./DPG.html#sec:distributed-distributional-ddpg-d4pg"><span class="toc-section-number">4.4.3</span> Distributed Distributional DDPG (D4PG)</a></li>
</ul></li>
<li><a href="./NaturalGradient.html#sec:natural-gradients"><span class="toc-section-number">4.5</span> Natural Gradients</a><ul>
<li><a href="./NaturalGradient.html#sec:principle-of-natural-gradients"><span class="toc-section-number">4.5.1</span> Principle of natural gradients</a></li>
<li><a href="./NaturalGradient.html#sec:natural-policy-gradient-and-natural-actor-critic-nac"><span class="toc-section-number">4.5.2</span> Natural policy gradient and Natural Actor Critic (NAC)</a></li>
<li><a href="./NaturalGradient.html#sec:trust-region-policy-optimization-trpo"><span class="toc-section-number">4.5.3</span> Trust Region Policy Optimization (TRPO)</a></li>
<li><a href="./NaturalGradient.html#sec:proximal-policy-optimization-ppo"><span class="toc-section-number">4.5.4</span> Proximal Policy Optimization (PPO)</a></li>
<li><a href="./NaturalGradient.html#sec:actor-critic-with-experience-replay-acer"><span class="toc-section-number">4.5.5</span> Actor-Critic with Experience Replay (ACER)</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:distributional-learning"><span class="toc-section-number">4.6</span> Distributional learning</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:the-reactor"><span class="toc-section-number">4.6.1</span> The Reactor</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:entropy-based-rl"><span class="toc-section-number">4.7</span> Entropy-based RL</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:soft-actor-critic-sac"><span class="toc-section-number">4.7.1</span> Soft Actor-Critic (SAC)</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:other-policy-search-methods"><span class="toc-section-number">4.8</span> Other policy search methods</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:stochastic-value-gradient-svg"><span class="toc-section-number">4.8.1</span> Stochastic Value Gradient (SVG)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:q-prop"><span class="toc-section-number">4.8.2</span> Q-Prop</a></li>
<li><a href="./OtherPolicyGradient.html#sec:normalized-advantage-function-naf"><span class="toc-section-number">4.8.3</span> Normalized Advantage Function (NAF)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:fictitious-self-play-fsp"><span class="toc-section-number">4.8.4</span> Fictitious Self-Play (FSP)</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:comparison-between-value-based-and-policy-gradient-methods"><span class="toc-section-number">4.9</span> Comparison between value-based and policy gradient methods</a></li>
<li><a href="./OtherPolicyGradient.html#sec:gradient-free-policy-search"><span class="toc-section-number">4.10</span> Gradient-free policy search</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:cross-entropy-method-cem"><span class="toc-section-number">4.10.1</span> Cross-entropy Method (CEM)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:evolutionary-search-es"><span class="toc-section-number">4.10.2</span> Evolutionary Search (ES)</a></li>
</ul></li>
</ul></li>
<li><a href="./ModelBased.html#sec:model-based-rl"><span class="toc-section-number">5</span> Model-based RL</a><ul>
<li><a href="./ModelBased.html#sec:dyna-q"><span class="toc-section-number">5.1</span> Dyna-Q</a></li>
<li><a href="./ModelBased.html#sec:unsorted-references"><span class="toc-section-number">5.2</span> Unsorted references</a></li>
</ul></li>
<li><a href="./Practice.html#sec:deep-rl-in-practice"><span class="toc-section-number">6</span> Deep RL in practice</a><ul>
<li><a href="./Practice.html#sec:limitations"><span class="toc-section-number">6.1</span> Limitations</a></li>
<li><a href="./Practice.html#sec:reward-shaping"><span class="toc-section-number">6.2</span> Reward shaping</a></li>
<li><a href="./Practice.html#sec:simulation-environments"><span class="toc-section-number">6.3</span> Simulation environments</a></li>
<li><a href="./Practice.html#sec:algorithm-implementations"><span class="toc-section-number">6.4</span> Algorithm implementations</a></li>
</ul></li>
<li><a href="./References.html#sec:references">References</a></li>
</ul>


</nav>



<h2 id="sec:natural-gradients"><span class="header-section-number">4.5</span> Natural Gradients</h2>
<p>The deep networks used as function approximators in the methods presented until now were all optimized (trained) using <strong>stochastic gradient descent</strong> (SGD) or any of its variants (RMSProp, Adam, etc). The basic idea is to change the parameters <span class="math inline">\(\theta\)</span> in the opposite direction of the gradient of the loss function (or the same direction as the policy gradient, in which case it is called gradient ascent), proportionally to a small learning rate <span class="math inline">\(\eta\)</span>:</p>
<p><span class="math display">\[
    \Delta \theta = - \eta \, \nabla_\theta \mathcal{L}(\theta)
\]</span></p>
<p>SGD is also called a <strong>steepest descent method</strong>: one searches for the smallest parameter change <span class="math inline">\(\Delta \theta\)</span> inducing the biggest negative change of the loss function. In classical supervised learning, this is what we want: we want to minimize the loss function as fast as possible, while keeping weight changes as small as possible, otherwise learning might become unstable (weight changes computed for a single minibatch might erase the changes made on previous minibatches). The main difficulty of supervised learning is to choose the right value for the learning rate: too high and learning is unstable; too low and learning takes forever.</p>
<p>In deep RL, we have an additional problem: the problem is not stationary (see Section <a href="./Valuebased.html#sec:limitations-of-deep-neural-networks-for-function-approximation">3.1</a>). In Q-learning, the target <span class="math inline">\(r(s, a, s&#39;) + \gamma \, \max_{a&#39;} Q_\theta(S&#39;, a&#39;)\)</span> is changing with <span class="math inline">\(\theta\)</span>. If the Q-values change a lot between two minibatches, the network will not get any stable target signal to learn from, and the policy will end up suboptimal. The trick is to use <strong>target networks</strong> to compute the target, which can be either an old copy of the current network (vanilla DQN), or a smoothed version of it (DDPG). Obviously, this introduces a bias (the targets are always wrong during training), but this bias converges to zero (after sufficient training, the targets will be almost correct), at the cost of a huge sample complexity.</p>
<p>Target networks cannot be used in <strong>on-policy</strong> methods, especially actor-critic architectures. As seen in Section <a href="./ImportanceSampling.html#sec:off-policy-actor-critic">4.3</a>, the critic must learn from transitions recently generated by the actor (although importance sampling and the Retrace algorithm might help). The problem with on-policy methods is that they waste a lot of data: they always need fresh samples to learn from and never reuse past experiences. The policy gradient theorem shows why:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, Q^{\pi_\theta}(s, a))] \approx  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, Q_\varphi(s, a))]
\]</span></p>
<p>If the policy <span class="math inline">\(\pi_\theta\)</span> changes a lot between two updates, the estimated Q-value <span class="math inline">\(Q_\varphi(s, a)\)</span> will represent the value of the action for a totally different policy, not the true Q-value <span class="math inline">\(Q^{\pi_\theta}(s, a)\)</span>. The estimated policy gradient will then be strongly biased and learning will be suboptimal. In other words, the actor should not change much faster than the critic, and vice versa. A naive solution would be to use a very small learning rate for the actor, but this just slows down learning (adding to the sample complexity) without solving the problem.</p>
<p>To solve the problem, we should actually do the opposite of the steepest descent: <em>search for the biggest parameter change <span class="math inline">\(\Delta \theta\)</span> inducing the smallest change in the policy, but in the right direction</em>. If the parameter change is high, the actor will learn a lot internally from each experience. But if the policy change is small between two updates (although the parameters have changed a lot), we might be able to reuse past experiences, as the targets will not be that wrong.</p>
<p>This is where <strong>natural gradients</strong> come into play, which are originally a statistical method to optimize over spaces of probability distributions, for example for variational inference. The idea to use natural gradients to train neural networks comes from <span class="citation" data-cites="Amari1998">Amari (<a href="References.html#ref-Amari1998">1998</a>)</span>. <span class="citation" data-cites="Kakade2001">Kakade (<a href="References.html#ref-Kakade2001">2001</a>)</span> applied natural gradients to policy gradient methods, while <span class="citation" data-cites="Peters2008">Peters and Schaal (<a href="References.html#ref-Peters2008">2008</a>)</span> proposed a natural actor-critic algorithm for linear function approximators. The idea was adapted to deep RL by Schulman and colleagues, with Trust Region Policy Optimization <span class="citation" data-cites="Schulman2015a">(TRPO, Schulman, Levine, et al. <a href="References.html#ref-Schulman2015a">2015</a>)</span> and Proximal Policy Optimization <span class="citation" data-cites="Schulman2017">(PPO, Schulman et al. <a href="References.html#ref-Schulman2017">2017</a>)</span>, the latter gaining momentum over DDPG as the go-to method for continuous RL problems, particularly because of its smaller sample complexity and its robustness to hyperparameters.</p>
<h3 id="sec:principle-of-natural-gradients"><span class="header-section-number">4.5.1</span> Principle of natural gradients</h3>
<figure>
<img src="img/naturalgradient.png" alt="Figure 26: Euclidian distances in the parameter space do not represent well the statistical distance between probability distributions. The two Gaussians on the left (\mathcal{N}(0, 0.2) and \mathcal{N}(1, 0.2)) have the same Euclidian distance in the parameter space (d = \sqrt{(\mu_0 - \mu_1)^2+(\sigma_0 - \sigma_1)^2}) than the two Gaussians on the right (\mathcal{N}(0, 10) and \mathcal{N}(1, 10)). However, the Gaussians on the right are much more similar than the two on the left: if you have a single sample, you could not say from which distribution it comes for the Gaussians on the right, while it is obvious for the Gaussians on the left." id="fig:naturalgradient" style="width:80.0%" /><figcaption>Figure 26: Euclidian distances in the parameter space do not represent well the statistical distance between probability distributions. The two Gaussians on the left (<span class="math inline">\(\mathcal{N}(0, 0.2)\)</span> and <span class="math inline">\(\mathcal{N}(1, 0.2)\)</span>) have the same Euclidian distance in the parameter space (<span class="math inline">\(d = \sqrt{(\mu_0 - \mu_1)^2+(\sigma_0 - \sigma_1)^2}\)</span>) than the two Gaussians on the right (<span class="math inline">\(\mathcal{N}(0, 10)\)</span> and <span class="math inline">\(\mathcal{N}(1, 10)\)</span>). However, the Gaussians on the right are much more similar than the two on the left: if you have a single sample, you could not say from which distribution it comes for the Gaussians on the right, while it is obvious for the Gaussians on the left.</figcaption>
</figure>
<p>Consider the two Gaussian distributions in the left part of Fig. <a href="#fig:naturalgradient">26</a> (<span class="math inline">\(\mathcal{N}(0, 0.2)\)</span> and <span class="math inline">\(\mathcal{N}(1, 0.2)\)</span>) and the two on the right (<span class="math inline">\(\mathcal{N}(0, 10)\)</span> and <span class="math inline">\(\mathcal{N}(1, 10)\)</span>). In both cases, the Distance in the Euclidian space <span class="math inline">\(d = \sqrt{(\mu_0 - \mu_1)^2+(\sigma_0 - \sigma_1)^2}\)</span> is the same between the two Gaussians. Obviously, the two distributions on the left are however further away from each other than the two on the the right. This indicates that the Euclidian distance in the parameter space (which is what <em>normal</em> gradients act on) is not a correct measurement of the statistical distance between two distributions (which what we want to minimize between two iterations of PG).</p>
<p>In statistics, a common measurement of the statistical distance between two distributions <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> is the <strong>Kullback-Leibler (KL) divergence</strong> <span class="math inline">\(D_{KL}(p||q)\)</span>, also called relative entropy or information gain. It is defined as:</p>
<p><span class="math display">\[
    D_{KL}(p || q) = \mathbb{E}_{x \sim p} [\log \frac{p(x)}{q(x)}]  = \int p(x) \, \log \frac{p(x)}{q(x)} \, dx
\]</span></p>
<p>Its minimum is 0 when <span class="math inline">\(p=q\)</span> (as <span class="math inline">\(\log \frac{p(x)}{q(x)}\)</span> is then 0) and is positive otherwise. Minimizing the KL divergence is equivalent to “matching” two distributions. Note that supervised methods in machine learning can all be interpreted as a minimization of the KL divergence: if <span class="math inline">\(p(x)\)</span> represents the distribution of the data (the label of a sample <span class="math inline">\(x\)</span>) and <span class="math inline">\(q(x)\)</span> the one of the model (the prediction of a neural network for the same sample <span class="math inline">\(x\)</span>), supervised methods want the output distribution of the model to match the distribution of the data, i.e. make predictions that are the same as the labels. For generative models, this is for example at the core of <em>generative adversarial networks</em> <span class="citation" data-cites="Goodfellow2014 Arjovsky2017">(Goodfellow et al. <a href="References.html#ref-Goodfellow2014">2014</a>; Arjovsky, Chintala, and Bottou <a href="References.html#ref-Arjovsky2017">2017</a>)</span> or <em>variational autoencoders</em> <span class="citation" data-cites="Kingma2013">(Kingma and Welling <a href="References.html#ref-Kingma2013">2013</a>)</span>.</p>
<p>The KL divergence is however not symmetrical (<span class="math inline">\(D_{KL}(p || q) \neq D_{KL}(q || p)\)</span>), so a more useful divergence is the symmetric KL divergence, also known as Jensen-Shannon (JS) divergence:</p>
<p><span class="math display">\[
    D_{JS}(p || q) = \frac{D_{KL}(p || q) + D_{KL}(q || p)}{2}
\]</span></p>
<p>Other forms of divergence measurements exist, such as the Wasserstein distance which improves generative adversarial networks <span class="citation" data-cites="Arjovsky2017">(Arjovsky, Chintala, and Bottou <a href="References.html#ref-Arjovsky2017">2017</a>)</span>, but they are not relevant here. See <a href="https://www.alexirpan.com/2017/02/22/wasserstein-gan.html" class="uri">https://www.alexirpan.com/2017/02/22/wasserstein-gan.html</a> for more explanations.</p>
<p>We now have a global measurement of the similarity between two distributions on the whole input space, but which is hard to compute. How can we use it anyway in our optimization problem? As mentioned above, we search for the biggest parameter change <span class="math inline">\(\Delta \theta\)</span> inducing the smallest change in the policy. We need a metric linking changes in the parameters of the distribution (the weights of the network) to changes in the distribution itself. In other terms, we will apply gradient descent on the statistical manifold defined by the parameters rather than on the parameters themselves.</p>
<figure>
<img src="img/riemannian.png" alt="Figure 27: Illustration of the Riemannian metric. The Euclidian distance between p(x; \theta) and p(x; \theta + \Delta \theta) depends on the Euclidian distance between \theta and \theta + \Delta\theta, i.e. \theta. Riemannian metrics follow the geometry of the manifold to compute that distance, depending on its curvature." id="fig:riemannian" style="width:50.0%" /><figcaption>Figure 27: Illustration of the Riemannian metric. The Euclidian distance between <span class="math inline">\(p(x; \theta)\)</span> and <span class="math inline">\(p(x; \theta + \Delta \theta)\)</span> depends on the Euclidian distance between <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\theta + \Delta\theta\)</span>, i.e. <span class="math inline">\(\theta\)</span>. Riemannian metrics follow the geometry of the manifold to compute that distance, depending on its curvature.</figcaption>
</figure>
<p>Let’s consider a parameterized distribution <span class="math inline">\(p(x; \theta)\)</span> and its new value <span class="math inline">\(p(x; \theta + \Delta \theta)\)</span> after applying a small parameter change <span class="math inline">\(\Delta \theta\)</span>. As depicted on Fig. <a href="#fig:riemannian">27</a>, the Euclidian metric in the parameter space (<span class="math inline">\(||\theta + \Delta \theta - \theta||^2\)</span>) does take the structure of the statistical manifold into account. We need to define a <strong>Riemannian metric</strong> which accounts locally for the curvature of the manifold between <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\theta + \Delta \theta\)</span>. The Riemannian distance is defined by the dot product:</p>
<p><span class="math display">\[
    ||\Delta \theta||^2 = &lt; \Delta \theta , F(\theta) \, \Delta \theta &gt;
\]</span></p>
<p>where <span class="math inline">\(F(\theta)\)</span> is called the Riemannian metric tensor and is an inner product on the tangent space of the manifold at the point <span class="math inline">\(\theta\)</span>.</p>
<p>When using the symmetric KL divergence to measure the distance between two distributions, the corresponding Riemannian metric is the <strong>Fisher Information Matrix</strong>, define as the Hessian matrix of the KL divergence around <span class="math inline">\(\theta\)</span>, i.e. the matrix of second order derivatives w.r.t the elements of <span class="math inline">\(\theta\)</span>. See <a href="https://stats.stackexchange.com/questions/51185/connection-between-fisher-metric-and-the-relative-entropy" class="uri">https://stats.stackexchange.com/questions/51185/connection-between-fisher-metric-and-the-relative-entropy</a> for an explanation of the link between the Fisher metric and KL divergence.</p>
<p>The Fisher information matrix is defined as the Hessian of the KL divergence around <span class="math inline">\(\theta\)</span>, i.e. how the manifold locally changes around <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
    F(\theta) = \nabla^2 D_{JS}(p(x; \theta) || p(x; \theta + \Delta \theta))|_{\Delta \theta = 0}
\]</span></p>
<p>which necessitates to compute second order derivatives which are very complex and slow to obtain, especially when there are many parameters <span class="math inline">\(\theta\)</span> (the weights of the NN). Fortunately, it also has a simpler form which only depends on the outer product between the gradients of the log-probabilities:</p>
<p><span class="math display">\[
    F(\theta) = \mathbb{E}_{x \sim p(x, \theta)}[ \nabla \log p(x; \theta)  (\nabla \log p(x; \theta))^T]
\]</span></p>
<p>which is something we can easily sample and compute.</p>
<p>Why is it useful? The Fisher Information matrix allows to <strong>locally</strong> approximate (for small <span class="math inline">\(\Delta \theta\)</span>) the KL divergence between the two close distributions (using a second order Taylor series expansion):</p>
<p><span class="math display">\[
    D_{JS}(p(x; \theta) || p(x; \theta + \Delta \theta)) \approx \Delta \theta^T \, F(\theta) \, \Delta \theta
\]</span></p>
<p>The KL divergence is then locally quadratic, which means that the update rules obtained when minimizing the KL divergence with gradient descent will be linear. Suppose we want to minimize a loss function <span class="math inline">\(L\)</span> parameterized by <span class="math inline">\(\theta\)</span> and depending on the distribution <span class="math inline">\(p\)</span>. <strong>Natural gradient descent</strong> <span class="citation" data-cites="Amari1998">(Amari <a href="References.html#ref-Amari1998">1998</a>)</span> attempts to move along the statistical manifold defined by <span class="math inline">\(p\)</span> by correcting the gradient of <span class="math inline">\(L(\theta)\)</span> using the local curvature of the KL-divergence surface, i.e. moving some given distance in the direction <span class="math inline">\(\tilde{\nabla_\theta} L(\theta)\)</span>:</p>
<p><span class="math display">\[
    \tilde{\nabla_\theta} L(\theta) = F(\theta)^{-1} \, \nabla_\theta L(\theta)
\]</span></p>
<p><span class="math inline">\(\tilde{\nabla_\theta} L(\theta)\)</span> is the <strong>natural gradient</strong> of <span class="math inline">\(L(\theta)\)</span>. Natural gradient descent simply takes steps in this direction:</p>
<p><span class="math display">\[
    \delta \theta = - \eta \, \tilde{\nabla_\theta} L(\theta)
\]</span></p>
<p>When the manifold is not curved (<span class="math inline">\(F(\theta)\)</span> is the identity matrix), natural gradient descent is the regular gradient descent.</p>
<p>But what is the advantage of natural gradients? The problem with regular gradient descent is that it relies on a fixed learning rate. In regions where the loss function is flat (a plateau), the gradient will be almost zero, leading to very slow improvements. Because the natural gradient depends on the inverse of the curvature (Fisher), the magnitude of the gradient will be higher in flat regions, leading to bigger steps, and smaller in very steep regions (around minima). Natural GD therefore converges faster and better than regular GD.</p>
<p>Natural gradient descent is a generic optimization method, it can for example be used to train more efficiently deep networks in supervised learning <span class="citation" data-cites="Pascanu2013">(Pascanu and Bengio <a href="References.html#ref-Pascanu2013">2013</a>)</span>. Its main drawback is the necessity to inverse the Fisher information matrix, whose size depends on the number of free parameters (if you have N weights in the NN, you need to inverse a NxN matrix). Several approximations allows to remediate to this problem, for example Kronecker-Factored Approximate Curvature (K-FAC, see <a href="https://syncedreview.com/2017/03/25/optimizing-neural-networks-using-structured-probabilistic-models/" class="uri">https://syncedreview.com/2017/03/25/optimizing-neural-networks-using-structured-probabilistic-models/</a>).</p>
<p><strong>Additional resources</strong> to understand natural gradients:</p>
<ul>
<li><a href="http://andymiller.github.io/2016/10/02/natural_gradient_bbvi.html" class="uri">http://andymiller.github.io/2016/10/02/natural_gradient_bbvi.html</a></li>
<li><a href="https://hips.seas.harvard.edu/blog/2013/01/25/the-natural-gradient/" class="uri">https://hips.seas.harvard.edu/blog/2013/01/25/the-natural-gradient/</a></li>
<li><a href="http://kvfrans.com/what-is-the-natural-gradient-and-where-does-it-appear-in-trust-region-policy-optimization" class="uri">http://kvfrans.com/what-is-the-natural-gradient-and-where-does-it-appear-in-trust-region-policy-optimization</a></li>
<li>A tutorial by John Schulman (OpenAI) <a href="https://www.youtube.com/watch?v=xvRrgxcpaHY" class="uri">https://www.youtube.com/watch?v=xvRrgxcpaHY</a></li>
<li>A blog post on the related Hessian-free optimization and conjuguate gradients <a href="http://andrew.gibiansky.com/blog/machine-learning/hessian-free-optimization/" class="uri">http://andrew.gibiansky.com/blog/machine-learning/hessian-free-optimization/</a></li>
</ul>
<p><strong>Note:</strong> Natural gradients can also be used to train DQN architectures, resulting in more efficient and stable learning behaviors <span class="citation" data-cites="Knight2018">(Knight and Lerner <a href="References.html#ref-Knight2018">2018</a>)</span>.</p>
<h3 id="sec:natural-policy-gradient-and-natural-actor-critic-nac"><span class="header-section-number">4.5.2</span> Natural policy gradient and Natural Actor Critic (NAC)</h3>
<p><span class="citation" data-cites="Kakade2001">Kakade (<a href="References.html#ref-Kakade2001">2001</a>)</span> applied the principle of natural gradients proposed by <span class="citation" data-cites="Amari1998">Amari (<a href="References.html#ref-Amari1998">1998</a>)</span> to the policy gradient theorem:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, Q^{\pi_\theta}(s, a)]
\]</span></p>
<p>This <em>regular</em> gradient does not take into account the underlying structure of the policy distribution <span class="math inline">\(\pi(s, a)\)</span>. The Fisher information matrix for the policy is defined by:</p>
<p><span class="math display">\[
    F(\theta) = \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[ \nabla \log \pi_\theta(s, a)  (\nabla \log \pi_\theta(s, a))^T]
\]</span></p>
<p>The natural policy gradient is simply:</p>
<p><span class="math display">\[
    \tilde{\nabla}_\theta J(\theta) = F(\theta)^{-1} \, \nabla_\theta J(\theta)  = \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[ F(\theta)^{-1} \,  \nabla_\theta \log \pi_\theta(s, a) \, Q^{\pi_\theta}(s, a)]
\]</span></p>
<p><span class="citation" data-cites="Kakade2001">Kakade (<a href="References.html#ref-Kakade2001">2001</a>)</span> also shows that you can replace the true Q-value <span class="math inline">\(Q^{\pi_\theta}(s, a)\)</span> with a compatible approximation <span class="math inline">\(Q\varphi(s, a)\)</span> (as long as it minimizes the quadratic error) and still obtained an unbiased natural gradient. He experimented this new rule on various simple MDPs and observed drastic improvements over vanilla PG.</p>
<p><span class="citation" data-cites="Peters2008">Peters and Schaal (<a href="References.html#ref-Peters2008">2008</a>)</span> extended on the work of <span class="citation" data-cites="Kakade2001">Kakade (<a href="References.html#ref-Kakade2001">2001</a>)</span> to propose the natural actor-critic (NAC). The exact derivations would be too complex to summarize here. He showed that the <span class="math inline">\(F(\theta)\)</span>) is a true Fisher information matrix even when using sampled episodes, and derived a baseline <span class="math inline">\(b\)</span> to reduce the variance of the natural policy gradient. He demonstrated the power of this algorithm by letting a robot learning motor primitives for baseball.</p>
<h3 id="sec:trust-region-policy-optimization-trpo"><span class="header-section-number">4.5.3</span> Trust Region Policy Optimization (TRPO)</h3>
<p><span class="citation" data-cites="Schulman2015a">Schulman, Levine, et al. (<a href="References.html#ref-Schulman2015a">2015</a>)</span></p>
<h3 id="sec:proximal-policy-optimization-ppo"><span class="header-section-number">4.5.4</span> Proximal Policy Optimization (PPO)</h3>
<p><span class="citation" data-cites="Schulman2017">Schulman et al. (<a href="References.html#ref-Schulman2017">2017</a>)</span></p>
<p>Explanations from OpenAI: <a href="https://blog.openai.com/openai-baselines-ppo/#content" class="uri">https://blog.openai.com/openai-baselines-ppo/#content</a></p>
<h3 id="sec:actor-critic-with-experience-replay-acer"><span class="header-section-number">4.5.5</span> Actor-Critic with Experience Replay (ACER)</h3>
<p><span class="citation" data-cites="Wang2017">Wang et al. (<a href="References.html#ref-Wang2017">2017</a>)</span></p>

<br>
<div class="arrows">
<a href="DPG.html" class="previous">&laquo; Previous</a>
<a href="OtherPolicyGradient.html" class="next">Next &raquo;</a>
</div>


</article>
</body>
</html>
