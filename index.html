<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Julien Vitay">
  <title>Deep Reinforcement Learning</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link href=https://fonts.googleapis.com/css?family=Roboto rel=stylesheet>
  <link rel="stylesheet" href="github.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>

<body class="markdown-body">

<header>
<h1 class="title"><strong>Deep Reinforcement Learning</strong></h1>
<p class="author">Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de">julien.vitay@informatik.tu-chemnitz.de</a></p>
</header>

<article>
<p class="abstract">The goal of this document is to keep track the state-of-the-art in deep reinforcement learning. It starts with basics in reinforcement learning and deep learning to introduce the notations. It tries to cover different classes of deep RL models, from value-based to policy-based, model-free or model-based, etc, with a focus on the application of deep RL to robotics.</p>

<nav id="TOC" class ="toc">
<h2><strong>Deep Reinforcement Learning</strong></h2>
<p class="author">Julien Vitay</p>
<ul>
<li><a href="#sec:basics"><span class="toc-section-number">1</span> Basics</a><ul>
<li><a href="#sec:reinforcement-learning"><span class="toc-section-number">1.1</span> Reinforcement learning</a><ul>
<li><a href="#sec:mdp-markov-decision-process"><span class="toc-section-number">1.1.1</span> MDP: Markov Decision Process</a></li>
<li><a href="#sec:pomdp-partially-observable-markov-decision-process"><span class="toc-section-number">1.1.2</span> POMDP: Partially Observable Markov Decision Process</a></li>
<li><a href="#sec:policy-and-value-fucntions"><span class="toc-section-number">1.1.3</span> Policy and value fucntions</a></li>
<li><a href="#sec:bellman-equations"><span class="toc-section-number">1.1.4</span> Bellman equations</a></li>
<li><a href="#sec:dynamic-programming"><span class="toc-section-number">1.1.5</span> Dynamic programming</a></li>
<li><a href="#sec:monte-carlo-sampling"><span class="toc-section-number">1.1.6</span> Monte-Carlo sampling</a></li>
<li><a href="#sec:temporal-difference"><span class="toc-section-number">1.1.7</span> Temporal Difference</a></li>
<li><a href="#sec:actor-critic-architectures"><span class="toc-section-number">1.1.8</span> Actor-critic architectures</a></li>
<li><a href="#sec:function-approximation"><span class="toc-section-number">1.1.9</span> Function approximation</a></li>
</ul></li>
<li><a href="#sec:deep-learning"><span class="toc-section-number">1.2</span> Deep learning</a><ul>
<li><a href="#sec:deep-neural-networks"><span class="toc-section-number">1.2.1</span> Deep neural networks</a></li>
<li><a href="#sec:convolutional-networks"><span class="toc-section-number">1.2.2</span> Convolutional networks</a></li>
<li><a href="#sec:recurrent-neural-networks"><span class="toc-section-number">1.2.3</span> Recurrent neural networks</a></li>
</ul></li>
</ul></li>
<li><a href="#sec:value-based-methods"><span class="toc-section-number">2</span> Value-based methods</a><ul>
<li><a href="#sec:limitations-of-deep-neural-networks-for-function-approximation"><span class="toc-section-number">2.1</span> Limitations of deep neural networks for function approximation</a></li>
<li><a href="#sec:deep-q-network-dqn"><span class="toc-section-number">2.2</span> Deep Q-Network (DQN)</a></li>
<li><a href="#sec:double-dqn"><span class="toc-section-number">2.3</span> Double DQN</a></li>
<li><a href="#sec:prioritised-replay"><span class="toc-section-number">2.4</span> Prioritised replay</a></li>
<li><a href="#sec:duelling-network"><span class="toc-section-number">2.5</span> Duelling network</a></li>
<li><a href="#sec:distributed-dqn-gorila"><span class="toc-section-number">2.6</span> Distributed DQN (GORILA)</a></li>
<li><a href="#sec:deep-recurrent-q-learning-drqn"><span class="toc-section-number">2.7</span> Deep Recurrent Q-learning (DRQN)</a></li>
<li><a href="#sec:other-variants-of-dqn"><span class="toc-section-number">2.8</span> Other variants of DQN</a></li>
</ul></li>
<li><a href="#sec:policy-based-methods"><span class="toc-section-number">3</span> Policy-based methods</a><ul>
<li><a href="#sec:actor-critic"><span class="toc-section-number">3.1</span> Actor-critic</a><ul>
<li><a href="#sec:reinforce"><span class="toc-section-number">3.1.1</span> REINFORCE</a></li>
<li><a href="#sec:advantage-actor-critic-a2c"><span class="toc-section-number">3.1.2</span> Advantage Actor-Critic (A2C)</a></li>
<li><a href="#sec:asynchronous-advantage-actor-critic-a3c"><span class="toc-section-number">3.1.3</span> Asynchronous Advantage Actor-critic (A3C)</a></li>
</ul></li>
<li><a href="#sec:policy-gradient"><span class="toc-section-number">3.2</span> Policy gradient</a><ul>
<li><a href="#sec:stochastic-value-gradient-svg"><span class="toc-section-number">3.2.1</span> Stochastic Value Gradient (SVG)</a></li>
<li><a href="#sec:deterministic-policy-gradient-dpg"><span class="toc-section-number">3.2.2</span> Deterministic Policy Gradient (DPG)</a></li>
<li><a href="#sec:deep-deterministic-policy-gradient-ddpg"><span class="toc-section-number">3.2.3</span> Deep Deterministic Policy Gradient (DDPG)</a></li>
<li><a href="#sec:fictitious-self-play-fsp"><span class="toc-section-number">3.2.4</span> Fictitious Self-Play (FSP)</a></li>
<li><a href="#sec:trust-region-policy-optimization-trpo"><span class="toc-section-number">3.2.5</span> Trust Region Policy Optimization (TRPO)</a></li>
<li><a href="#sec:generalized-advantage-estimation-gae"><span class="toc-section-number">3.2.6</span> Generalized Advantage Estimation (GAE)</a></li>
<li><a href="#sec:proximal-policy-optimization-ppo"><span class="toc-section-number">3.2.7</span> Proximal Policy Optimization (PPO)</a></li>
<li><a href="#sec:cross-entropy-method-cem"><span class="toc-section-number">3.2.8</span> Cross-entropy Method (CEM)</a></li>
<li><a href="#sec:comparison-between-value-based-and-policy-based-methods"><span class="toc-section-number">3.2.9</span> Comparison between value-based and policy-based methods</a></li>
</ul></li>
</ul></li>
<li><a href="#sec:recurrent-attention-models"><span class="toc-section-number">4</span> Recurrent Attention Models</a></li>
<li><a href="#sec:model-based-rl"><span class="toc-section-number">5</span> Model-based RL</a></li>
<li><a href="#sec:deep-rl-for-robotics"><span class="toc-section-number">6</span> Deep RL for robotics</a></li>
<li><a href="#sec:rl-libraries"><span class="toc-section-number">7</span> RL libraries</a><ul>
<li><a href="#sec:environments"><span class="toc-section-number">7.1</span> Environments</a></li>
<li><a href="#sec:algorithms"><span class="toc-section-number">7.2</span> Algorithms</a></li>
</ul></li>
<li><a href="#sec:thanks">Thanks</a></li>
<li><a href="#sec:notes">Notes</a></li>
<li><a href="#sec:references">References</a></li>
</ul>
</nav>

<p>Different classes of deep RL can be identified. This document focuses on the following ones:</p>
<ol type="1">
<li>Value-based algorithms (DQN…) used mostly for discrete problems like video games.</li>
<li>Policy-based algorithms (A3C, DDPG…) used for continuous control problems such as robotics.</li>
<li>Recurrent attention models (RAM…) for partially observable problems.</li>
<li>Model-based RL to reduce the sample complexity by incorporating a model of the environment.</li>
<li>Application of deep RL to robotics</li>
</ol>
<p><strong>Additional resources</strong></p>
<p>See <span class="citation" data-cites="Li2017">Li (<a href="#ref-Li2017">2017</a>)</span>, <span class="citation" data-cites="Arulkumaran2017">Arulkumaran, Deisenroth, Brundage, and Bharath (<a href="#ref-Arulkumaran2017">2017</a>)</span> and <span class="citation" data-cites="Mousavi2018">Mousavi, Schukat, and Howley (<a href="#ref-Mousavi2018">2018</a>)</span> for recent overviews of deep RL.</p>
<p>This series of posts from Arthur Juliani <a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0" class="uri">https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0</a> also provide a very good introduction to deep RL, associated to code samples using tensorflow.</p>
<h1 id="sec:basics"><span class="header-section-number">1</span> Basics</h1>
<p>Deep reinforcement learning (deep RL) is the successful integration of deep learning methods, classically used in supervised or unsupervised learning contexts, with reinforcement learning (RL), a well-studied adaptive control method used in problems with delayed and partial feedback <span class="citation" data-cites="Sutton1998">(Sutton and Barto, <a href="#ref-Sutton1998">1998</a>)</span>. This section starts with the basics of RL, mostly to set the notations, and provides a quick overview of deep neural networks.</p>
<h2 id="sec:reinforcement-learning"><span class="header-section-number">1.1</span> Reinforcement learning</h2>
<p>RL methods apply to problems where an agent interacts with an environment in discrete time steps (Fig. <a href="#fig:agentenv">1</a>). At time <span class="math inline">\(t\)</span>, the agent is in state <span class="math inline">\(s_t\)</span> and decides to perform an action <span class="math inline">\(a_t\)</span>. At the next time step, it arrives in the state <span class="math inline">\(s_{t+1}\)</span> and obtains the reward <span class="math inline">\(r_{t+1}\)</span>. The goal of the agent is to maximize the reward obtained on the long term. The textbook by <span class="citation" data-cites="Sutton1998">Sutton and Barto (<a href="#ref-Sutton1998">1998</a>)</span> (updated in <span class="citation" data-cites="Sutton2017">Sutton and Barto (<a href="#ref-Sutton2017">2017</a>)</span>) defines the field extensively.</p>
<figure>
<img src="img/rl-agent.jpg" alt="Figure 1: Interaction between an agent and its environment. Taken from Sutton and Barto (1998)." id="fig:agentenv" style="width:50.0%" /><figcaption>Figure 1: Interaction between an agent and its environment. Taken from <span class="citation" data-cites="Sutton1998">Sutton and Barto (<a href="#ref-Sutton1998">1998</a>)</span>.</figcaption>
</figure>
<h3 id="sec:mdp-markov-decision-process"><span class="header-section-number">1.1.1</span> MDP: Markov Decision Process</h3>
<p>Most reinforcement learning problems are modeled as <strong>Markov Decision Processes</strong> (MDP) defined by five quantities:</p>
<ul>
<li>a state space <span class="math inline">\(\mathcal{S}\)</span> where each state <span class="math inline">\(s\)</span> respects the Markovian property. It can be finite or infinite.</li>
<li>an action space <span class="math inline">\(\mathcal{A}\)</span> of actions <span class="math inline">\(a\)</span>, which can be finite or infinite, discrete or continuous.</li>
<li>an initial state distribution <span class="math inline">\(p_0(s_0)\)</span> (from which states is the agent likely to start).</li>
<li>a transition dynamics model with density <span class="math inline">\(p(s&#39;|s, a)\)</span>, sometimes noted <span class="math inline">\(\mathcal{P}_{ss&#39;}^a\)</span>. It defines the probability of arriving in the state <span class="math inline">\(s&#39;\)</span> at time <span class="math inline">\(t+1\)</span> when being in the state <span class="math inline">\(s\)</span> and performing the action <span class="math inline">\(a\)</span> a time <span class="math inline">\(t\)</span>.</li>
<li>a reward function <span class="math inline">\(r(s, a, s&#39;) : \mathcal{S}\times\mathcal{A}\times\mathcal{S} \rightarrow \Re\)</span> defining the (stochastic) reward obtained after performing <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span> and arriving in <span class="math inline">\(s&#39;\)</span>.</li>
</ul>
<p>The <strong>Markovian property</strong> states that:</p>
<p><span><span class="math display">\[
    p(s_{t+1}|s_t, a_t) = p(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, \dots s_0, a_0)
\qquad(1)\]</span></span></p>
<p>i.e. you do not need the full history of the agent to predict where it will arrive after an action. If this is not the case, RL methods will not converge (or poorly). For example, solving video games by using a single frame as state is not Markovian: you can not predict in which direction an object is moving based on a single frame. You will either need to stack several consecutive frames together to create Markovian states, or use recurrent networks in the model to integrate non-Markovian states over time and imitate that property (see Section <a href="#sec:recurrent-attention-models">4</a>).</p>
<h3 id="sec:pomdp-partially-observable-markov-decision-process"><span class="header-section-number">1.1.2</span> POMDP: Partially Observable Markov Decision Process</h3>
<p>In many problems (e.g. vision-based), one does not have access to the true states of the agent, but one can only indirectly observe them. For example, in a video game, the true state is defined by a couple of variables: coordinates <span class="math inline">\((x, y)\)</span> of the two players, position of the ball, speed, etc. However, all you have access to are the raw pixels: sometimes the ball may be hidden behing a wall or a tree, but it still exists in the state space. Speed information is also not observable in a single frame.</p>
<p>In a <strong>Partially Observable Markov Decision Process</strong> (POMDP), observations <span class="math inline">\(o_t\)</span> come from a space <span class="math inline">\(\mathcal{O}\)</span> and are linked to underlying states using the density function <span class="math inline">\(p(o_t| s_t)\)</span>. Observations are usually not Markovian, so the full history of observations <span class="math inline">\(h_t = (o_0, a_0, \dots o_t, a_t)\)</span> is needed to solve the problem (see Section <a href="#sec:recurrent-attention-models">4</a>).</p>
<h3 id="sec:policy-and-value-fucntions"><span class="header-section-number">1.1.3</span> Policy and value fucntions</h3>
<p>The policy defines the behavior of the agent: which action should be taken in each state. One distinguishes two kinds of policies:</p>
<ul>
<li>a stochastic policy <span class="math inline">\(\pi : \mathcal{S} \rightarrow P(\mathcal{A})\)</span> defines the probability distribution <span class="math inline">\(P(\mathcal{A})\)</span> of performing an action.</li>
<li>a deterministic policy <span class="math inline">\(\mu(s_t)\)</span> is a discrete mapping of <span class="math inline">\(\mathcal{S} \rightarrow \mathcal{A}\)</span>.</li>
</ul>
<p>The policy can be used to explore the environment and generate trajectories of states, rewards and actions. The performance of a policy is determined by calculating the <strong>expected discounted return</strong>, i.e. the sum of all rewards received from time step t onwards:</p>
<p><span><span class="math display">\[
    R_t = \sum_{k=0}^{\infty} \gamma^k \, r_{t+k+1}
\qquad(2)\]</span></span></p>
<p>where <span class="math inline">\(0 &lt; \gamma &lt; 1\)</span> is the discount rate and <span class="math inline">\(r_{t+1}\)</span> represents the reward obtained during the transition from <span class="math inline">\(s_t\)</span> to <span class="math inline">\(s_{t+1}\)</span>.</p>
<p>The Q-value of a state-action pair <span class="math inline">\((s, a)\)</span> is defined as the expected discounted reward received if the agent takes <span class="math inline">\(a\)</span> from a state <span class="math inline">\(s\)</span> and follows the policy distribution <span class="math inline">\(\pi\)</span> thereafter:</p>
<p><span><span class="math display">\[
    Q^{\pi}(s, a) = {E}_{\pi}(R_t | s_t = s, a_t=a)
\qquad(3)\]</span></span></p>
<p>Similarly, the V-value of a state <span class="math inline">\(s\)</span> is the expected discounted reward received if the agent starts in <span class="math inline">\(s\)</span> and follows its policy <span class="math inline">\(\pi\)</span>.</p>
<p><span><span class="math display">\[
    V^{\pi}(s) = {E}_{\pi}(R_t | s_t = s)
\qquad(4)\]</span></span></p>
<p>Obviously, these quantities depend on the states/actions themselves (some chessboard configurations are intrinsically better than others, i.e. you are more likely to win from that state), but also on the policy (if you can kill your opponent in one move - meaning you are in an intrinsically good state - but systematically take the wrong decision and lose, this is a bad state).</p>
<h3 id="sec:bellman-equations"><span class="header-section-number">1.1.4</span> Bellman equations</h3>
<p>The V- and Q-values are obviously linked with each other. The value of state depend on the value of the actions possible in that state, modulated by the probability that an action will be taken (i.e. the policy):</p>
<p><span id="eq:v-value"><span class="math display">\[
    V^{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(s, a) \, Q^\pi(s,a)
\qquad(5)\]</span></span></p>
<p>For a deterministic policy (<span class="math inline">\(\pi(s, a) = 1\)</span> if <span class="math inline">\(a=a^*\)</span> and <span class="math inline">\(0\)</span> otherwise), the value of a state is the same as the value of the action that will be systematically taken.</p>
<p>Noting that:</p>
<p><span id="eq:return"><span class="math display">\[
    R_t = r_{t+1} + \gamma R_{t+1}
\qquad(6)\]</span></span></p>
<p>i.e. that the expected return at time <span class="math inline">\(t\)</span> is the sum of the immediate reward received during the next transition <span class="math inline">\(r_{t+1}\)</span> and of the expected return at the next state (<span class="math inline">\(R_{t+1}\)</span>, discounted by <span class="math inline">\(\gamma\)</span>), we can also write:</p>
<p><span id="eq:q-value"><span class="math display">\[
    Q^{\pi}(s, a) = \sum_{s&#39; \in \mathcal{S}} p(s&#39;|s, a) [r(s, a, s&#39;) + \gamma \, V^\pi(s&#39;)]
\qquad(7)\]</span></span></p>
<p>The value of an action depends on which state you arrive in (<span class="math inline">\(s&#39;\)</span>), with which probability (<span class="math inline">\(p(s&#39;|s, a)\)</span>), how much reward you receive immediately (<span class="math inline">\(r(s, a, s&#39;)\)</span>) and how much you will receive later (summarized by <span class="math inline">\(V^\pi(s&#39;)\)</span>).</p>
<p>Putting together Eq. <a href="#eq:v-value">5</a> and Eq. <a href="#eq:q-value">7</a>, we obtain the <strong>Bellman equations</strong>:</p>
<p><span><span class="math display">\[
    V^{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(s, a) \, \sum_{s&#39; \in \mathcal{S}} p(s&#39;|s, a) [r(s, a, s&#39;) + \gamma \, V^\pi(s&#39;)]
\qquad(8)\]</span></span></p>
<p><span><span class="math display">\[
    Q^{\pi}(s, a) = \sum_{s&#39; \in \mathcal{S}} p(s&#39;|s, a) [r(s, a, s&#39;) + \gamma \, \sum_{a&#39; \in \mathcal{A}} \pi(s&#39;, a&#39;) \, Q^\pi(s&#39;,a&#39;)]
\qquad(9)\]</span></span></p>
<p>The Bellman equations mean that the value of a state (resp. state-action pair) depends on the value of all other states (resp. state-action pairs), the current policy <span class="math inline">\(\pi\)</span> and the dynamics of the MDP (<span class="math inline">\(p(s&#39;|s, a)\)</span> and <span class="math inline">\(r(s, a, s&#39;)\)</span>).</p>
<figure>
<img src="img/backup.png" alt="Figure 2: Backup diagrams correponding to the Bellman equations. Taken from Sutton and Barto (1998)." id="fig:backup" style="width:50.0%" /><figcaption>Figure 2: Backup diagrams correponding to the Bellman equations. Taken from <span class="citation" data-cites="Sutton1998">Sutton and Barto (<a href="#ref-Sutton1998">1998</a>)</span>.</figcaption>
</figure>
<h3 id="sec:dynamic-programming"><span class="header-section-number">1.1.5</span> Dynamic programming</h3>
<p>The interesting property of the Bellman equations is that, if the states have the Markovian property, they admit <em>one and only one</em> solution. This means that for a given policy, if the dynamics of the MDP are known, it is possible to compute the value of all states or state-action pairs by solving the Bellman equations for all states or state-action pairs (<em>policy evaluation</em>).</p>
<p>Once the values are known for a given policy, it is possible to improve the policy by selecting with the highest probability the action with the highest Q-value. For example, if the current policy chooses the action <span class="math inline">\(a_1\)</span> over <span class="math inline">\(a_2\)</span> in <span class="math inline">\(s\)</span> (<span class="math inline">\(\pi(s, a_1) &gt; \pi(s, a_2)\)</span>), but after evaluating the policy it turns out that <span class="math inline">\(Q^\pi(s, a_2) &gt; Q^\pi(s, a_1)\)</span> (the expected return after <span class="math inline">\(a_2\)</span> is higher than after <span class="math inline">\(a_1\)</span>), it makes more sense to preferentially select <span class="math inline">\(a_2\)</span>, as there is more reward afterwards. We can then create a new policy <span class="math inline">\(\pi&#39;\)</span> where <span class="math inline">\(\pi&#39;(s, a_2) &gt; \pi&#39;(s, a_1)\)</span>, which is is <em>better</em> policy than <span class="math inline">\(\pi\)</span> as more reward can be gathered after <span class="math inline">\(s\)</span>.</p>
<figure>
<img src="img/dynamicprogramming.png" alt="Figure 3: Dynamic programming alternates between policy evaluation and policy improvement. Taken from Sutton and Barto (1998)." id="fig:dynamicprogramming" style="width:20.0%" /><figcaption>Figure 3: Dynamic programming alternates between policy evaluation and policy improvement. Taken from <span class="citation" data-cites="Sutton1998">Sutton and Barto (<a href="#ref-Sutton1998">1998</a>)</span>.</figcaption>
</figure>
<p><strong>Dynamic programming</strong> (DP) alternates between policy evaluation and policy improvement. If the problem is Markovian, it can be shown that DP converges to the <em>optimal policy</em> <span class="math inline">\(\pi^*\)</span>, i.e. the policy where the expected return is maximal in all states.</p>
<p>Note that by definition the optimal policy is <em>deterministic</em> and <em>greedy</em>: if there is an action with a maximal Q-value for the optimal policy, it should be systematically taken. For the optimal policy <span class="math inline">\(\pi^*\)</span>, the Bellman equations become:</p>
<p><span><span class="math display">\[
    V^{*}(s) = \max_{a \in \mathcal{A}} \sum_{s \in \mathcal{S}} p(s&#39; | s, a) \cdot [ r(s, a, s&#39;) + \gamma \cdot V^{*} (s&#39;) ]
\qquad(10)\]</span></span></p>
<p><span><span class="math display">\[
    Q^{*}(s, a) = \sum_{s&#39; \in \mathcal{S}} p(s&#39; | s, a) \cdot [r(s, a, s&#39;) + \gamma \max_{a&#39; \in \mathcal{A}} Q^* (s&#39;, a&#39;) ]
\qquad(11)\]</span></span></p>
<p>Dynamic programming can only be used when:</p>
<ul>
<li>the dynamics of the MDP (<span class="math inline">\(p(s&#39;|s, a)\)</span> and <span class="math inline">\(r(s, a, s&#39;)\)</span>) are fully known.</li>
<li>the number of states and state-action pairs is small (one Bellman equation per state or state/action to solve).</li>
</ul>
<p>In practice, sample-based methods such as Monte-Carlo or temporal difference are used.</p>
<h3 id="sec:monte-carlo-sampling"><span class="header-section-number">1.1.6</span> Monte-Carlo sampling</h3>
<figure>
<img src="img/unifiedreturn.png" alt="Figure 4: Monte-Carlo methods accumulate rewards over a complete episode. Taken from Sutton and Barto (1998)." id="fig:mc" style="width:50.0%" /><figcaption>Figure 4: Monte-Carlo methods accumulate rewards over a complete episode. Taken from <span class="citation" data-cites="Sutton1998">Sutton and Barto (<a href="#ref-Sutton1998">1998</a>)</span>.</figcaption>
</figure>
<p>When the environment is <em>a priori</em> unknown, it has to be explored in order to build estimates of the V or Q value functions. The key idea of <strong>Monte-Carlo</strong> sampling (MC) is rather simple:</p>
<ol type="1">
<li>Start from a state <span class="math inline">\(s_0\)</span>.</li>
<li>Perform an episode (sequence of state-action transitions) until a terminal state <span class="math inline">\(s_T\)</span> is reached using your current policy <span class="math inline">\(\pi\)</span>.</li>
<li>Accumulate the rewards into the actual return for that episode <span class="math inline">\(R_t^{(e)} = \sum_{k=0}^T r_{t+k+1}\)</span> for each time step.</li>
<li>Repeat often enough so that the value of a state <span class="math inline">\(s\)</span> can be approximated by the average of many actual returns:</li>
</ol>
<p><span><span class="math display">\[V^\pi(s) = E^\pi(R_t | s_t = s) = \frac{1}{M} \sum_{e=1}^M R_t^{(e)}\qquad(12)\]</span></span></p>
<p>In practice, the estimated values are updated using continuous updates:</p>
<p><span><span class="math display">\[
    V^\pi(s) \leftarrow V^\pi(s) + \alpha (R_t - V^\pi(s))
\qquad(13)\]</span></span></p>
<p>Q-values can also be approximated using the same procedure:</p>
<p><span><span class="math display">\[
    Q^\pi(s, a) \leftarrow Q^\pi(s, a) + \alpha (R_t - Q^\pi(s, a))
\qquad(14)\]</span></span></p>
<p>The two main drawbacks of MC methods are:</p>
<ol type="1">
<li>The task must be episodic, i.e. stop after a finite amount of transitions. Updates are only applied at the end of an episode.</li>
<li>A sufficient level of exploration has to be ensured to make sure the estimates converge to the optimal values.</li>
</ol>
<p>The second issue is linked to the <strong>exploration-exploitation</strong> dilemma: the episode is generated using the current policy (or a policy derived from it, see later). If the policy always select the same actions from the beginning (exploitation), the agent will never discover better alternatives: the values will converge to a local minimum. If the policy always pick randomly actions (exploration), the policy which is evaluated is not the current policy <span class="math inline">\(\pi\)</span>, but the random policy. A trade-off between the two therefore has to be maintained: usually a lot of exploration at the beginning of learning to accumulate knowledge about the environment, less towards the end to actually use the knowledge and perform optimally.</p>
<p>There are two types of methods trying to cope with exploration:</p>
<ul>
<li><strong>On-policy</strong> methods generate the episodes using the learned policy <span class="math inline">\(\pi\)</span>, but it has to be <em><span class="math inline">\(\epsilon\)</span>-soft</em>, i.e. stochastic: it has to let a probability of at least <span class="math inline">\(\epsilon\)</span> of selecting another action than the greedy action (the one with the highest estimated Q-value).</li>
<li><strong>Off-policy</strong> methods use a second policy called the <em>behavior policy</em> to generate the episodes, but learn a different policy for exploitation, which can even be deterministic.</li>
</ul>
<p><span class="math inline">\(\epsilon\)</span>-soft policies are easy to create. The simplest one is the <strong><span class="math inline">\(\epsilon\)</span>-greedy</strong> action selection method, which assigns a probability <span class="math inline">\((1-\epsilon)\)</span> of selecting the greedy action (the one with the highest Q-value), and a probability <span class="math inline">\(\epsilon\)</span> of selecting any of the other available actions:</p>
<p><span><span class="math display">\[ 
    a_t = \begin{cases} a_t^* \quad \text{with probability} \quad (1 - \epsilon) \\
                       \text{any other action with probability } \epsilon \end{cases}
\qquad(15)\]</span></span></p>
<p>Another solution is the <strong>Softmax</strong> (or Gibbs distribution) action selection method, which assigns to each action a probability of being selected depending on their relative Q-values:</p>
<p><span><span class="math display">\[
    P(s, a) = \frac{\exp Q^\pi(s, a) / \tau}{ \sum_b \exp Q^\pi(s, b) / \tau}
\qquad(16)\]</span></span></p>
<p><span class="math inline">\(\tau\)</span> is a positive parameter called the temperature: high temperatures cause the actions to be nearly equiprobable, while low temperatures cause <span class="math inline">\(\tau\)</span> is a positive parameter called the temperature.</p>
<p>The advantage of off-policy methods is that domain knowledge can be used to restrict the search in the state-action space. For example, only moves actually played by chess experts in a given state will be actually explored, not random stupid moves. The obvious drawback being that if the optimal solution is not explored by the behavior policy, the agent has no way to discover it by itself.</p>
<h3 id="sec:temporal-difference"><span class="header-section-number">1.1.7</span> Temporal Difference</h3>
<p>The main drawback of Monte-Carlo methods is that the task must be composed of finite episodes. Not only is it not always possible, but value updates have to wait for the end of the episode, what slows learning down. <strong>Temporal difference</strong> methods simply replace the actual return obtained after a state or an action, by an estimation composed of the reward immediately received plus the value of the next state or action, as in Eq. <a href="#eq:return">6</a>:</p>
<p><span><span class="math display">\[
    R_t \approx r(s, a, s&#39;) + \gamma \, V^\pi(s&#39;) \approx r + \gamma \, Q^\pi(s&#39;, a&#39;)
\qquad(17)\]</span></span></p>
<p>This gives us the following learning rules:</p>
<p><span><span class="math display">\[
    V^\pi(s) \leftarrow V^\pi(s) + \alpha (r(s, a, s&#39;) + \gamma \, V^\pi(s&#39;) - V^\pi(s))
\qquad(18)\]</span></span></p>
<p><span><span class="math display">\[
    Q^\pi(s, a) \leftarrow Q^\pi(s, a) + \alpha (r(s, a, s&#39;) + \gamma \, Q^\pi(s&#39;, a&#39;) - Q^\pi(s, a))
\qquad(19)\]</span></span></p>
<p>The quantity:</p>
<p><span><span class="math display">\[
 \delta = r(s, a, s&#39;) + \gamma \, V^\pi(s&#39;) - V^\pi(s)
\qquad(20)\]</span></span></p>
<p>or:</p>
<p><span><span class="math display">\[
    \delta = r(s, a, s&#39;) + \gamma \, Q^\pi(s&#39;, a&#39;) - Q^\pi(s, a)
\qquad(21)\]</span></span></p>
<p>is called the <strong>reward-prediction error</strong> (RPE) or <strong>TD error</strong>: it defines the surprise between the current reward prediction (<span class="math inline">\(V^\pi(s)\)</span> or <span class="math inline">\(Q^\pi(s, a)\)</span>) and the sum of the immediate reward plus the reward prediction in the next state / after the next action.</p>
<ul>
<li>If <span class="math inline">\(\delta &gt; 0\)</span>, the transition was positively surprising: one obtains more reward or lands in a better state than expected. The initial state or action was actually underrated, so its estimated value must be increased.</li>
<li>If <span class="math inline">\(\delta &lt; 0\)</span>, the transition was negatively surprising. The initial state or action was overrated, its value must be decreased.</li>
<li>If <span class="math inline">\(\delta = 0\)</span>, the transition was fully predicted: one obtains as much reward as expected, so the values should stay as they are.</li>
</ul>
<p>The main advantage of this learning method is that the update of the V- or Q-value can be applied immediately after a transition: no need to wait until the end of an episode, or even to have episodes at all.</p>
<figure>
<img src="img/backup-TD.png" alt="Figure 5: Temporal difference algorithms update values after a single transition. Taken from Sutton and Barto (1998)." id="fig:td" style="width:3.0%" /><figcaption>Figure 5: Temporal difference algorithms update values after a single transition. Taken from <span class="citation" data-cites="Sutton1998">Sutton and Barto (<a href="#ref-Sutton1998">1998</a>)</span>.</figcaption>
</figure>
<p>When learning Q-values directly, the question is which next action <span class="math inline">\(a&#39;\)</span> should be used in the update rule: the action that will actually be taken for the next transition (defined by <span class="math inline">\(\pi(s&#39;, a&#39;)\)</span>), or the greedy action (<span class="math inline">\(a^* = \text{argmax}_a Q^\pi(s&#39;, a)\)</span>). This relates to the <em>on-policy / off-policy</em> distinction already seen for MC methods:</p>
<ul>
<li><em>On-policy</em> TD learning is called <strong>SARSA</strong> (state-action-reward-state-action). It uses the next action sampled from the policy <span class="math inline">\(\pi(s&#39;, a&#39;)\)</span> to update the current transition. This selected action could be noted <span class="math inline">\(\pi(s&#39;)\)</span> for simplicity. It is required that this next action will actually be performed for the next transition. The policy must be <span class="math inline">\(\epsilon\)</span>-soft, for example <span class="math inline">\(\epsilon\)</span>-greedy or softmax:</li>
</ul>
<p><span><span class="math display">\[
    \delta = r(s, a, s&#39;) + \gamma \, Q^\pi(s&#39;, \pi(s&#39;)) - Q^\pi(s, a)
\qquad(22)\]</span></span></p>
<ul>
<li><em>Off-policy</em> TD learning is called <strong>Q-learning</strong> <span class="citation" data-cites="Watkins1989">(Watkins, <a href="#ref-Watkins1989">1989</a>)</span>. The greedy action in the next state (the one with the highest Q-value) is used to update the current transition. It does not mean that the greedy action will actually have to be selected for the next transition. The learned policy can therefore also be deterministic:</li>
</ul>
<p><span><span class="math display">\[
    \delta = r(s, a, s&#39;) + \gamma \, \max_{a&#39;} Q^\pi(s&#39;, a&#39;) - Q^\pi(s, a)
\qquad(23)\]</span></span></p>
<p>In Q-learning, the behavior policy has to ensure exploration, while this is achieved implicitely by the learned policy in SARSA, as it must be <span class="math inline">\(\epsilon\)</span>-soft. An easy way of building a behavior policy based on a deterministic learned policy is <span class="math inline">\(\epsilon\)</span>-greedy: the deterministic action <span class="math inline">\(\mu(s_t)\)</span> is chosen with probability 1 - <span class="math inline">\(\epsilon\)</span>, the other actions with probability <span class="math inline">\(\epsilon\)</span>. In continuous action spaces, additive noise (e.g. Ohrstein-Uhlenbeck) can be added to the action.</p>
<p>Alternatively, domain knowledge can be used to create the behavior policy and restrict the search to meaningful actions: compilation of expert moves in games, approximate solutions, etc. Again, the risk is that the behavior policy never explores the actually optimal actions.</p>
<h3 id="sec:actor-critic-architectures"><span class="header-section-number">1.1.8</span> Actor-critic architectures</h3>
<p>Let’s consider the TD error based on state values:</p>
<p><span><span class="math display">\[
 \delta = r(s, a, s&#39;) + \gamma \, V^\pi(s&#39;) - V^\pi(s)
\qquad(24)\]</span></span></p>
<p>As noted in the previous section, the TD error represents how surprisingly good (or bad) a transition between two states has been (ergo the corresponding action). It can be used to update the value of the state <span class="math inline">\(s_t\)</span>:</p>
<p><span><span class="math display">\[
    V^\pi(s) \leftarrow V^\pi(s) + \alpha \, \delta
\qquad(25)\]</span></span></p>
<p>This allows to estimate the values of all states for the current policy. However, this does not help to 1) directy select the best action or 2) improve the policy. When only the V-values are given, one can only want to reach the next state <span class="math inline">\(V^\pi(s&#39;)\)</span> with the highest value: one needs to know which action leads to this better state, i.e. have a model of the environment. Actually, one selects the action with the highest Q-value:</p>
<p><span><span class="math display">\[
    Q^{\pi}(s, a) = \sum_{s&#39; \in \mathcal{S}} p(s&#39;|s, a) [r(s, a, s&#39;) + \gamma \, V^\pi(s&#39;)]
\qquad(26)\]</span></span></p>
<p>An action may lead to a high-valued state, but with such a small probability that it is actually not worth it. <span class="math inline">\(p(s&#39;|s, a)\)</span> and <span class="math inline">\(r(s, a, s&#39;)\)</span> therefore have to be known (or at least approximated), what defeats the purpose of sample-based methods.</p>
<figure>
<img src="img/actorcritic.png" alt="Figure 6: Actor-critic architecture (Sutton and Barto, 1998)." id="fig:actorcritic" style="width:30.0%" /><figcaption>Figure 6: Actor-critic architecture <span class="citation" data-cites="Sutton1998">(Sutton and Barto, <a href="#ref-Sutton1998">1998</a>)</span>.</figcaption>
</figure>
<p><strong>Actor-critic</strong> architectures have been proposed to solve this problem:</p>
<ol type="1">
<li>The <strong>critic</strong> learns to estimate the value of a state <span class="math inline">\(V^\pi(s)\)</span> and compute the RPE <span class="math inline">\(\delta = r(s, a, s&#39;) + \gamma \, V^\pi(s&#39;) - V^\pi(s)\)</span>.</li>
<li>The <strong>actor</strong> uses the RPE to update a <em>preference</em> for the executed action: action with positive RPEs (positively surprising) should be reinforced (i.e. taken again in the future), while actions with negative RPEs should be avoided in the future.</li>
</ol>
<p>The main interest of this architecture is that the actor can take any form (neural network, decision tree), as long as it able to use the RPE for learning. The simplest actor would be a softmax action selection mechanism, which maintains a <em>preference</em> <span class="math inline">\(p(s, a)\)</span> for each action and updates it using the TD error:</p>
<p><span><span class="math display">\[
    p(s, a) \leftarrow p(s, a) + \alpha \, \delta_t
\qquad(27)\]</span></span></p>
<p>The policy uses the softmax rule on these preferences:</p>
<p><span><span class="math display">\[
    \pi(s, a) = \frac{p(s, a)}{\sum_a p(s, a)}
\qquad(28)\]</span></span></p>
<p>Actor-critic algorithms learn at the same time two aspects of the problem:</p>
<ul>
<li>A value function (e.g. <span class="math inline">\(V^\pi(s)\)</span>) to compute the TD error in the critic,</li>
<li>A policy <span class="math inline">\(\pi\)</span> in the actor.</li>
</ul>
<p>Classical TD learning only learn a value function (<span class="math inline">\(V^\pi(s)\)</span> or <span class="math inline">\(Q^\pi(s, a)\)</span>): these methods are called <strong>value-based</strong> methods. Actor-critic architectures are an example of <strong>policy-based</strong> methods (see Section <a href="#sec:policy-based-methods">3</a>).</p>
<h3 id="sec:function-approximation"><span class="header-section-number">1.1.9</span> Function approximation</h3>
<p>All the methods presented before are <em>tabular methods</em>, as one needs to store one value per state-action pair: either the Q-value of the action or a preference for that action. In most useful applications, the number of values to store would quickly become redhibitory: when working on raw images, the number of possible states alone is untractable. Moreover, these algorithms require that each state-action pair is visited a sufficient number of times to converge towards the optimal policy: if a single state-action pair is never visited, there is no guarantee that the optimal policy will be found. The problem becomes even more obvious when considering <em>continuous</em> state or action spaces.</p>
<p>However, in a lot of applications, the optimal action to perform in two very close states is likely to be the same: changing one pixel in a video game does not change which action should be applied. It would therefore be very useful to be able to <em>interpolate</em> Q-values between different states: only a subset of all state-action pairs has to explored; the others will be “guessed” depending on the proximity between the states and/or the actions. The problem is now <strong>generalization</strong>, i.e. transferring acquired knowledge to unseen but similar situations.</p>
<p>This is where <strong>function approximation</strong> becomes useful: the Q-values or the policy are not stored in a table, but rather learned by a function approximator. The type of function approximator does not really matter here: in deep RL we are of course interested in deep neural networks (Section <a href="#sec:deep-learning">1.2</a>), but any kind of regressor theoretically works (linear algorithms, radial-basis function network, SVR…).</p>
<h4 id="sec:value-based-function-approximation" class="unnumbered">Value-based function approximation</h4>
<p>In <strong>value-based</strong> methods, we want to approximate the Q-values <span class="math inline">\(Q^\pi(s,a)\)</span> of all possible state-action pairs for a given policy. The function approximator depends on a set of parameters <span class="math inline">\(\theta\)</span>. <span class="math inline">\(\theta\)</span> can for example represent all the weights and biases of a neural network. The approximated Q-value can now be noted <span class="math inline">\(Q(s, a ;\theta)\)</span> or <span class="math inline">\(Q_\theta(s, a)\)</span>. As the parameters will change over time during learning, we can omit the time <span class="math inline">\(t\)</span> from the notation. Similarly, action selection is usually <span class="math inline">\(\epsilon\)</span>-greedy or softmax, so the policy <span class="math inline">\(\pi\)</span> depends directly on the estimated Q-values and can therefore on the parameters: it is noted <span class="math inline">\(\pi_\theta\)</span>.</p>
<figure>
<img src="img/functionapprox.png" alt="Figure 7: Function approximators can either take a state-action pair as input and output the Q-value, or simply take a state as input and output the Q-values of all possible actions." id="fig:functionapprox" style="width:50.0%" /><figcaption>Figure 7: Function approximators can either take a state-action pair as input and output the Q-value, or simply take a state as input and output the Q-values of all possible actions.</figcaption>
</figure>
<p>There are basically two options regarding the structure of the function approximator (Fig. <a href="#fig:functionapprox">7</a>):</p>
<ol type="1">
<li>The approximator takes a state-action pair <span class="math inline">\((s, a)\)</span> as input and returns a single Q-value <span class="math inline">\(Q(s, a)\)</span>.</li>
<li>It takes a state <span class="math inline">\(s\)</span> as input and returns the Q-value of all possible actions in that state.</li>
</ol>
<p>The second option is of course only possible when the action space is discrete, but has the advantage to generalize better over similar states.</p>
<p>The goal of a function approximator is to minimize a <em>loss function</em> (or cost function) <span class="math inline">\(\mathcal{L}(\theta)\)</span>, so that the estimated Q-values converge for all state-pairs towards their target value, depending on the chosen algorithm:</p>
<ul>
<li>Monte-Carlo methods: the Q-value of each <span class="math inline">\((s, a)\)</span> pair should converge towards the mean expected return (mathematical expectation):</li>
</ul>
<p><span><span class="math display">\[
    \mathcal{L}(\theta) = E([R_t - Q_\theta(s, a)]^2)
\qquad(29)\]</span></span></p>
<p>If we learn over <span class="math inline">\(M\)</span> episodes of length <span class="math inline">\(T\)</span>, the loss function can be written as:</p>
<p><span><span class="math display">\[
    \mathcal{L}(\theta) = \frac{1}{M\, T} \sum_{e=1}^M \sum_{t = 1}^T [R^e_t - Q_\theta(s_t, a_t)]^2
\qquad(30)\]</span></span></p>
<ul>
<li><p>Temporal difference methods: the Q-values should converge towards an estimation of the mean expected return.</p>
<ul>
<li>For SARSA:</li>
</ul>
<p><span><span class="math display">\[
  \mathcal{L}(\theta) = E([r(s, a, s&#39;) + \gamma \, Q_\theta(s&#39;, \pi(s&#39;)) - Q_\theta(s, a)]^2)
  \qquad(31)\]</span></span></p>
<ul>
<li>For Q-learning:</li>
</ul>
<p><span><span class="math display">\[
  \mathcal{L}(\theta) = E([r(s, a, s&#39;) + \gamma \, \max_{a&#39;} Q_\theta(s&#39;, a&#39;) - Q_\theta(s, a)]^2)
  \qquad(32)\]</span></span></p></li>
</ul>
<p>Any function approximator able to minimize these loss functions can be used.</p>
<h4 id="sec:policy-based-function-approximation" class="unnumbered">Policy-based function approximation</h4>
<p>In policy-based function approximation, we want to directly learn a policy <span class="math inline">\(\pi_\theta(s, a)\)</span> that maximizes the expected return of each possible transition, i.e. the ones which are selected by the policy. The <strong>objective function</strong> to be maximized is therefore defined over all trajectories <span class="math inline">\(\tau = (s_t, a_t, s_{t+1}, a_{t+1}, \ldots)\)</span>:</p>
<p><span><span class="math display">\[
    J(\theta) = {E}_{\tau \sim \pi_\theta}(R_t)
\qquad(33)\]</span></span></p>
<p>In short, the learned policy <span class="math inline">\(\pi_\theta\)</span> should only produce trajectories <span class="math inline">\(\tau\)</span> where each state is associated to a high expected return <span class="math inline">\(R_t\)</span> and avoid trajectories with low expected returns. Although this objective function leads to the desired behaviour, it is not computationally tractable as we would need to integrate over all possible trajectories. The methods presented in Section <a href="#sec:policy-based-methods">3</a> will provide estimates of the gradient of this objective function.</p>
<h2 id="sec:deep-learning"><span class="header-section-number">1.2</span> Deep learning</h2>
<p>Deep RL uses deep neural networks as function approximators, allowing complex representations of the value of state-action pairs to be learned. This section provides a very quick overview of deep learning. For additional details, refer to the excellent book of <span class="citation" data-cites="Goodfellow2016">Goodfellow, Bengio, and Courville (<a href="#ref-Goodfellow2016">2016</a>)</span>.</p>
<h3 id="sec:deep-neural-networks"><span class="header-section-number">1.2.1</span> Deep neural networks</h3>
<p>A deep neural network (DNN) consists of one input layer <span class="math inline">\(\mathbf{x}\)</span>, one or several hidden layers <span class="math inline">\(\mathbf{h_1}, \mathbf{h_2}, \ldots, \mathbf{h_n}\)</span> and one output layer <span class="math inline">\(\mathbf{y}\)</span> (Fig. <a href="#fig:dnn">8</a>).</p>
<figure>
<img src="img/dnn.png" alt="Figure 8: Architecture of a deep neural network. Figure taken from Nielsen (2015), CC-BY-NC." id="fig:dnn" /><figcaption>Figure 8: Architecture of a deep neural network. Figure taken from <span class="citation" data-cites="Nielsen2015">Nielsen (<a href="#ref-Nielsen2015">2015</a>)</span>, CC-BY-NC.</figcaption>
</figure>
<p>Each layer <span class="math inline">\(k\)</span> (called <em>fully-connected</em>) transforms the activity of the previous layer (the vector <span class="math inline">\(\mathbf{h_{k-1}}\)</span>) into another vector <span class="math inline">\(\mathbf{h_{k}}\)</span> by multiplying it with a <strong>weight matrix</strong> <span class="math inline">\(W_k\)</span>, adding a <strong>bias</strong> vector <span class="math inline">\(\mathbf{b_k}\)</span> and applying a non-linear <strong>activation function</strong> <span class="math inline">\(f\)</span>.</p>
<p><span id="eq:fullyconnected"><span class="math display">\[
    \mathbf{h_{k}} = f(W_k \times \mathbf{h_{k-1}} + \mathbf{b_k})
\qquad(34)\]</span></span></p>
<p>The activation function can theoretically be of any type as long as it is non-linear (sigmoid, tanh…), but modern neural networks use preferentially the <strong>Rectified Linear Unit</strong> (ReLU) function <span class="math inline">\(f(x) = \max(0, x)\)</span> or its parameterized variants.</p>
<p>The goal of learning is to find the weights and biases <span class="math inline">\(\theta\)</span> minimizing a given <strong>loss function</strong> on a training set <span class="math inline">\(\mathcal{D}\)</span>.</p>
<ul>
<li>In <em>regression</em> problems, the <strong>mean square error</strong> (mse) is minimized:</li>
</ul>
<p><span><span class="math display">\[
    \mathcal{L}(\theta) = E_{\mathbf{x}, \mathbf{t} \in \mathcal{D}} ||\mathbf{t} - \mathbf{y}||^2
\qquad(35)\]</span></span></p>
<p>where <span class="math inline">\(\mathbf{x}\)</span> is the input, <span class="math inline">\(\mathbf{t}\)</span> the true output (defined in the training set) and <span class="math inline">\(\mathbf{y}\)</span> the prediction of the NN for the input <span class="math inline">\(\mathbf{x}\)</span>. The closer the prediction from the true value, the smaller the mse.</p>
<ul>
<li>In <em>classification</em> problems, the <strong>cross entropy</strong> (or negative log-likelihood) is minimized:</li>
</ul>
<p><span><span class="math display">\[
    \mathcal{L}(\theta) = - E_{\mathbf{x}, \mathbf{t} \in \mathcal{D}} \sum_i t_i \log y_i
\qquad(36)\]</span></span></p>
<p>where the log-likelihood of the prediction <span class="math inline">\(\mathbf{y}\)</span> to match the data <span class="math inline">\(\mathbf{t}\)</span> is maximized over the training set. The mse could be used for classification problems too, but the output layer usually has a softmax activation function for classification problems, which works nicely with the cross entropy loss function. See <a href="https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss" class="uri">https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss</a> for the link between cross entropy and log-likelihood and <a href="https://deepnotes.io/softmax-crossentropy" class="uri">https://deepnotes.io/softmax-crossentropy</a> for the interplay between softmax and cross entropy.</p>
<p>Once the loss function is defined, it has to be minimized by searching optimal values for the free parameters <span class="math inline">\(\theta\)</span>. This optimization procedure is based on <strong>gradient descent</strong>, which is an iterative procedure modifying estimates of the free parameters in the opposite direction of the gradient of the loss function:</p>
<p><span><span class="math display">\[
\Delta \theta = -\eta \, \nabla_\theta \mathcal{L}(\theta) = -\eta \, \frac{\partial \mathcal{L}(\theta)}{\partial \theta}
\qquad(37)\]</span></span></p>
<p>The learning rate <span class="math inline">\(\eta\)</span> is chosen very small to ensure a smooth convergence. Intuitively, the gradient (or partial derivative) represents how the loss function changes when each parameter is slightly increased. If the gradient w.r.t a single parameter (e.g. a weight <span class="math inline">\(w\)</span>) is positive, increasing the weight increases the loss function (i.e. the error), so the weight should be slightly decreased instead. If the gradient is negative, one should increase the weight.</p>
<p>The question is now to compute the gradient of the loss function w.r.t all the parameters of the DNN, i.e. each single weight and bias. The solution is given by the <strong>backpropagation</strong> algorithm, which is simply an application of the <strong>chain rule</strong> to feedforward neural networks:</p>
<p><span><span class="math display">\[
    \frac{\partial \mathcal{L}(\theta)}{\partial W_k} = \frac{\partial \mathcal{L}(\theta)}{\partial \mathbf{y}} \times \frac{\partial \mathbf{y}}{\partial \mathbf{h_n}} \times \frac{\partial \mathbf{h_n}}{\partial \mathbf{h_{n-1}}} \times \ldots \times \frac{\partial \mathbf{h_k}}{\partial W_k}
\qquad(38)\]</span></span></p>
<p>Each layer of the network adds a contribution to the gradient when going <strong>backwards</strong> from the loss function to the parameters. Importantly, all functions used in a NN are differentiable, i.e. those partial derivatives exist (and are easy to compute). For the fully connected layer represented by Eq. <a href="#eq:fullyconnected">34</a>, the partial derivative is given by:</p>
<p><span><span class="math display">\[
    \frac{\partial \mathbf{h_{k}}}{\partial \mathbf{h_{k-1}}} = f&#39;(W_k \times \mathbf{h_{k-1}} + \mathbf{b_k}) \, W_k
\qquad(39)\]</span></span></p>
<p>and its dependency on the parameters is:</p>
<p><span><span class="math display">\[
    \frac{\partial \mathbf{h_{k}}}{\partial W_k} = f&#39;(W_k \times \mathbf{h_{k-1}} + \mathbf{b_k}) \, \mathbf{h_{k-1}}
\qquad(40)\]</span></span> <span><span class="math display">\[
    \frac{\partial \mathbf{h_{k}}}{\partial \mathbf{b_k}} = f&#39;(W_k \times \mathbf{h_{k-1}} + \mathbf{b_k})
\qquad(41)\]</span></span></p>
<p>Activation functions are chosen to have an easy-to-compute derivative, such as the ReLU function:</p>
<p><span><span class="math display">\[
    f&#39;(x) = \begin{cases} 1 \quad \text{if} \quad x &gt; 0 \\ 0 \quad \text{otherwise.} \end{cases}
\qquad(42)\]</span></span></p>
<p>Partial derivatives are automatically computed by the underlying libraries, such as tensorflow, theano, pytorch, etc. The next step is choose an <strong>optimizer</strong>, i.e. a gradient-based optimization method allow to modify the free parameters using the gradients. Optimizers do not work on the whole training set, but use <strong>minibatches</strong> (a random sample of training examples: their number is called the <em>batch size</em>) to compute iteratively the loss function. The most popular optimizers are:</p>
<ul>
<li>SGD (stochastic gradient descent): vanilla gradient descent on random minibatches.</li>
<li>SGD with momentum (Nesterov or not): additional momentum to avoid local minima of the loss function.</li>
<li>Adagrad</li>
<li>Adadelta</li>
<li>RMSprop</li>
<li>Adam</li>
<li>Many others. Check the doc of keras to see what is available: <a href="https://keras.io/optimizers" class="uri">https://keras.io/optimizers</a></li>
</ul>
<p>See this useful post for a comparison of the different optimizers: <a href="http://ruder.io/optimizing-gradient-descent" class="uri">http://ruder.io/optimizing-gradient-descent</a> <span class="citation" data-cites="Ruder2016">(Ruder, <a href="#ref-Ruder2016">2016</a>)</span>. The common wisdom is that SGD with Nesterov momentum works best (i.e. it finds a better minimum) but its meta-parameters (learning rate, momentum) are hard to find, while Adam works out-of-the-box, at the cost of a slightly worse minimum. For deep RL, Adam is usually preferred, as the goal is to quickly find a working solution, not to optimize it to the last decimal.</p>
<figure>
<img src="img/optimizers.gif" alt="Figure 9: Comparison of different optimizers. Source: Ruder (2016), http://ruder.io/optimizing-gradient-descent." id="fig:optimizers" style="width:50.0%" /><figcaption>Figure 9: Comparison of different optimizers. Source: <span class="citation" data-cites="Ruder2016">Ruder (<a href="#ref-Ruder2016">2016</a>)</span>, <a href="http://ruder.io/optimizing-gradient-descent" class="uri">http://ruder.io/optimizing-gradient-descent</a>.</figcaption>
</figure>
<p>Additional regularization mechanisms are now typically part of DNNs in order to avoid overfitting (learning by heart the training set but failing to generalize): L1/L2 regularization, dropout, batch normalization, etc. Refer to <span class="citation" data-cites="Goodfellow2016">Goodfellow et al. (<a href="#ref-Goodfellow2016">2016</a>)</span> for further details.</p>
<h3 id="sec:convolutional-networks"><span class="header-section-number">1.2.2</span> Convolutional networks</h3>
<p>Convolutional Neural Networks (CNN) are an adaptation of DNNs to deal with highly dimensional input spaces such as images. The idea is that neurons in the hidden layer reuse (“share”) weights over the input image, as the features learned by early layers are probably local in visual classification tasks: in computer vision, an edge can be detected by the same filter all over the input image.</p>
<p>A <strong>convolutional layer</strong> learns to extract a given number of features (typically 16, 32, 64, etc) represented by 3x3 or 5x5 matrices. These matrices are then convoluted over the whole input image (or the previous convolutional layer) to produce <strong>feature maps</strong>. If the input image has a size NxMx1 (grayscale) or NxMx3 (colored), the convolutional layer will be a tensor of size NxMxF, where F is the number of extracted features. Padding issues may reduce marginally the spatial dimensions. One important aspect is that the convolutional layer is fully differentiable, so backpropagation and the usual optimizers can be used to learn the filters.</p>
<figure>
<img src="img/convlayer.gif" alt="Figure 10: Convolutional layer. Source: https://github.com/vdumoulin/conv_arithmetic." id="fig:convlayer" /><figcaption>Figure 10: Convolutional layer. Source: <a href="https://github.com/vdumoulin/conv_arithmetic" class="uri">https://github.com/vdumoulin/conv_arithmetic</a>.</figcaption>
</figure>
<p>After a convolutional layer, the spatial dimensions are preserved. In classification tasks, it does not matter where the object is in the image, the only thing that matters is what it is: classification requires <strong>spatial invariance</strong> in the learned representations. The <strong>max-pooling layer</strong> was introduced to downsample each feature map individually and increase their spatial invariance. Each feature map is divided into 2x2 blocks (generally): only the maximal feature activation in that block is preserved in the max-pooling layer. This reduces the spatial dimensions by a factor two in each direction, but keeps the number of features equal.</p>
<figure>
<img src="img/maxpooling.png" alt="Figure 11: Max-pooling layer. Source: Stanford’s CS231n course http://cs231n.github.io/convolutional-networks" id="fig:maxpooling" /><figcaption>Figure 11: Max-pooling layer. Source: Stanford’s CS231n course <a href="http://cs231n.github.io/convolutional-networks" class="uri">http://cs231n.github.io/convolutional-networks</a></figcaption>
</figure>
<p>A convolutional neural network is simply a sequence of convolutional layers and max-pooling layers (sometime two convolutional layers are applied in a row before max-pooling, as in VGG <span class="citation" data-cites="Simonyan2014">(Simonyan and Zisserman, <a href="#ref-Simonyan2014">2014</a>)</span>), followed by a couple of fully-connected layers and a softmax output layer. Fig. <a href="#fig:alexnet">12</a> shows the architecture of AlexNet, the winning architecture of the ImageNet challenge in 2012 <span class="citation" data-cites="Krizhevsky2012">(Krizhevsky, Sutskever, and Hinton, <a href="#ref-Krizhevsky2012">2012</a>)</span>.</p>
<figure>
<img src="img/alexnet.png" alt="Figure 12: Architecture of the AlexNet CNN. Taken from Krizhevsky et al. (2012)." id="fig:alexnet" /><figcaption>Figure 12: Architecture of the AlexNet CNN. Taken from <span class="citation" data-cites="Krizhevsky2012">Krizhevsky et al. (<a href="#ref-Krizhevsky2012">2012</a>)</span>.</figcaption>
</figure>
<p>Many improvements have been proposed since 2012 (e.g. ResNets <span class="citation" data-cites="He2015">(He, Zhang, Ren, and Sun, <a href="#ref-He2015">2015</a>)</span>) but the idea stays similar. Generally, convolutional and max-pooling layers are alternated until the spatial dimensions are so reduced (around 10x10) that they can be put into a single vector and fed into a fully-connected layer. This is <strong>NOT</strong> the case in deep RL! Contrary to object classification, spatial information is crucial in deep RL: position of the ball, position of the body, etc. It matters whether the ball is to the right or to the left of your paddle when you decide how to move it. Max-pooling layers are therefore omitted and the CNNs only consist of convolutional and fully-connected layers. This greatly increases the number of weights in the networks, hence the number of training examples needed to train the network. This is still the main limitation of using CNNs in deep RL.</p>
<h3 id="sec:recurrent-neural-networks"><span class="header-section-number">1.2.3</span> Recurrent neural networks</h3>
<p>Feedforward neural networks learn to efficiently map static inputs <span class="math inline">\(\mathbf{x}\)</span> to outputs <span class="math inline">\(\mathbf{y}\)</span> but have no memory or context: the output at time <span class="math inline">\(t\)</span> does not depend on the inputs at time <span class="math inline">\(t-1\)</span> or <span class="math inline">\(t-2\)</span>, only the one at time <span class="math inline">\(t\)</span>. This is problematic when dealing with video sequences for example: if the task is to classify videos into happy/sad, a frame by frame analysis is going to be inefficient (most frames a neutral). Concatenating all frames in a giant input vector would increase dramatically the complexity of the classifier and no generalization can be expected.</p>
<p>Recurrent Neural Networks (RNN) are designed to deal with time-varying inputs, where the relevant information to take a decision at time <span class="math inline">\(t\)</span> may have happened at different times in the past. The general structure of a RNN is depicted on Fig. <a href="#fig:rnn">13</a>:</p>
<figure>
<img src="img/RNN-unrolled.png" alt="Figure 13: Architecture of a RNN. Left: recurrent architecture. Right: unrolled network, showing that a RNN is equivalent to a deep network. Taken from http://colah.github.io/posts/2015-08-Understanding-LSTMs." id="fig:rnn" style="width:90.0%" /><figcaption>Figure 13: Architecture of a RNN. Left: recurrent architecture. Right: unrolled network, showing that a RNN is equivalent to a deep network. Taken from <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs" class="uri">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</figcaption>
</figure>
<p>The output <span class="math inline">\(\mathbf{h}_t\)</span> of the RNN at time <span class="math inline">\(t\)</span> depends on its current input <span class="math inline">\(\mathbf{x}_t\)</span>, but also on its previous output <span class="math inline">\(\mathbf{h}_{t-1}\)</span>, which, by recursion, depends on the whole history of inputs <span class="math inline">\((x_0, x_1, \ldots, x_t)\)</span>.</p>
<p><span><span class="math display">\[
    \mathbf{h}_t = f(W_x \, \mathbf{x}_{t} + W_h \, \mathbf{h}_{t-1} + \mathbf{b})
\qquad(43)\]</span></span></p>
<p>Once unrolled, a RNN is equivalent to a deep network, with <span class="math inline">\(t\)</span> layers of weights between the first input <span class="math inline">\(\mathbf{x}_0\)</span> and the current output <span class="math inline">\(\mathbf{h}_t\)</span>. The only difference with a feedforward network is that weights are reused between two time steps / layers. <strong>Backpropagation though time</strong> (BPTT) can be used to propagate the gradient of the loss function backwards in time and learn the weights <span class="math inline">\(W_x\)</span> and <span class="math inline">\(W_h\)</span> using the usual optimizer (SGD, Adam…).</p>
<p>However, this kind of RNN can only learn short-term dependencies because of the <strong>vanishing gradient problem</strong> <span class="citation" data-cites="Hochreiter1991">(Hochreiter, <a href="#ref-Hochreiter1991">1991</a>)</span>. When the gradient of the loss funcion travels backwards from <span class="math inline">\(\mathbf{h}_t\)</span> to <span class="math inline">\(\mathbf{x}_0\)</span>, it will be multiplied <span class="math inline">\(t\)</span> times by the recurrent weights <span class="math inline">\(W_h\)</span>. If <span class="math inline">\(|W_h| &gt; 1\)</span>, the gradient will explode with increasing <span class="math inline">\(t\)</span>, while if <span class="math inline">\(|W_h| &lt; 1\)</span>, the gradient will vanish to 0.</p>
<p>The solution to this problem is provided by <strong>long short-term memory networks</strong> <span class="citation" data-cites="Hochreiter1997">(LSTM; Hochreiter and Schmidhuber, <a href="#ref-Hochreiter1997">1997</a>)</span>. LSTM layers maintain additionally a state <span class="math inline">\(\mathbf{C}_t\)</span> (also called context or memory) which is manipulated by three learnable gates (input, forget and output gates). As in regular RNNs, a <em>candidate state</em> <span class="math inline">\(\tilde{\mathbf{C}_t}\)</span> is computed based on the current input and the previous output:</p>
<p><span><span class="math display">\[
    \tilde{\mathbf{C}_t} = f(W_x \, \mathbf{x}_{t} + W_h \, \mathbf{h}_{t-1} + \mathbf{b})
\qquad(44)\]</span></span></p>
<figure>
<img src="img/LSTM.png" alt="Figure 14: Architecture of a LSTM layer. Taken from http://colah.github.io/posts/2015-08-Understanding-LSTMs." id="fig:lstm" style="width:40.0%" /><figcaption>Figure 14: Architecture of a LSTM layer. Taken from <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs" class="uri">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</figcaption>
</figure>
<p>The activation function <span class="math inline">\(f\)</span> is usually a tanh function. The input and forget learn to decide how the candidate state should be used to update the current state:</p>
<ul>
<li>The input gate decides which part of the candidate state <span class="math inline">\(\tilde{\mathbf{C}_t}\)</span> will be used to update the current state <span class="math inline">\(\mathbf{C}_t\)</span>:</li>
</ul>
<p><span><span class="math display">\[
    \mathbf{i}_t = \sigma(W^i_x \, \mathbf{x}_{t} + W^i_h \, \mathbf{h}_{t-1} + \mathbf{b}^i)
\qquad(45)\]</span></span></p>
<p>The sigmoid activation function <span class="math inline">\(\sigma\)</span> is used to output a number between 0 and 1 for each neuron: 0 means the candidate state will not be used at all, 1 means completely.</p>
<ul>
<li>The forget gate decides which part of the current state should be kept or forgotten:</li>
</ul>
<p><span><span class="math display">\[
    \mathbf{f}_t = \sigma(W^f_x \, \mathbf{x}_{t} + W^f_h \, \mathbf{h}_{t-1} + \mathbf{b}^f)
\qquad(46)\]</span></span></p>
<p>Similarly, 0 means taht the corresponding element of the current state will be erased, 1 that it will be kept.</p>
<p>Once the input and forget gates are computed, the current state can be updated based on its previous value and the candidate state:</p>
<p><span><span class="math display">\[
   \mathbf{C}_t =  \mathbf{i}_t \odot \tilde{\mathbf{C}_t} + \mathbf{f}_t \odot \mathbf{C}_{t-1}
\qquad(47)\]</span></span></p>
<p>where <span class="math inline">\(\odot\)</span> is the element-wise multiplication.</p>
<ul>
<li>The output gate finally learns to select which part of the current state <span class="math inline">\(\mathbf{C}_t\)</span> should be used to produce the current output <span class="math inline">\(\mathbf{h}_t\)</span>:</li>
</ul>
<p><span><span class="math display">\[
    \mathbf{o}_t = \sigma(W^o_x \, \mathbf{x}_{t} + W^o_h \, \mathbf{h}_{t-1} + \mathbf{b}^o)
\qquad(48)\]</span></span></p>
<p><span><span class="math display">\[
    \mathbf{h}_t = \mathbf{o}_t \odot \tanh \mathbf{C}_t
\qquad(49)\]</span></span></p>
<p>The architecture may seem complex, but everything is differentiable: backpropagation though time can be used to learn not only the input and recurrent weights for the candidate state, but also the weights and and biases of the gates. The main advantage of LSTMs is that they solve the vanishing gradient problem: if the input at time <span class="math inline">\(t=0\)</span> is important to produce a response at time <span class="math inline">\(t\)</span>, the input gate will learn to put it into the memory and the forget gate will learn to maintain in the current state until it is not needed anymore. During this “working memory” phase, the gradient is multiplied by exactly one as nothing changes: the dependency can be learned with arbitrary time delays!</p>
<p>There are alternatives to the classical LSTM layer such as the gated recurrent unit <span class="citation" data-cites="Cho2014">(GRU; Cho et al., <a href="#ref-Cho2014">2014</a>)</span> or peephole connections <span class="citation" data-cites="Gers2001">(Gers, <a href="#ref-Gers2001">2001</a>)</span>. See <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs" class="uri">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>, <a href="https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714" class="uri">https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714</a> or <a href="http://blog.echen.me/2017/05/30/exploring-lstms/" class="uri">http://blog.echen.me/2017/05/30/exploring-lstms/</a> for more visual explanations of LSTMs and their variants.</p>
<p>RNNs are particularly useful for deep RL when considering POMDPs, i.e. partially observable problems. If an observation does not contain enough information about the underlying state (e.g. a single image does not contain speed information), LSTM can integrate these observations over time and learn to implicitely represent speed in its context vector, allowing efficient policies to be learned.</p>
<h1 id="sec:value-based-methods"><span class="header-section-number">2</span> Value-based methods</h1>
<h2 id="sec:limitations-of-deep-neural-networks-for-function-approximation"><span class="header-section-number">2.1</span> Limitations of deep neural networks for function approximation</h2>
<p>The goal of value-based deep RL is to approximate the Q-value of each possible state-action pair using a deep (convolutional) neural network. As shown on Fig. <a href="#fig:functionapprox">7</a>, the network can either take a state-action pair as input and return a single output value, or take only the state as input and return the Q-value of all possible actions (only possible if the action space is discrete), In both cases, the goal is to learn estimates <span class="math inline">\(Q_\theta(s, a)\)</span> with a NN with parameters <span class="math inline">\(\theta\)</span>.</p>
<p><img src="img/functionapprox.png" style="width:40.0%" /></p>
<p>When using Q-learning, we have already seen in Section <a href="#sec:function-approximation">1.1.9</a> that the problem is a regression problem, where the following mse loss function has to be minimized:</p>
<p><span><span class="math display">\[
    \mathcal{L}(\theta) = E([r_t + \gamma \, \max_{a&#39;} Q_\theta(s&#39;, a&#39;) - Q_\theta(s, a)]^2)
\qquad(50)\]</span></span></p>
<p>In short, we want to reduce the prediction error, i.e. the mismatch between the estimate of the value of an action <span class="math inline">\(Q_\theta(s, a)\)</span> and the real expected return, here approximated with <span class="math inline">\(r(s, a, s&#39;) + \gamma \, \text{max}_{a&#39;} Q_\theta(s&#39;, a&#39;)\)</span>.</p>
<p>We can compute this loss by gathering enough samples <span class="math inline">\((s, a, r, s&#39;)\)</span> (i.e. single transitions), concatenating them randomly in minibatches, and let the DNN learn to minimize the prediction error using backpropagation and SGD, indirectly improving the policy. The following pseudocode would describe the training procedure when gathering transitions <strong>online</strong>, i.e. when directly interacting with the environment:</p>
<hr />
<ul>
<li>Initialize value network <span class="math inline">\(Q_{\theta}\)</span> with random weights.</li>
<li>Initialize empty minibatch <span class="math inline">\(\mathcal{D}\)</span> of maximal size <span class="math inline">\(n\)</span>.</li>
<li>Observe the initial state <span class="math inline">\(s_0\)</span>.</li>
<li>for <span class="math inline">\(t \in [0, T_\text{total}]\)</span>:
<ul>
<li>Select the action <span class="math inline">\(a_t\)</span> based on the behaviour policy derived from <span class="math inline">\(Q_\theta(s_t, a)\)</span> (e.g. softmax).</li>
<li>Perform the action <span class="math inline">\(a_t\)</span> and observe the next state <span class="math inline">\(s_{t+1}\)</span> and the reward <span class="math inline">\(r_{t+1}\)</span>.</li>
<li>Predict the Q-value of the greedy action in the next state <span class="math inline">\(\max_{a&#39;} Q_\theta(s_{t+1}, a&#39;)\)</span></li>
<li>Store <span class="math inline">\((s_t, a_t, r_{t+1} + \gamma \, \max_{a&#39;} Q_\theta(s_{t+1}, a&#39;))\)</span> in the minibatch.</li>
<li>If minibatch <span class="math inline">\(\mathcal{D}\)</span> is full:
<ul>
<li>Train the value network <span class="math inline">\(Q_{\theta}\)</span> on <span class="math inline">\(\mathcal{D}\)</span> to minimize <span class="math inline">\(\mathcal{L}(\theta) = E_\mathcal{D}([r(s, a, s&#39;) + \gamma \, \text{max}_{a&#39;} Q_\theta(s&#39;, a&#39;) - Q_\theta(s, a)]^2)\)</span></li>
<li>Empty the minibatch <span class="math inline">\(\mathcal{D}\)</span>.</li>
</ul></li>
</ul></li>
</ul>
<hr />
<p>However, the definition of the loss function uses the mathematical expectation operator <span class="math inline">\(E\)</span> over all transitions, which can only be approximated by <strong>randomly</strong> sampling the distribution (the MDP). This implies that the samples concatenated in a minibatch should be independent from each other (i.i.d). When gathering transitions online, the samples are correlated: <span class="math inline">\((s_t, a_t, r_{t+1}, s_{t+1})\)</span> will be followed by <span class="math inline">\((s_{t+1}, a_{t+1}, r_{t+2}, s_{t+2})\)</span>, etc. When playing video games, two successive frames will be very similar (a few pixels will change, or even none if the sampling rate is too high) and the optimal action will likely not change either (to catch the ball in pong, you will need to perform the same action - going left - many times in a row).</p>
<p><strong>Correlated inputs/outputs</strong> are very bad for deep neural networks: the DNN will overfit and fall into a very bad local minimum. That is why stochastic gradient descent works so well: it randomly samples values from the training set to form minibatches and minimize the loss function on these uncorrelated samples (hopefully). If all samples of a minibatch were of the same class (e.g. zeros in MNIST), the network would converge poorly. This is the first problem preventing an easy use of deep neural networks as function approximators in RL.</p>
<p>The second major problem is the <strong>non-stationarity</strong> of the targets in the loss function. In classification or regression, the desired values <span class="math inline">\(\mathbf{t}\)</span> are fixed throughout learning: the class of an object does not change in the middle of the training phase.</p>
<p><span><span class="math display">\[
    \mathcal{L}(\theta) = - E_{\mathbf{x}, \mathbf{t} \in \mathcal{D}} ||\mathbf{t} - \mathbf{y}||^2
\qquad(51)\]</span></span></p>
<p>In Q-learning, the target <span class="math inline">\(r(s, a, s&#39;) + \gamma \, \max_{a&#39;} Q_\theta(s&#39;, a&#39;)\)</span> will change during learning, as <span class="math inline">\(Q_\theta(s&#39;, a&#39;)\)</span> depends on the weights <span class="math inline">\(\theta\)</span> and will hopefully increase as the performance improves. This is the second problem of deep RL: deep NN are particularly bad on non-stationary problems, especially feedforward networks. They iteratively converge towards the desired value, but have troubles when the target also moves (like a dog chasing its tail).</p>
<h2 id="sec:deep-q-network-dqn"><span class="header-section-number">2.2</span> Deep Q-Network (DQN)</h2>
<p><span class="citation" data-cites="Mnih2015">Mnih et al. (<a href="#ref-Mnih2015">2015</a>)</span> (originally arXived in <span class="citation" data-cites="Mnih2013">Mnih et al. (<a href="#ref-Mnih2013">2013</a>)</span>) proposed an elegant solution to the problems of correlated inputs/outputs and non-stationarity inherent to RL. This article is a milestone of deep RL and it is fair to say that it started or at least strongly renewed the interest for deep RL.</p>
<p>The first idea proposed by <span class="citation" data-cites="Mnih2015">Mnih et al. (<a href="#ref-Mnih2015">2015</a>)</span> solves the problem of correlated input/outputs and is actually quite simple: instead of feeding successive transitions into a minibatch and immediately training the NN on it, transitions are stored in a huge buffer called <strong>experience replay memory</strong> (ERM) able to store 100000 transitions. When the buffer is full, new transitions replace the old ones. SGD can now randomly sample the ERM to form minibatches and train the NN.</p>
<figure>
<img src="img/ERM.svg" alt="Figure 15: Experience replay memory. Interactions with the environment are stored in the ERM. Random minibatches are sampled from it to train the DQN value network." id="fig:erm" style="width:40.0%" /><figcaption>Figure 15: Experience replay memory. Interactions with the environment are stored in the ERM. Random minibatches are sampled from it to train the DQN value network.</figcaption>
</figure>
<p>The second idea solves the non-stationarity of the targets <span class="math inline">\(r(s, a, s&#39;) + \gamma \, \max_{a&#39;} Q_\theta(s&#39;, a&#39;)\)</span>. Instead of computing it with the current parameters <span class="math inline">\(\theta\)</span> of the NN, they are computed with an old version of the NN called the <strong>target network</strong> with parameters <span class="math inline">\(\theta&#39;\)</span>. The target network is updated only infrequently (every thousands of iterations or so) with the learned weights <span class="math inline">\(\theta\)</span>. As this target network does not change very often, the targets stay constant for a long period of time, and the problem becomes more stationary.</p>
<p>The resulting algorithm is called <strong>Deep Q-Network (DQN)</strong>. It is summarized by the following pseudocode:</p>
<hr />
<ul>
<li>Initialize value network <span class="math inline">\(Q_{\theta}\)</span> with random weights.</li>
<li>Copy <span class="math inline">\(Q_{\theta}\)</span> to create the target network <span class="math inline">\(Q_{\theta&#39;}\)</span>.</li>
<li>Initialize experience replay memory <span class="math inline">\(\mathcal{D}\)</span> of maximal size <span class="math inline">\(N\)</span>.</li>
<li>Observe the initial state <span class="math inline">\(s_0\)</span>.</li>
<li>for <span class="math inline">\(t \in [0, T_\text{total}]\)</span>:
<ul>
<li>Select the action <span class="math inline">\(a_t\)</span> based on the behaviour policy derived from <span class="math inline">\(Q_\theta(s_t, a)\)</span> (e.g. softmax).</li>
<li>Perform the action <span class="math inline">\(a_t\)</span> and observe the next state <span class="math inline">\(s_{t+1}\)</span> and the reward <span class="math inline">\(r_{t+1}\)</span>.</li>
<li>Store <span class="math inline">\((s_t, a_t, r_{t+1}, s_{t+1})\)</span> in the experience replay memory.</li>
<li>Every <span class="math inline">\(T_\text{train}\)</span> steps:
<ul>
<li>Sample a minibatch <span class="math inline">\(\mathcal{D}_s\)</span> randomly from <span class="math inline">\(\mathcal{D}\)</span>.</li>
<li>For each transition <span class="math inline">\((s, a, r, s&#39;)\)</span> in the minibatch:
<ul>
<li>Predict the Q-value of the greedy action in the next state <span class="math inline">\(\max_{a&#39;} Q_{\theta&#39;}(s&#39;, a&#39;)\)</span> using the target network.</li>
<li>Compute the target value <span class="math inline">\(y = r + \gamma \, \max_{a&#39;} Q_{\theta&#39;}(s&#39;, a&#39;)\)</span>.</li>
</ul></li>
<li>Train the value network <span class="math inline">\(Q_{\theta}\)</span> on <span class="math inline">\(\mathcal{D}_s\)</span> to minimize <span class="math inline">\(\mathcal{L}(\theta) = E_{\mathcal{D}_s}([y - Q_\theta(s, a)]^2)\)</span></li>
</ul></li>
<li>Every <span class="math inline">\(T_\text{target}\)</span> steps:
<ul>
<li>Update the target network with the trained value network: <span class="math inline">\(\theta&#39; \leftarrow \theta\)</span></li>
</ul></li>
</ul></li>
</ul>
<hr />
<p>In this document, pseudocode will omit many details to simplify the explanations (for example here, the case where a state is terminal - the game ends - and the next state has to be chosen from the distribution of possible starting states). Refer to the original publication for more exact algorithms.</p>
<p>The first thing to notice is that experienced transitions are not immediately used for learning, but simply stored in the ERM to be sampled later. Due to the huge size of the ERM, it is even likely that the recently experienced transition will only be used for learning hundreds or thousands of steps later. Meanwhile, very old transitions, generated using an initially bad policy, can be used to train the network for a very long time.</p>
<p>The second thing is that the target network is not updated very often (<span class="math inline">\(T_\text{target}=10000\)</span>), so the target values are going to be wrong a long time. More recent algorithms use a a smoothed version of the current weights, as proposed in <span class="citation" data-cites="Lillicrap2015">Lillicrap et al. (<a href="#ref-Lillicrap2015">2015</a>)</span>:</p>
<p><span><span class="math display">\[
    \theta&#39; = \tau \, \theta + (1-\tau) \, \theta&#39;
\qquad(52)\]</span></span></p>
<p>If this rule is applied after each step with a very small rate <span class="math inline">\(\tau\)</span>, the target network will slowly track the learned network, but never be the same.</p>
<p>These two facts make DQN extremely slow to learn: millions of transitions are needed to obtain a satisfying policy. This is called the <strong>sample complexity</strong>, i.e. the number of transitions needed to obtain a satisfying performance. DQN finds very good policies, but at the cost of a very long training time.</p>
<p>DQN was initially applied to solve various Atari 2600 games. Video frames were used as observations and the set of possible discrete actions was limited (left/right/up/down, shoot, etc). The CNN used is depicted on Fig. <a href="#fig:dqn">16</a>. It has two convolutional layers, no max-pooling, 2 fully-connected layer and one output layer representing the Q-value of all possible actions in the games.</p>
<figure>
<img src="img/dqn.png" alt="Figure 16: Architecture of the CNN used in the original DQN paper. Taken from Mnih et al. (2015)." id="fig:dqn" /><figcaption>Figure 16: Architecture of the CNN used in the original DQN paper. Taken from <span class="citation" data-cites="Mnih2015">Mnih et al. (<a href="#ref-Mnih2015">2015</a>)</span>.</figcaption>
</figure>
<p>The problem of partial observability is solved by concatenating the four last video frames into a single tensor used as input to the CNN. The convolutional layers become able through learning to extract the speed information from it. Some of the Atari games (Pinball, Breakout) were solved with a performance well above human level, especially when they are mostly reactive. Games necessitating more long-term planning (Montezuma’ Revenge) were still poorly learned, though.</p>
<p>Beside being able to learn using delayed and sparse rewards in highly dimensional input spaces, the true <em>tour de force</em> of DQN is that it was able to learn the 49 Atari games in a row, using the same architecture and hyperparameters, and without resetting the weights between two games: knowledge acquired in one game could be reused for the next game. This created great excitement, as the ability to reuse knowledge over different tasks is a fundamental property of true intelligence.</p>
<h2 id="sec:double-dqn"><span class="header-section-number">2.3</span> Double DQN</h2>
<p>In DQN, the experience replay memory and the target network were decisive in allowing the CNN to learn the tasks through RL. Their drawback is that they drastically slow down learning and increase the sample complexity. Additionally, DQN has stability issues: the same network may not converge the same way in different runs. One first improvement on DQN was proposed by <span class="citation" data-cites="vanHasselt2015">Hasselt, Guez, and Silver (<a href="#ref-vanHasselt2015">2015</a>)</span> and called <strong>double DQN</strong>.</p>
<p>The idea is that the target value <span class="math inline">\(y = r(s, a, s&#39;) + \gamma \, \max_{a&#39;} Q_{\theta&#39;}(s&#39;, a&#39;)\)</span> is frequently over-estimating the true expected return because of the max operator. Especially at the beginning of learning when Q-values are far from being correct, if an action is over-estimated (<span class="math inline">\(Q_{\theta&#39;}(s&#39;, a)\)</span> is higher that its true value) and selected by the target network as the next greedy action, the learned Q-value <span class="math inline">\(Q_{\theta}(s, a)\)</span> will also become over-estimated, what will propagate to all previous actions on the long-term. <span class="citation" data-cites="vanHasselt2010">Hasselt (<a href="#ref-vanHasselt2010">2010</a>)</span> showed that this over-estimation is inevitable in regular Q-learning and proposed <strong>double learning</strong>.</p>
<p>The idea is to train independently two value networks: one will be used to find the greedy action (the action with the maximal Q-value), the other to estimate the Q-value itself. Even if the first network choose an over-estimated action as the greedy action, the other might provide a less over-estimated value for it, solving the problem.</p>
<p>Applying double learning to DQN is particularly straightforward: there are already two value networks, the trained network and the target network. Instead of using the target network to both select the greedy action in the next state and estimate its Q-value, here the trained network <span class="math inline">\(\theta\)</span> is used to select the greedy action <span class="math inline">\(a^* = \text{argmax}_{a&#39;} Q_\theta (s&#39;, a&#39;)\)</span> while the target network only estimates its Q-value. The target value becomes:</p>
<p><span><span class="math display">\[
    y = r(s, a, s&#39;) + \gamma \, Q_{\theta&#39;}(s&#39;, \text{argmax}_{a&#39;} Q_\theta (s&#39;, a&#39;)) 
\qquad(53)\]</span></span></p>
<p>This induces only a small modification of the DQN algorithm and significantly improves its performance and stability:</p>
<hr />
<ul>
<li>Every <span class="math inline">\(T_\text{train}\)</span> steps:
<ul>
<li>Sample a minibatch <span class="math inline">\(\mathcal{D}_s\)</span> randomly from <span class="math inline">\(\mathcal{D}\)</span>.</li>
<li>For each transition <span class="math inline">\((s, a, r, s&#39;)\)</span> in the minibatch:
<ul>
<li>Select the greedy action in the next state <span class="math inline">\(a^* = \text{argmax}_{a&#39;} Q_\theta (s&#39;, a&#39;)\)</span> using the trained network.<br />
</li>
<li>Predict its Q-value <span class="math inline">\(Q_{\theta&#39;}(s&#39;, a^*)\)</span> using the target network.</li>
<li>Compute the target value <span class="math inline">\(y = r + \gamma \, Q_{\theta&#39;}(s&#39;, a*)\)</span>.</li>
</ul></li>
</ul></li>
</ul>
<hr />
<h2 id="sec:prioritised-replay"><span class="header-section-number">2.4</span> Prioritised replay</h2>
<p>Another drawback of the original DQN is that the experience replay memory is sampled uniformly. Novel and interesting transitions are selected with the same probability as old well-predicted transitions, what slows down learning. The main idea of <strong>prioritized replay</strong> <span class="citation" data-cites="Schaul2015">(Schaul, Quan, Antonoglou, and Silver, <a href="#ref-Schaul2015">2015</a>)</span> is to order the transitions in the experience replay memory in decreasing order of their TD error:</p>
<p><span><span class="math display">\[
    \delta = r(s, a, s&#39;) + \gamma \, Q_{\theta&#39;}(s&#39;, \text{argmax}_{a&#39;} Q_\theta (s&#39;, a&#39;)) - Q_\theta(s, a)
\qquad(54)\]</span></span></p>
<p>and sample with a higher probability those surprising transitions to form a minibatch. However, non-surprising transitions might become relevant again after enough training, as the <span class="math inline">\(Q_\theta(s, a)\)</span> change, so prioritized replay has a softmax function over the TD error to ensure “exploration” of memorized transitions. This data structure has of course a non-negligeable computational cost, but accelerates learning so much that it is worth it. See <a href="https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/" class="uri">https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/</a> for a presentation of double DQN with prioritized replay.</p>
<h2 id="sec:duelling-network"><span class="header-section-number">2.5</span> Duelling network</h2>
<p>The classical DQN architecture uses a single NN to predict directly the value of all possible actions <span class="math inline">\(Q_\theta(s, a)\)</span>. The value of an action depends on two factors:</p>
<ul>
<li>the value of the underlying state <span class="math inline">\(s\)</span>: in some states, all actions are bad, you lose whatever you do.</li>
<li>the interest of that action: some actions are better than others for a given state.</li>
</ul>
<p>This leads to the definition of the <strong>advantage</strong> <span class="math inline">\(A^\pi(s,a)\)</span> of an action:</p>
<p><span id="eq:advantagefunction"><span class="math display">\[
    A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s)
\qquad(55)\]</span></span></p>
<p>The advantage of the optimal action in <span class="math inline">\(s\)</span> is equal to zero: the expected return in <span class="math inline">\(s\)</span> is the same as the expected return when being in <span class="math inline">\(s\)</span> and taking <span class="math inline">\(a\)</span>, as the optimal policy will choose <span class="math inline">\(a\)</span> in <span class="math inline">\(s\)</span> anyway. The advantage of all other actions is negative: they bring less reward than the optimal action (by definition), so they are less advantageous. Note that this is only true if your estimate of <span class="math inline">\(V^\pi(s)\)</span> is correct.</p>
<p><span class="citation" data-cites="Baird1993">Baird (<a href="#ref-Baird1993">1993</a>)</span> has shown that it is advantageous to decompose the Q-value of an action into the value of the state and the advantage of the action (<em>advantage updating</em>):</p>
<p><span><span class="math display">\[
    Q^\pi(s, a) = V^\pi(s) + A^\pi(s, a)
\qquad(56)\]</span></span></p>
<p>If you already know that the value of a state is very low, you do not need to bother exploring and learning the value of all actions in that state, they will not bring much. Moreover, the advantage function has less variance than the Q-values, which is a very good property when using neural networks for function approximation. Let’s suppose we have two states with values -10 and 10, and two actions with advantage 0 and -1 (it does not matter which one). The Q-values will vary between -11 (the worst action in the worst state) and 10 (the best action in the best state), while the advantage only varies between -1 and 0. It is going to be much easier for a neural network to learn the advantages than the Q-values: <strong>reducing the variance</strong> of the outputs is the main difficulty in value-based deep RL as Q-values are theoretically not bounded.</p>
<figure>
<img src="img/duelling.png" alt="Figure 17: Duelling network architecture. Top: classical feedforward architecture to predict Q-values. Bottom: Duelling networks predicting state values and advantage functions to form the Q-values. Taken from Wang et al. (2016)." id="fig:duelling" style="width:60.0%" /><figcaption>Figure 17: Duelling network architecture. Top: classical feedforward architecture to predict Q-values. Bottom: Duelling networks predicting state values and advantage functions to form the Q-values. Taken from <span class="citation" data-cites="Wang2016">Wang et al. (<a href="#ref-Wang2016">2016</a>)</span>.</figcaption>
</figure>
<p><span class="citation" data-cites="Wang2016">Wang et al. (<a href="#ref-Wang2016">2016</a>)</span> incorporated the idea of <em>advantage updating</em> in a double DQN architecture with prioritized replay (Fig. <a href="#fig:duelling">17</a>). As in DQN, the last layer represents the Q-values of the possible actions and has to minimize the mse loss:</p>
<p><span><span class="math display">\[
    \mathcal{L}(\theta) = E([r(s, a, s&#39;) + \gamma \, Q_{\theta&#39;, \alpha&#39;, \beta&#39;}(s&#39;, \text{argmax}_{a&#39;} Q_{\theta, \alpha, \beta} (s&#39;, a&#39;)) - Q_{\theta, \alpha, \beta}(s, a)]^2)
\qquad(57)\]</span></span></p>
<p>The difference is that the previous fully-connected layer is forced to represent the value of the input state <span class="math inline">\(V_{\theta, \beta}(s)\)</span> and the advantage of each action <span class="math inline">\(A_{\theta, \alpha}(s, a)\)</span> separately. There are two separate sets of weights in the network, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, to predict these two values, sharing representations from the early convolutional layers through weights <span class="math inline">\(\theta\)</span>. The output layer performs simply a parameter-less summation of both sub-networks:</p>
<p><span><span class="math display">\[
    Q_{\theta, \alpha, \beta}(s, a) = V_{\theta, \beta}(s) + A_{\theta, \alpha}(s, a)
\qquad(58)\]</span></span></p>
<p>The issue with this formulation is that one could add a constant to <span class="math inline">\(V_{\theta, \beta}(s)\)</span> and substract it from <span class="math inline">\(A_{\theta, \alpha}(s, a)\)</span> while obtaining the same result. An easy way to constrain the summation is to normalize the advantages, so that the greedy action has an advantage of zero as expected:</p>
<p><span><span class="math display">\[
    Q_{\theta, \alpha, \beta}(s, a) = V_{\theta, \beta}(s) + (A_{\theta, \alpha}(s, a) - \max_a A_{\theta, \alpha}(s, a))
\qquad(59)\]</span></span></p>
<p>By doing this, the advantages are still free, but the state value will have to take the correct valu. <span class="citation" data-cites="Wang2016">Wang et al. (<a href="#ref-Wang2016">2016</a>)</span> found that it is actually better to replace the <span class="math inline">\(\max\)</span> operator by the mean of the advantages. In this case, the advantages only need to change as fast as their mean, instead of having to compensate quickly for any change in the greedy action as the policy improves:</p>
<p><span><span class="math display">\[
    Q_{\theta, \alpha, \beta}(s, a) = V_{\theta, \beta}(s) + (A_{\theta, \alpha}(s, a) - \frac{1}{|\mathcal{A}|} \sum_a A_{\theta, \alpha}(s, a))
\qquad(60)\]</span></span></p>
<p>Apart from this specific output layer, everything works as usual, especially the gradient of the mse loss function can travel backwards using backpropagation to update the weights <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. The resulting architecture outperforms double DQN with prioritized replay on most Atari games, particularly games with repetitive actions.</p>
<h2 id="sec:distributed-dqn-gorila"><span class="header-section-number">2.6</span> Distributed DQN (GORILA)</h2>
<p>The main limitation of deep RL is the slowness of learning, which is mainly influenced by two factors:</p>
<ul>
<li>the <em>sample complexity</em>, i.e. the number of transitions needed to learn a satisfying policy.</li>
<li>the online interaction with the environment.</li>
</ul>
<p>The second factor is particularly critical in real-world applications like robotics: physical robots evolve in real time, so the acquisition speed of transitions will be limited. Even in simulation (video games, robot emulators), the simulator might turn out to be much slower than training the underlying neural network. Google Deepmind proposed the GORILA (General Reinforcement Learning Architecture) framework to speed up the training of DQN networks using distributed actors and learners <span class="citation" data-cites="Nair2015">(Nair et al., <a href="#ref-Nair2015">2015</a>)</span>. The framework is quite general and the distribution granularity can change depending on the task.</p>
<figure>
<img src="img/gorila-global.png" alt="Figure 18: GORILA architecture. Multiple actors interact with multiple copies of the environment and store their experiences in a (distributed) experience replay memory. Multiple DQN learners sample from the ERM and compute the gradient of the loss function w.r.t the parameters \theta. A master network (parameter server, possibly distributed) gathers the gradients, apply weight updates and synchronizes regularly both the actors and the learners with new parameters. Taken from Nair et al. (2015)." id="fig:gorila" style="width:90.0%" /><figcaption>Figure 18: GORILA architecture. Multiple actors interact with multiple copies of the environment and store their experiences in a (distributed) experience replay memory. Multiple DQN learners sample from the ERM and compute the gradient of the loss function w.r.t the parameters <span class="math inline">\(\theta\)</span>. A master network (parameter server, possibly distributed) gathers the gradients, apply weight updates and synchronizes regularly both the actors and the learners with new parameters. Taken from <span class="citation" data-cites="Nair2015">Nair et al. (<a href="#ref-Nair2015">2015</a>)</span>.</figcaption>
</figure>
<p>In GORILA, multiple actors interact with the environment to gather transitions. Each actor has an independent copy of the environment, so they can gather <span class="math inline">\(N\)</span> times more samples per second if there are <span class="math inline">\(N\)</span> actors. This is possible in simulation (starting <span class="math inline">\(N\)</span> instances of the same game in parallel) but much more complicated for real-world systems (but see <span class="citation" data-cites="Gu2017">Gu, Holly, Lillicrap, and Levine (<a href="#ref-Gu2017">2017</a>)</span> for an example where multiple identical robots are used to gather experiences in parallel).</p>
<p>The experienced transitions are sent as in DQN to an experience replay memory, which may be distributed or centralized. Multiple DQN learners will then sample a minibatch from the ERM and compute the DQN loss on this minibatch (also using a target network). All learners start with the same parameters <span class="math inline">\(\theta\)</span> and simply compute the gradient of the loss function <span class="math inline">\(\frac{\partial \mathcal{L}(\theta)}{\partial \theta}\)</span> on the minibatch. The gradients are sent to a parameter server (a master network) which uses the gradients to apply the optimizer (e.g. SGD) and find new values for the parameters <span class="math inline">\(\theta\)</span>. Weight updates can also be applied in a distributed manner. This distributed method to train a network using multiple learners is now quite standard in deep learning: on multiple GPU systems, each GPU has a copy of the network and computes gradients on a different minibatch, while a master network integrates these gradients and updates the slaves.</p>
<p>The parameter server regularly updates the actors (to gather samples with the new policy) and the learners (to compute gradients w.r.t the new parameter values). Such a distributed system can greatly accelerate learning, but it can be quite tricky to find the optimum number of actors and learners (too many learners might degrade the stability) or their update rate (if the learners are not updated frequently enough, the gradients might not be correct). A similar idea is at the core of the A3C algorithm (Section <a href="#sec:asynchronous-advantage-actor-critic-a3c">3.1.3</a>).</p>
<h2 id="sec:deep-recurrent-q-learning-drqn"><span class="header-section-number">2.7</span> Deep Recurrent Q-learning (DRQN)</h2>
<p>The Atari games used as a benchmark for value-based methods are <strong>partially observable MDPs</strong> (POMDP), i.e. a single frame does not contain enough information to predict what is going to happen next (e.g. the speed and direction of the ball on the screen is not known). In DQN, partial observability is solved by stacking four consecutive frames and using the resulting tensor as an input to the CNN. if this approach worked well for most Atari games, it has several limitations (as explained in <a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc" class="uri">https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc</a>):</p>
<ol type="1">
<li>It increases the size of the experience replay memory, as four video frames have to be stored for each transition.</li>
<li>It solves only short-term dependencies (instantaneous speeds). If the partial observability has long-term dependencies (an object has been hidden a long time ago but now becomes useful), the input to the neural network will not have that information. This is the main explanation why the original DQN performed so poorly on games necessitating long-term planning like Montezuma’s revenge.</li>
</ol>
<p>Building on previous ideas from the Schmidhuber’s group <span class="citation" data-cites="Bakker2001 Wierstra2007">(Bakker, <a href="#ref-Bakker2001">2001</a>; Wierstra, Foerster, Peters, and Schmidhuber, <a href="#ref-Wierstra2007">2007</a>)</span>, <span class="citation" data-cites="Hausknecht2015">Hausknecht and Stone (<a href="#ref-Hausknecht2015">2015</a>)</span> replaced one of the fully-connected layers of the DQN network by a LSTM layer (see Section <a href="#sec:recurrent-neural-networks">1.2.3</a>) while using single frames as inputs. The resulting <strong>deep recurrent q-learning</strong> (DRQN) network became able to solve POMDPs thanks to the astonishing learning abilities of LSTMs: the LSTM layer learn to remember which part of the sensory information will be useful to take decisions later.</p>
<p>However, LSTMs are not a magical solution either. They are trained using <em>truncated BPTT</em>, i.e. on a limited history of states. Long-term dependencies exceeding the truncation horizon cannot be learned. Additionally, all states in that horizon (i.e. all frames) have to be stored in the ERM to train the network, increasing drastically its size. Despite these limitations, DRQN is a much more elegant solution to the partial observability problem, letting the network decide which horizon it needs to solve long-term dependencies.</p>
<h2 id="sec:other-variants-of-dqn"><span class="header-section-number">2.8</span> Other variants of DQN</h2>
<p>Double duelling DQN with prioritized replay is currently the state-of-the-art method for value-based deep RL. Several minor to significant improvements have been proposed since the corresponding milestone papers. This section provides some short explanations and links to the original papers (to be organized and extended).</p>
<p><strong>Average-DQN</strong> proposes to increase the stability and performance of DQN by replacing the single target network (a copy of the trained network) by an average of the last parameter values, in other words an average of many past target networks <span class="citation" data-cites="Anschel2016">(Anschel, Baram, and Shimkin, <a href="#ref-Anschel2016">2016</a>)</span>.</p>
<p><span class="citation" data-cites="He2016">He, Liu, Schwing, and Peng (<a href="#ref-He2016">2016</a>)</span> proposed <strong>fast reward propagation</strong> thourgh optimality tightening to speedup learning: when rewards are sparse, they require a lot of episodes to propagate these rare rewards to all actions leading to it. Their method combines immediate rewards (single steps) with actual returns (as in Monte-Carlo) via a constrained optimization approach.</p>
<p>All RL methods based on the Bellman equations use the expectation operator to average returns and compute values. <span class="citation" data-cites="Bellemare2017">Bellemare, Dabney, and Munos (<a href="#ref-Bellemare2017">2017</a>)</span> propose to learn instead the <strong>value distribution</strong> through a modification of the Bellman equation. They show that learning the distribution of rewards rather than their mean leads to performance improvements. See <a href="https://deepmind.com/blog/going-beyond-average-reinforcement-learning/" class="uri">https://deepmind.com/blog/going-beyond-average-reinforcement-learning/</a> for more explanations.</p>
<h1 id="sec:policy-based-methods"><span class="header-section-number">3</span> Policy-based methods</h1>
<p><em>Policy gradient</em> methods directly learn to produce the policy (stochastic or not). The goal of the neural network is to maximize an objective function <span class="math inline">\(J(\theta) = {E}_{\pi_\theta}(R_t)\)</span>. The   provides a useful estimate of the gradient that should be given to the neural network:</p>
<p><span><span class="math display">\[
\nabla_\theta J(\theta) = {E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) Q^{\pi_\theta}(s, a)]
\qquad(61)\]</span></span></p>
<p><span class="citation" data-cites="Sutton1999">Sutton, McAllester, Singh, and Mansour (<a href="#ref-Sutton1999">1999</a>)</span>, <span class="citation" data-cites="Silver2014">Silver et al. (<a href="#ref-Silver2014">2014</a>)</span></p>
<p><a href="http://www.scholarpedia.org/article/Policy_gradient_methods" class="uri">http://www.scholarpedia.org/article/Policy_gradient_methods</a></p>
<h2 id="sec:actor-critic"><span class="header-section-number">3.1</span> Actor-critic</h2>
<h3 id="sec:reinforce"><span class="header-section-number">3.1.1</span> REINFORCE</h3>
<p><span class="citation" data-cites="Williams1992">Williams (<a href="#ref-Williams1992">1992</a>)</span></p>
<h3 id="sec:advantage-actor-critic-a2c"><span class="header-section-number">3.1.2</span> Advantage Actor-Critic (A2C)</h3>
<p>Advantage actor-critic</p>
<h3 id="sec:asynchronous-advantage-actor-critic-a3c"><span class="header-section-number">3.1.3</span> Asynchronous Advantage Actor-critic (A3C)</h3>
<p><span class="citation" data-cites="Mnih2016">Mnih et al. (<a href="#ref-Mnih2016">2016</a>)</span></p>
<p>HogWild!: <span class="citation" data-cites="Niu2011">Niu, Recht, Re, and Wright (<a href="#ref-Niu2011">2011</a>)</span></p>
<h2 id="sec:policy-gradient"><span class="header-section-number">3.2</span> Policy gradient</h2>
<h3 id="sec:stochastic-value-gradient-svg"><span class="header-section-number">3.2.1</span> Stochastic Value Gradient (SVG)</h3>
<p><span class="citation" data-cites="Heess2015">Heess et al. (<a href="#ref-Heess2015">2015</a>)</span></p>
<h3 id="sec:deterministic-policy-gradient-dpg"><span class="header-section-number">3.2.2</span> Deterministic Policy Gradient (DPG)</h3>
<p><span class="citation" data-cites="Silver2014">Silver et al. (<a href="#ref-Silver2014">2014</a>)</span></p>
<h3 id="sec:deep-deterministic-policy-gradient-ddpg"><span class="header-section-number">3.2.3</span> Deep Deterministic Policy Gradient (DDPG)</h3>
<p><span class="citation" data-cites="Lillicrap2015">Lillicrap et al. (<a href="#ref-Lillicrap2015">2015</a>)</span></p>
<h3 id="sec:fictitious-self-play-fsp"><span class="header-section-number">3.2.4</span> Fictitious Self-Play (FSP)</h3>
<p><span class="citation" data-cites="Heinrich2015">Heinrich, Lanctot, and Silver (<a href="#ref-Heinrich2015">2015</a>)</span> <span class="citation" data-cites="Heinrich2016">Heinrich and Silver (<a href="#ref-Heinrich2016">2016</a>)</span></p>
<h3 id="sec:trust-region-policy-optimization-trpo"><span class="header-section-number">3.2.5</span> Trust Region Policy Optimization (TRPO)</h3>
<p>Natural policy gradient <span class="citation" data-cites="Kakade2001">Kakade (<a href="#ref-Kakade2001">2001</a>)</span></p>
<p><span class="citation" data-cites="Schulman2015a">Schulman et al. (<a href="#ref-Schulman2015a">2015</a><a href="#ref-Schulman2015a">a</a>)</span></p>
<h3 id="sec:generalized-advantage-estimation-gae"><span class="header-section-number">3.2.6</span> Generalized Advantage Estimation (GAE)</h3>
<p><span class="citation" data-cites="Schulman2015b">Schulman et al. (<a href="#ref-Schulman2015b">2015</a><a href="#ref-Schulman2015b">b</a>)</span></p>
<h3 id="sec:proximal-policy-optimization-ppo"><span class="header-section-number">3.2.7</span> Proximal Policy Optimization (PPO)</h3>
<p><span class="citation" data-cites="Schulman2017">Schulman, Wolski, Dhariwal, Radford, and Klimov (<a href="#ref-Schulman2017">2017</a>)</span></p>
<h3 id="sec:cross-entropy-method-cem"><span class="header-section-number">3.2.8</span> Cross-entropy Method (CEM)</h3>
<p><span class="citation" data-cites="Szita2006">Szita and Lörincz (<a href="#ref-Szita2006">2006</a>)</span></p>
<h3 id="sec:comparison-between-value-based-and-policy-based-methods"><span class="header-section-number">3.2.9</span> Comparison between value-based and policy-based methods</h3>
<p><a href="https://flyyufelix.github.io/2017/10/12/dqn-vs-pg.html" class="uri">https://flyyufelix.github.io/2017/10/12/dqn-vs-pg.html</a></p>
<h1 id="sec:recurrent-attention-models"><span class="header-section-number">4</span> Recurrent Attention Models</h1>
<p><strong>Work in progress</strong></p>
<p><span class="citation" data-cites="Mnih2014">Mnih, Heess, Graves, and Kavukcuoglu (<a href="#ref-Mnih2014">2014</a>)</span>, <span class="citation" data-cites="Ba2014">Ba, Mnih, and Kavukcuoglu (<a href="#ref-Ba2014">2014</a>)</span>. <span class="citation" data-cites="Stollenga2014">Stollenga, Masci, Gomez, and Schmidhuber (<a href="#ref-Stollenga2014">2014</a>)</span></p>
<h1 id="sec:model-based-rl"><span class="header-section-number">5</span> Model-based RL</h1>
<p><strong>Work in progress</strong></p>
<p><span class="citation" data-cites="Watter2015">Watter, Springenberg, Boedecker, and Riedmiller (<a href="#ref-Watter2015">2015</a>)</span>, <span class="citation" data-cites="Corneil2018">Corneil, Gerstner, and Brea (<a href="#ref-Corneil2018">2018</a>)</span>, <span class="citation" data-cites="Feinberg2018">Feinberg et al. (<a href="#ref-Feinberg2018">2018</a>)</span>, <span class="citation" data-cites="Weber2017">Weber et al. (<a href="#ref-Weber2017">2017</a>)</span></p>
<h1 id="sec:deep-rl-for-robotics"><span class="header-section-number">6</span> Deep RL for robotics</h1>
<p><strong>Work in progress</strong></p>
<p><span class="citation" data-cites="Agrawal2016">Agrawal, Nair, Abbeel, Malik, and Levine (<a href="#ref-Agrawal2016">2016</a>)</span>, <span class="citation" data-cites="Dosovitskiy2016">Dosovitskiy and Koltun (<a href="#ref-Dosovitskiy2016">2016</a>)</span>, <span class="citation" data-cites="Gu2017">Gu et al. (<a href="#ref-Gu2017">2017</a>)</span>, <span class="citation" data-cites="Levine2016a">Levine et al. (<a href="#ref-Levine2016a">2016</a><a href="#ref-Levine2016a">a</a>)</span>, <span class="citation" data-cites="Levine2016b">Levine et al. (<a href="#ref-Levine2016b">2016</a><a href="#ref-Levine2016b">b</a>)</span>, <span class="citation" data-cites="Zhang2015">Zhang, Leitner, Milford, Upcroft, and Corke (<a href="#ref-Zhang2015">2015</a>)</span></p>
<h1 id="sec:rl-libraries"><span class="header-section-number">7</span> RL libraries</h1>
<h2 id="sec:environments"><span class="header-section-number">7.1</span> Environments</h2>
<p>Standard RL environments are needed to better compare the performance of RL algoritms. Below is a list of the most popular ones.</p>
<ul>
<li>OpenAI Gym <a href="https://gym.openai.com" class="uri">https://gym.openai.com</a>: a standard toolkit for comparing RL algorithms provided by the OpenAI fundation. It provides many environments, from the classical toy problems in RL (GridWorld, pole-balancing) to more advanced problems (Mujoco simulated robots, Atari games, Minecraft…). The main advantage is the simplicity of the interface: the user only needs to select which task he wants to solve, and a simple for loop allows to perform actions and observe their consequences:</li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="im">import</span> gym</a>
<a class="sourceLine" id="cb1-2" data-line-number="2">env <span class="op">=</span> gym.make(<span class="st">&quot;Taxi-v1&quot;</span>)</a>
<a class="sourceLine" id="cb1-3" data-line-number="3">observation <span class="op">=</span> env.reset()</a>
<a class="sourceLine" id="cb1-4" data-line-number="4"><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</a>
<a class="sourceLine" id="cb1-5" data-line-number="5">    env.render()</a>
<a class="sourceLine" id="cb1-6" data-line-number="6">    action <span class="op">=</span> env.action_space.sample()</a>
<a class="sourceLine" id="cb1-7" data-line-number="7">    observation, reward, done, info <span class="op">=</span> env.step(action)</a></code></pre></div>
<ul>
<li><p>OpenAI Universe <a href="https://universe.openai.com" class="uri">https://universe.openai.com</a>: a similar framework from OpenAI, but to control realistic video games (GTA V, etc).</p></li>
<li><p>Darts environment <a href="https://github.com/DartEnv/dart-env" class="uri">https://github.com/DartEnv/dart-env</a>: a fork of gym to use the Darts simulator instead of Mujoco.</p></li>
<li><p>Roboschool <a href="https://github.com/openai/roboschool" class="uri">https://github.com/openai/roboschool</a>: another alternative to Mujoco for continuous robotic control, this time from openAI.</p></li>
<li><p>NIPS 2017 musculoskettal challenge <a href="https://github.com/stanfordnmbl/osim-rl" class="uri">https://github.com/stanfordnmbl/osim-rl</a></p></li>
</ul>
<h2 id="sec:algorithms"><span class="header-section-number">7.2</span> Algorithms</h2>
<p>State-of-the-art algorithms in deep RL are already implemented and freely available on the internet. Below is a preliminary list of the most popular ones. Most of them rely on tensorflow or keras for training the neural networks and interact directly with gym-like interfaces.</p>
<ul>
<li><p><code>rl-code</code> <a href="https://github.com/rlcode/reinforcement-learning" class="uri">https://github.com/rlcode/reinforcement-learning</a>: many code samples for simple RL problems (GridWorld, Cartpole, Atari Games). The code samples are mostly for educational purpose (Policy Iteration, Value Iteration, Monte-Carlo, SARSA, Q-learning, REINFORCE, DQN, A2C, A3C).</p></li>
<li><p><code>keras-rl</code> <a href="https://github.com/matthiasplappert/keras-rl" class="uri">https://github.com/matthiasplappert/keras-rl</a>: many deep RL algorithms implemented directly in keras.</p>
<ul>
<li>Deep Q Learning (DQN)</li>
<li>Double DQN</li>
<li>Deep Deterministic Policy Gradient (DDPG)</li>
<li>Continuous DQN (CDQN or NAF)</li>
<li>Cross-Entropy Method (CEM)</li>
<li>Dueling network DQN (Dueling DQN)</li>
<li>Deep SARSA</li>
</ul></li>
<li><p><code>Coach</code> <a href="https://github.com/NervanaSystems/coach" class="uri">https://github.com/NervanaSystems/coach</a> from Intel Nervana also provides many state-of-the-art algorithms.</p>
<ul>
<li>Deep Q Network (DQN</li>
<li>Double Deep Q Network (DDQN)</li>
<li>Dueling Q Network</li>
<li>Mixed Monte Carlo (MMC)</li>
<li>Persistent Advantage Learning (PAL)</li>
<li>Distributional Deep Q Network</li>
<li>Bootstrapped Deep Q Network</li>
<li>N-Step Q Learning | Distributed</li>
<li>Neural Episodic Control (NEC)</li>
<li>Normalized Advantage Functions (NAF) | Distributed</li>
<li>Policy Gradients (PG) | Distributed</li>
<li>Actor Critic / A3C | Distributed</li>
<li>Deep Deterministic Policy Gradients (DDPG) | Distributed</li>
<li>Proximal Policy Optimization (PPO)</li>
<li>Clipped Proximal Policy Optimization | Distributed</li>
<li>Direct Future Prediction (DFP) | Distributed</li>
</ul></li>
<li><p><code>OpenAI Baselines</code> <a href="https://github.com/openai/baselines" class="uri">https://github.com/openai/baselines</a> from OpenAI too.</p>
<ul>
<li>A2C</li>
<li>ACER</li>
<li>ACKTR</li>
<li>DDPG</li>
<li>DQN</li>
<li>PPO1 (Multi-CPU using MPI)</li>
<li>PPO2 (Optimized for GPU)</li>
<li>TRPO</li>
</ul></li>
</ul>
<h1 id="sec:thanks" class="unnumbered">Thanks</h1>
<p>Thanks to all the students who helped me dive into that exciting research field, in particular: Winfried Lötzsch, Johannes Jung, Frank Witscher, Danny Hofmann, Oliver Lange, Vinayakumar Murganoor.</p>
<h1 id="sec:notes" class="unnumbered">Notes</h1>
<p>This document is meant to stay <em>work in progress</em> forever, as new algorithms will be added as they are published. Feel free to comment, correct, suggest, pull request by writing to <a href="mailto:julien.vitay@informatik.tu-chemnitz.de">julien.vitay@informatik.tu-chemnitz.de</a>.</p>
<p>For some reason, this document is better printed using chrome. The style is adapted from the Github-Markdown CSS template <a href="https://www.npmjs.com/package/github-markdown-css" class="uri">https://www.npmjs.com/package/github-markdown-css</a>.</p>
<p>Some figures are taken from the original publication (“Taken from” or “Source” in the caption). Their copyright stays to the respective authors, naturally. The rest is my own work and can be reproduced under any free license.</p>
<h1 id="sec:references" class="unnumbered">References</h1>
<div id="refs" class="references">
<div id="ref-Agrawal2016">
<p>Agrawal, P., Nair, A., Abbeel, P., Malik, J., and Levine, S. (2016). Learning to Poke by Poking: Experiential Learning of Intuitive Physics. Retrieved from <a href="http://arxiv.org/abs/1606.07419" class="uri">http://arxiv.org/abs/1606.07419</a></p>
</div>
<div id="ref-Anschel2016">
<p>Anschel, O., Baram, N., and Shimkin, N. (2016). Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning. Retrieved from <a href="http://arxiv.org/abs/1611.01929" class="uri">http://arxiv.org/abs/1611.01929</a></p>
</div>
<div id="ref-Arulkumaran2017">
<p>Arulkumaran, K., Deisenroth, M. P., Brundage, M., and Bharath, A. A. (2017). A Brief Survey of Deep Reinforcement Learning. Retrieved from <a href="https://arxiv.org/pdf/1708.05866.pdf" class="uri">https://arxiv.org/pdf/1708.05866.pdf</a></p>
</div>
<div id="ref-Ba2014">
<p>Ba, J., Mnih, V., and Kavukcuoglu, K. (2014). Multiple Object Recognition with Visual Attention. Retrieved from <a href="http://arxiv.org/abs/1412.7755" class="uri">http://arxiv.org/abs/1412.7755</a></p>
</div>
<div id="ref-Baird1993">
<p>Baird, L. (1993). <em>Advantage updating</em>. Technical report WL- TR-93-1146, Wright-Patterson Air Force Base.</p>
</div>
<div id="ref-Bakker2001">
<p>Bakker, B. (2001). Reinforcement Learning with Long Short-Term Memory. In <em>Adv. Neural inf. Process. Syst. 14 (nips 2001)</em> (pp. 1475–1482). Retrieved from <a href="https://papers.nips.cc/paper/1953-reinforcement-learning-with-long-short-term-memory" class="uri">https://papers.nips.cc/paper/1953-reinforcement-learning-with-long-short-term-memory</a></p>
</div>
<div id="ref-Bellemare2017">
<p>Bellemare, M. G., Dabney, W., and Munos, R. (2017). A Distributional Perspective on Reinforcement Learning. Retrieved from <a href="http://arxiv.org/abs/1707.06887" class="uri">http://arxiv.org/abs/1707.06887</a></p>
</div>
<div id="ref-Cho2014">
<p>Cho, K., Merrienboer, B. van, Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y. (2014). Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. Retrieved from <a href="http://arxiv.org/abs/1406.1078" class="uri">http://arxiv.org/abs/1406.1078</a></p>
</div>
<div id="ref-Corneil2018">
<p>Corneil, D., Gerstner, W., and Brea, J. (2018). Efficient Model-Based Deep Reinforcement Learning with Variational State Tabulation. Retrieved from <a href="http://arxiv.org/abs/1802.04325" class="uri">http://arxiv.org/abs/1802.04325</a></p>
</div>
<div id="ref-Dosovitskiy2016">
<p>Dosovitskiy, A., and Koltun, V. (2016). Learning to Act by Predicting the Future. Retrieved from <a href="http://arxiv.org/abs/1611.01779" class="uri">http://arxiv.org/abs/1611.01779</a></p>
</div>
<div id="ref-Feinberg2018">
<p>Feinberg, V., Wan, A., Stoica, I., Jordan, M. I., Gonzalez, J. E., and Levine, S. (2018). Model-Based Value Estimation for Efficient Model-Free Reinforcement Learning. Retrieved from <a href="http://arxiv.org/abs/1803.00101" class="uri">http://arxiv.org/abs/1803.00101</a></p>
</div>
<div id="ref-Gers2001">
<p>Gers, F. (2001). <em>Long Short-Term Memory in Recurrent Neural Networks</em> (PhD Thesis). Ecole Polytechnique Fédérale de Lausanne. Retrieved from <a href="http://www.felixgers.de/papers/phd.pdf" class="uri">http://www.felixgers.de/papers/phd.pdf</a></p>
</div>
<div id="ref-Goodfellow2016">
<p>Goodfellow, I., Bengio, Y., and Courville, A. (2016). <em>Deep Learning</em>. MIT Press. Retrieved from <a href="http://www.deeplearningbook.org" class="uri">http://www.deeplearningbook.org</a></p>
</div>
<div id="ref-Gu2017">
<p>Gu, S., Holly, E., Lillicrap, T., and Levine, S. (2017). Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates. In <em>Proc. ICRA</em>. Retrieved from <a href="http://arxiv.org/abs/1610.00633" class="uri">http://arxiv.org/abs/1610.00633</a></p>
</div>
<div id="ref-vanHasselt2010">
<p>Hasselt, H. van. (2010). Double Q-learning. In <em>Proc. 23rd int. Conf. Neural inf. Process. Syst. - vol. 2</em> (pp. 2613–2621). Curran Associates Inc. Retrieved from <a href="https://dl.acm.org/citation.cfm?id=2997187" class="uri">https://dl.acm.org/citation.cfm?id=2997187</a></p>
</div>
<div id="ref-vanHasselt2015">
<p>Hasselt, H. van, Guez, A., and Silver, D. (2015). Deep Reinforcement Learning with Double Q-learning. Retrieved from <a href="http://arxiv.org/abs/1509.06461" class="uri">http://arxiv.org/abs/1509.06461</a></p>
</div>
<div id="ref-Hausknecht2015">
<p>Hausknecht, M., and Stone, P. (2015). Deep Recurrent Q-Learning for Partially Observable MDPs. Retrieved from <a href="http://arxiv.org/abs/1507.06527" class="uri">http://arxiv.org/abs/1507.06527</a></p>
</div>
<div id="ref-He2016">
<p>He, F. S., Liu, Y., Schwing, A. G., and Peng, J. (2016). Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening. Retrieved from <a href="http://arxiv.org/abs/1611.01606" class="uri">http://arxiv.org/abs/1611.01606</a></p>
</div>
<div id="ref-He2015">
<p>He, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep Residual Learning for Image Recognition. Retrieved from <a href="http://arxiv.org/abs/1512.03385" class="uri">http://arxiv.org/abs/1512.03385</a></p>
</div>
<div id="ref-Heess2015">
<p>Heess, N., Wayne, G., Silver, D., Lillicrap, T., Tassa, Y., and Erez, T. (2015). Learning continuous control policies by stochastic value gradients. MIT Press. Retrieved from <a href="http://dl.acm.org/citation.cfm?id=2969569" class="uri">http://dl.acm.org/citation.cfm?id=2969569</a></p>
</div>
<div id="ref-Heinrich2015">
<p>Heinrich, J., Lanctot, M., and Silver, D. (2015, June). Fictitious Self-Play in Extensive-Form Games. Retrieved from <a href="http://proceedings.mlr.press/v37/heinrich15.html" class="uri">http://proceedings.mlr.press/v37/heinrich15.html</a></p>
</div>
<div id="ref-Heinrich2016">
<p>Heinrich, J., and Silver, D. (2016). Deep Reinforcement Learning from Self-Play in Imperfect-Information Games. Retrieved from <a href="http://arxiv.org/abs/1603.01121" class="uri">http://arxiv.org/abs/1603.01121</a></p>
</div>
<div id="ref-Hochreiter1991">
<p>Hochreiter, S. (1991). <em>Untersuchungen zu dynamischen neuronalen Netzen</em> (Diploma thesis). TU Munich. Retrieved from <a href="http://people.idsia.ch/{~}juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf" class="uri">http://people.idsia.ch/{~}juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf</a></p>
</div>
<div id="ref-Hochreiter1997">
<p>Hochreiter, S., and Schmidhuber, J. (1997). Long Short-Term Memory. <em>Neural Comput.</em>, <em>9</em>(8), 1735–1780. <a href="https://doi.org/10.1162/neco.1997.9.8.1735" class="uri">https://doi.org/10.1162/neco.1997.9.8.1735</a></p>
</div>
<div id="ref-Kakade2001">
<p>Kakade, S. (2001). A Natural Policy Gradient. In <em>Adv. Neural inf. Process. Syst. 14</em>. Retrieved from <a href="https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf" class="uri">https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf</a></p>
</div>
<div id="ref-Krizhevsky2012">
<p>Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. In <em>Adv. Neural inf. Process. Syst.</em> Retrieved from <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" class="uri">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a></p>
</div>
<div id="ref-Levine2016a">
<p>Levine, S., Finn, C., Darrell, T., and Abbeel, P. (2016a). End-to-End Training of Deep Visuomotor Policies. <em>JMLR</em>, <em>17</em>. Retrieved from <a href="http://arxiv.org/abs/1504.00702" class="uri">http://arxiv.org/abs/1504.00702</a></p>
</div>
<div id="ref-Levine2016b">
<p>Levine, S., Pastor, P., Krizhevsky, A., and Quillen, D. (2016b). Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection. In <em>Proc. ISER</em>. Retrieved from <a href="http://arxiv.org/abs/1603.02199" class="uri">http://arxiv.org/abs/1603.02199</a></p>
</div>
<div id="ref-Li2017">
<p>Li, Y. (2017). Deep Reinforcement Learning: An Overview. Retrieved from <a href="http://arxiv.org/abs/1701.07274" class="uri">http://arxiv.org/abs/1701.07274</a></p>
</div>
<div id="ref-Lillicrap2015">
<p>Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., … Wierstra, D. (2015). Continuous control with deep reinforcement learning. <em>CoRR</em>. Retrieved from <a href="http://arxiv.org/abs/1509.02971" class="uri">http://arxiv.org/abs/1509.02971</a></p>
</div>
<div id="ref-Mnih2016">
<p>Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T. P., Harley, T., … Kavukcuoglu, K. (2016). Asynchronous Methods for Deep Reinforcement Learning. In <em>Proc. ICML</em>. Retrieved from <a href="http://arxiv.org/abs/1602.01783" class="uri">http://arxiv.org/abs/1602.01783</a></p>
</div>
<div id="ref-Mnih2014">
<p>Mnih, V., Heess, N., Graves, A., and Kavukcuoglu, K. (2014). Recurrent Models of Visual Attention. Retrieved from <a href="http://arxiv.org/abs/1406.6247" class="uri">http://arxiv.org/abs/1406.6247</a></p>
</div>
<div id="ref-Mnih2013">
<p>Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. (2013). Playing Atari with Deep Reinforcement Learning. Retrieved from <a href="http://arxiv.org/abs/1312.5602" class="uri">http://arxiv.org/abs/1312.5602</a></p>
</div>
<div id="ref-Mnih2015">
<p>Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., … Hassabis, D. (2015). Human-level control through deep reinforcement learning. <em>Nature</em>, <em>518</em>(7540), 529–533. <a href="https://doi.org/10.1038/nature14236" class="uri">https://doi.org/10.1038/nature14236</a></p>
</div>
<div id="ref-Mousavi2018">
<p>Mousavi, S. S., Schukat, M., and Howley, E. (2018). Deep Reinforcement Learning: An Overview. In (pp. 426–440). Springer, Cham. <a href="https://doi.org/10.1007/978-3-319-56991-8_32" class="uri">https://doi.org/10.1007/978-3-319-56991-8_32</a></p>
</div>
<div id="ref-Nair2015">
<p>Nair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., De Maria, A., … Silver, D. (2015). Massively Parallel Methods for Deep Reinforcement Learning. Retrieved from <a href="https://arxiv.org/pdf/1507.04296.pdf" class="uri">https://arxiv.org/pdf/1507.04296.pdf</a></p>
</div>
<div id="ref-Nielsen2015">
<p>Nielsen, M. A. (2015). <em>Neural Networks and Deep Learning</em>. Determination Press. Retrieved from <a href="http://neuralnetworksanddeeplearning.com/" class="uri">http://neuralnetworksanddeeplearning.com/</a></p>
</div>
<div id="ref-Niu2011">
<p>Niu, F., Recht, B., Re, C., and Wright, S. J. (2011). HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent. In <em>Proc. Adv. Neural inf. Process. Syst.</em> (p. 21). Retrieved from <a href="http://arxiv.org/abs/1106.5730" class="uri">http://arxiv.org/abs/1106.5730</a></p>
</div>
<div id="ref-Ruder2016">
<p>Ruder, S. (2016). An overview of gradient descent optimization algorithms. Retrieved from <a href="http://arxiv.org/abs/1609.04747" class="uri">http://arxiv.org/abs/1609.04747</a></p>
</div>
<div id="ref-Schaul2015">
<p>Schaul, T., Quan, J., Antonoglou, I., and Silver, D. (2015). Prioritized Experience Replay. Retrieved from <a href="http://arxiv.org/abs/1511.05952" class="uri">http://arxiv.org/abs/1511.05952</a></p>
</div>
<div id="ref-Schulman2015a">
<p>Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015a, June). Trust Region Policy Optimization. Retrieved from <a href="http://proceedings.mlr.press/v37/schulman15.html" class="uri">http://proceedings.mlr.press/v37/schulman15.html</a></p>
</div>
<div id="ref-Schulman2015b">
<p>Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P. (2015b). High-Dimensional Continuous Control Using Generalized Advantage Estimation. Retrieved from <a href="http://arxiv.org/abs/1506.02438" class="uri">http://arxiv.org/abs/1506.02438</a></p>
</div>
<div id="ref-Schulman2017">
<p>Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal Policy Optimization Algorithms. Retrieved from <a href="http://arxiv.org/abs/1707.06347" class="uri">http://arxiv.org/abs/1707.06347</a></p>
</div>
<div id="ref-Silver2014">
<p>Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M. (2014). Deterministic Policy Gradient Algorithms. In E. P. Xing and T. Jebara (Eds.), <em>Proc. ICML</em> (Vol. 32, pp. 387–395). Bejing, China: PMLR. Retrieved from <a href="http://proceedings.mlr.press/v32/silver14.html" class="uri">http://proceedings.mlr.press/v32/silver14.html</a></p>
</div>
<div id="ref-Simonyan2014">
<p>Simonyan, K., and Zisserman, A. (2014). Very Deep Convolutional Networks for Large-Scale Image Recognition. Retrieved from <a href="http://arxiv.org/abs/1409.1556" class="uri">http://arxiv.org/abs/1409.1556</a></p>
</div>
<div id="ref-Stollenga2014">
<p>Stollenga, M., Masci, J., Gomez, F., and Schmidhuber, J. (2014). Deep Networks with Internal Selective Attention through Feedback Connections. Retrieved from <a href="http://arxiv.org/abs/1407.3068" class="uri">http://arxiv.org/abs/1407.3068</a></p>
</div>
<div id="ref-Sutton1998">
<p>Sutton, R. S., and Barto, A. G. (1998). <em>Reinforcement Learning: An introduction</em>. Cambridge, MA: MIT press.</p>
</div>
<div id="ref-Sutton2017">
<p>Sutton, R. S., and Barto, A. G. (2017). <em>Reinforcement Learning: An Introduction</em> (2nd ed.). Cambridge, MA: MIT Press. Retrieved from <a href="http://incompleteideas.net/sutton/book/the-book-2nd.html" class="uri">http://incompleteideas.net/sutton/book/the-book-2nd.html</a></p>
</div>
<div id="ref-Sutton1999">
<p>Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour, Y. (1999). Policy Gradient Methods for Reinforcement Learning with Function Approximation. In <em>Neural inf. Process. Syst. 12</em> (pp. 1057–1063).</p>
</div>
<div id="ref-Szita2006">
<p>Szita, I., and Lörincz, A. (2006). Learning Tetris Using the Noisy Cross-Entropy Method. <em>Neural Comput.</em>, <em>18</em>(12), 2936–2941. <a href="https://doi.org/10.1162/neco.2006.18.12.2936" class="uri">https://doi.org/10.1162/neco.2006.18.12.2936</a></p>
</div>
<div id="ref-Wang2016">
<p>Wang, Z., Schaul, T., Hessel, M., Hasselt, H. van, Lanctot, M., and Freitas, N. de. (2016). Dueling Network Architectures for Deep Reinforcement Learning. In <em>Proc. ICML</em>. New York, NY, USA. Retrieved from <a href="http://arxiv.org/abs/1511.06581" class="uri">http://arxiv.org/abs/1511.06581</a></p>
</div>
<div id="ref-Watkins1989">
<p>Watkins, C. J. (1989). <em>Learning from delayed rewards</em> (PhD thesis). University of Cambridge, England.</p>
</div>
<div id="ref-Watter2015">
<p>Watter, M., Springenberg, J. T., Boedecker, J., and Riedmiller, M. (2015). Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images. Retrieved from <a href="https://arxiv.org/pdf/1506.07365.pdf" class="uri">https://arxiv.org/pdf/1506.07365.pdf</a></p>
</div>
<div id="ref-Weber2017">
<p>Weber, T., Racanière, S., Reichert, D. P., Buesing, L., Guez, A., Rezende, D. J., … Wierstra, D. (2017). Imagination-Augmented Agents for Deep Reinforcement Learning. Retrieved from <a href="http://arxiv.org/abs/1707.06203" class="uri">http://arxiv.org/abs/1707.06203</a></p>
</div>
<div id="ref-Wierstra2007">
<p>Wierstra, D., Foerster, A., Peters, J., and Schmidhuber, J. (2007). Solving Deep Memory POMDPs with Recurrent Policy Gradients. In (pp. 697–706). Springer, Berlin, Heidelberg. <a href="https://doi.org/10.1007/978-3-540-74690-4_71" class="uri">https://doi.org/10.1007/978-3-540-74690-4_71</a></p>
</div>
<div id="ref-Williams1992">
<p>Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. <em>Mach. Learn.</em>, <em>8</em>, 229–256.</p>
</div>
<div id="ref-Zhang2015">
<p>Zhang, F., Leitner, J., Milford, M., Upcroft, B., and Corke, P. (2015). Towards Vision-Based Deep Reinforcement Learning for Robotic Motion Control. In <em>Proc. Acra</em>. Retrieved from <a href="http://arxiv.org/abs/1511.03791" class="uri">http://arxiv.org/abs/1511.03791</a></p>
</div>
</div>

<article>



</body>
</html>
