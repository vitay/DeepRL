

## Natural Gradients

Natural policy gradient @Kakade2001

### Natural Actor Critic (NAC)

@Peters2008

### Trust Region Policy Optimization (TRPO)

@Schulman2015a

### Proximal Policy Optimization (PPO)

@Schulman2017

Explanations from OpenAI: <https://blog.openai.com/openai-baselines-ppo/#content>

## Off-policy Actor-Critic

### Importance sampling


@Meuleau200
@Tang2010
@Levine2013

### Linear Off-Policy Actor-Critic (OffPAC)

@Degris2012

### Actor-Critic with Experience Replay (ACER)

@Wang2017


## Other policy gradient methods


### Stochastic Value Gradient (SVG)

@Heess2015

### Fictitious Self-Play (FSP)

@Heinrich2015 @Heinrich2016

### Q-Prop

@Gu2016

### Normalized Advantage Function (NAF)

@Gu2016b

## Comparison between value-based and policy gradient methods

<https://flyyufelix.github.io/2017/10/12/dqn-vs-pg.html>

Advantages of policy gradient:

* Better convergence properties.
* Effective in high-dimensional or continuous action spaces.
* Can learn stochastic policies.

Disadvantages:

* Typically converge to a local rather than global optimum.
* Evaluating a policy is typically inefficient and high variance.

Policy gradient methods are therefore usually less sample efficient, but can be more stable than value-based methods (Duan et al., 2016).

## Gradient-free policy search

The policy gradient methods presented above rely on backpropagation and gradient descent/ascent to update the parameters of the policy and maximize the objective function. Gradient descent is generally slow, sample inefficient and subject to local minima, but is nevertheless the go-to method in neural networks. However, it is not the only optimization that can be used in deep RL. This section presents some of the alternatives. 

### Cross-entropy Method (CEM) 

@Szita2006

### Evolutionary Search (ES)

@Salimans2017

Explanations from OpenAI: <https://blog.openai.com/evolution-strategies/>

Deep neuroevolution at Uber: <https://eng.uber.com/deep-neuroevolution/>

