## Distributional learning

All RL methods based on the Bellman equations use the expectation operator to average returns and compute the values of states and actions:

$$
    Q^\pi(s, a) ) = \mathbb{E}_{\pi}[R(s, a)]
$$

@Bellemare2017 propose to learn instead the **value distribution** through a modification of the Bellman equation. They show that learning the distribution of rewards rather than their mean leads to performance improvements. 

See <https://deepmind.com/blog/going-beyond-average-reinforcement-learning/> for more explanations.


### The Reactor

@Gruslys2017


## Entropy-based RL


### Soft Actor-Critic (SAC)

@Haarnoya2018b

## Other policy search methods

### Stochastic Value Gradient (SVG)

@Heess2015


### Q-Prop

@Gu2016

### Normalized Advantage Function (NAF)

@Gu2016b


### Fictitious Self-Play (FSP)

@Heinrich2015 @Heinrich2016


## Comparison between value-based and policy gradient methods

Having now reviewed both value-based methods (DQN and its variants) and policy gradient methods (A3C, DDPG, PPO), the question is which method to choose? While not much happens right now for value-based methods, policy gradient methods are atrracting a lot of attention, as they are able to learn policies in continuous action spaces, what is very important in robotics.  <https://flyyufelix.github.io/2017/10/12/dqn-vs-pg.html> summarizes the advantages and inconvenients of policy gradient methods.

Advantages:

* Better convergence properties.
* Effective in high-dimensional or continuous action spaces.
* Can learn stochastic policies.

Disadvantages:

* Typically converge to a local rather than global optimum.
* Evaluating a policy is typically inefficient and high variance.

Policy gradient methods are therefore usually less sample efficient, but can be more stable than value-based methods [@Duan2016].

## Gradient-free policy search

The policy gradient methods presented above rely on backpropagation and gradient descent/ascent to update the parameters of the policy and maximize the objective function. Gradient descent is generally slow, sample inefficient and subject to local minima, but is nevertheless the go-to method in neural networks. However, it is not the only optimization that can be used in deep RL. This section presents some of the alternatives. 

### Cross-entropy Method (CEM) 

@Szita2006

### Evolutionary Search (ES)

@Salimans2017

Explanations from OpenAI: <https://blog.openai.com/evolution-strategies/>

Deep neuroevolution at Uber: <https://eng.uber.com/deep-neuroevolution/>